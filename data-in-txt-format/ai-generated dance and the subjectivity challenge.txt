Benedikte Wallace
AI-generated Dance and The
Subjectivity Challenge
Thesis submitted for the degree of Philosophiae Doctor
Department of Informatics
Faculty of Mathematics and Natural Sciences
RITMO Center for Interdisciplinary studies in Rhythm, Time and
Motion
2023
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
© Benedikte Wallace, 2023 
 
 
Series of dissertations submitted to the  
Faculty of Mathematics and Natural Sciences, University of Oslo 
No. 2652 
 
ISSN 1501-7710 
 
 
All rights reserved. No part of this publication may be  
reproduced or transmitted, in any form or by any means, without permission.   
 
 
 
 
 
 
 
 
 
Cover: UiO. 
Print production: Graphic center, University of Oslo. 
 
  
To my loved ones
Abstract
The inherently subjective notion of quality in creative artefacts gives rise to
both technical and philosophical questions regarding AI-generated art. In this
thesis, the development and evaluation of AI-generated dance forms the context
within which these questions are explored. Dance is a multifaceted art form. In
rituals and celebrations, improvised or choreographed, a social glue and personal
expression, dance plays a role in all human cultures and many aspects of life.
Dance movement is also a rich, complex data source. The main objectives of this
thesis are to understand more about how deep learning can be used to capture
salient features of movement, and especially dance movement, using full-body
motion capture data. We further explore how such technology might be aimed
to beneﬁt the creative practice of dancers.
Our work includes the collection of a data set of improvisation performed
by individual dancers and the implementation of generative models trained on
this data. We evaluate the model’s ability to learn the intricate relationship
between music and movement and investigate the trained model’s generative
versatility. We also examine the generated dance through subjective assessment
in a survey, and through dialogue and embodiment exercises with experienced
dancers and choreographers. This work broadens our understanding of how
leveraging the ability of AI to go beyond a mere replica of human movement can
itself lead to engagement and inspiration. By engaging in research with dancers,
we provide a stepping stone for building a thorough understanding of the impact
of AI-generated art on a creative ﬁeld.
This thesis consists of six publications and contributes an open-access motion
capture data set of improvised dance as a resource to be used in future research
and artistic practice.
iii
Sammendrag
Vår subjektive opplevelse av kvalitet i kunst reiser både tekniske og ﬁlosoﬁske
spørsmål om KI-generert kunst. I denne avhandlingen utgjør utviklingen og
evalueringen av KI-generert dans konteksten for utforskning av disse spørsmålene.
Dans er en mangefasettert kunstform. som spiller en rolle i alle menneskelige
kulturer og mange aspekter av livet: i ritualer og i feiring, improvisert og
koreografert, som sosialt lim og som personlig uttrykk. Dans og bevegelse er
også en rik og kompleks datakilde. Hovedmålene med denne avhandlingen er
å forstå mer om hvordan dyp læring kan brukes til å fange fremtredende trekk
ved bevegelse, spesielt dansebevegelse, ved hjelp av motion capture data. Vi
utforsker også hvordan slik teknologi kan gagne danseres kreative prosesser. Vårt
arbeid inkluderer samling av et datasett som inneholder improvisasjon utført
av individuelle dansere, samt implementering av generative modeller som er
trent på disse opptakene. Vi evaluerer modellens evne til å lære det komplekse
forholdet mellom lyd og bevegelse og undersøker den generative allsidigheten
til modellen. Vi utforsker også den genererte dansen subjektivt gjennom en
spørreundersøkelse, samt gjennom dialog og fysiske øvelser med erfarne dansere
og koreografer. Dette arbeidet utvider vår forståelse av hvordan vi kan utnytte
KI’s evne til å overskride begrensningene som menneskelig bevegelse har, og
gjennom dette skape engasjement og inspirasjon.
Ved å samarbeide med dansere i forskningen, skaper vi et springbrett for
å bygge en bred forståelse av innvirkningen som KI-generert kunst har på et
kreativt felt. Denne avhandlingen består av seks publikasjoner og et åpent
tilgjengelig datasett med bevegelsesopptak av improvisert dans som kan brukes
i fremtidig forskning og kunstnerisk utfoldelse.
v
Preface
This thesis is submitted in partial fulﬁlment of the requirements for the degree
of Philosophiae Doctor at the University of Oslo. The research presented here
was conducted at the University of Oslo, under the supervision of Charles P.
Martin, Kristian Nymoen and Jim Tøressen. In 2022 the author had a research
stay at the Creative Computing Institute at the University of the Arts London
supervised by Professor Rebecca Fiebrink. This work was partially supported
by the Research Council of Norway through its Centres of Excellence scheme,
project number 262762. The thesis is a collection of six papers, presented in
chronological order of writing.
Acknowledgements
I am lucky to have many people whom I would like to thank for their help
and support.
First of all, I would like to thank my supervisors Charles P.
Martin, Kristian Nymoen and Jim Tøressen for their guidance and motivation.
In particular, I would like to thank Kristian Nymoen, who served as my main
supervisor for the ﬁrst half of my work and continued to be a great support and
resource throughout; Jim Tøressen for sharing his experience and supporting my
work; Charles P. Martin for mentoring and motivating me and for many hours
of discussions even while being on opposite sides of the planet. I would also like
to thank Rebecca Fiebrink for her thoughtful input and for hosting me at CCI
in London.
My past and present colleagues at ROBIN and RITMO have made a great
work environment. I would like to extend my sincere thanks in particular to my
friends in oﬃce 4411, Emma and Frank who made sure there was also time for
creativity and play.
Thank you to Alireza Borhani, my coach at TEDx Arendal for helping me
present my work and ideas and for working with me to explore the themes which
have shaped my thesis in the following years.
Finally, I would like to thank my family and friends; My parents, Hege and
Petter, and my sister Karoline for their support and unwavering faith in me,
Bendik for being a source of great inspiration and my partner Magnús for his
many contributions to my work and for his patience. This work would not have
been possible without you. Should I have forgotten to mention anyone, please
accept my apologies and sincere gratitude.
Benedikte Wallace
Oslo, August 2023
vii
List of Publications
Paper I: Tracing from Sound to Movement with Mixture Density
Recurrent Neural Networks
B. Wallace, C.P. Martin, K. Nymoen.
6th International Conference on Movement and Computing
DOI: 10.1145/3347122.3371376
Paper II: Towards Movement Generation with Audio Features
B. Wallace, C.P. Martin, J. Torresen, K. Nymoen.
11th International Conference on Computational Creativity
arXiv: 2011.13453
Paper III: Exploring the Eﬀect of Sampling Strategy on Movement
Generation with Generative Neural Networks
B. Wallace, C.P. Martin, J. Torresen, K. Nymoen.
Artiﬁcial Intelligence in Music, Sound, Art and Design: 10th International
Conference, EvoMUSART
DOI: 10.1007/978-3-030-72914-1_23
Paper IV: Learning Embodied Sound-Motion Mappings: Evaluating
AI-Generated Dance Improvisation
B. Wallace, K. Nymoen, J. Torresen, C.P. Martin.
The 14th ACM Conference on Creativity and Cognition
DOI: 10.1145/3450741.3465245
Paper V: Embodying an Interactive AI for Dance Through Movement
Ideation
B. Wallace, C.Hilton, K. Nymoen, J. Torresen, C.P. Martin, R. Fiebrink.
Accepted at the 15th ACM Conference on Creativity and Cognition
Paper VI: Breaking from Realism: Exploring the Potential of Glitch
in AI-Generated Dance
B. Wallace, K. Nymoen, J. Torresen, C.P. Martin.
Under review (Journal)
ix
List of Publications
Papers written during the PhD, but not included in the
thesis:
• Noori, F. M., Wallace, B., Uddin, M. Z., & Torresen, J. (2019). A robust
human activity recognition approach using openpose, motion features, and
deep recurrent neural network. In Image Analysis: 21st Scandinavian
Conference, SCIA Springer International Publishing. DOI: 10.1007/978-3-
030-20205-7_25
• Wallace, B., & Martin, C. P. (2019). Comparing Models for Harmony
Prediction in an Interactive Audio Looper. In Computational Intelligence in
Music, Sound, Art and Design: 8th International Conference, EvoMUSART.
Springer International Publishing. DOI: 10.1007/978-3-030-16667-0_12
• Poveda Yánez, J., & Wallace, B. (2021). Using motion capture technologies
to assess the degree of similarity for intangible cultural heritage expressions.
In 1st International Online Conference on Digital Transformation in Culture
and Education (DTCE). DOI: 10.5281/zenodo.5153235
• Bentsen, L. Ø., Simionato, R., Wallace, B., & Krzyzaniak, M. J. (2022).
Transformer and LSTM Models for Automatic Counterpoint Generation
using Raw Audio. In Proceedings of the SMC Conferences. Society for
Sound and Music Computing. DOI: 10.5281/zenodo.6797651
• Erdem, C., Wallace, B., & Jensenius, A. R. (2022).
CAVI: A Coad-
aptive Audiovisual Instrument–Composition. In Proceedings of the In-
ternational Conference on New Interfaces for Musical Expression. DOI:
10.21428/92fbeb44.803c24dd
• Toverud Ruud, M., Hisdal Sandberg, T., Johan Vedde Tranvaag, U.,
Wallace, B., Mojtaba Karbasi, S., & Torresen, J. (2022), Reinforcement
Learning Based Dance Movement Generation. In Proceedings of the 8th
International Conference on Movement and Computing (pp. 1-5) DOI:
10.1145/3537972.3538007
x
Contents
Abstract
iii
Sammendrag
v
Preface
vii
List of Publications
ix
1
Introduction
1
1.1
The subjectivity challenge
. . . . . . . . . . . . . . . . . .
3
1.2
Research questions and objectives . . . . . . . . . . . . . .
6
1.3
Research contributions and publications
. . . . . . . . . .
7
1.4
Thesis outline . . . . . . . . . . . . . . . . . . . . . . . . .
8
2
Background
11
2.1
Dance and technology . . . . . . . . . . . . . . . . . . . . .
11
2.2
Generative AI . . . . . . . . . . . . . . . . . . . . . . . . .
13
2.3
Movement computing . . . . . . . . . . . . . . . . . . . . .
16
2.4
Computational creativity . . . . . . . . . . . . . . . . . . .
19
2.5
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . .
21
3
Methods
23
3.1
Data set
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
23
3.2
Preparing the data set for deep learning
. . . . . . . . . .
25
3.3
Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
28
3.4
Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . .
30
3.5
Ethical considerations . . . . . . . . . . . . . . . . . . . . .
32
4
Summary of papers
35
4.1
Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . .
35
4.2
Papers
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
37
4.3
Contributions
. . . . . . . . . . . . . . . . . . . . . . . . .
44
5
Discussion
47
5.1
Research questions
. . . . . . . . . . . . . . . . . . . . . .
47
5.2
Limitations and future work . . . . . . . . . . . . . . . . .
49
5.3
The impact of subjectivity . . . . . . . . . . . . . . . . . .
53
6
Conclusions
55
xi
Contents
Bibliography
59
Papers
69
I
Tracing from Sound to Movement with Mixture Density
Recurrent Neural Networks
73
II
Towards Movement Generation with Audio Features
79
III
Exploring the Eﬀect of Sampling Strategy on Movement
Generation with Generative Neural Networks
85
IV
Learning Embodied Sound-Motion Mappings: Evaluating
AI-Generated Dance Improvisation
103
V
Embodying an Interactive AI for Dance Through Move-
ment Ideation
113
VI
Breaking from Realism: Exploring the Potential of Glitch
in AI-Generated Dance
125
xii
Chapter 1
Introduction
The potential of using Artiﬁcial Intelligence (AI) to generate creative artefacts
has been discussed since the earliest days of computers. Ada Lovelace wrote
in the 1800s about the potential for Babbage’s analytical engine to generate
music [78]. She rightly predicted that enabling the engine to operate on pitch
and the laws of harmony would result in computers being able to “compose
elaborate and scientiﬁc pieces of music of any degree of complexity or extent.”1In
recent years there has been an inﬂux of AI research and applications in creative
domains, from music to visual art, to the ephemeral embodied practice of dance.
These AI applications range from assistive tools to algorithms which themselves
produce creative artefacts. With the entrance of AI into these ﬁelds come many
philosophical questions and technical challenges. Later in this chapter, I will
introduce the term the subjectivity challenge with the aim of teasing apart some
of these issues. In this thesis, I explore the creation and potential of AI-generated
dance. Dance involves multifactorial sensing of your environment, the space,
other bodies or objects, sounds, and your biological aﬀordances and limitations.
1from note A in Sketch of The Analytical Engine Invented by Charles Babbage, from the
Bibliothèque Universelle de Genève October 1842 No. 82 by Luigi Frederico Menabrea
1
1. Introduction
My work has investigated to what extent an artiﬁcial intelligence can learn these
complex interactions, and how AI-generated dance might integrate into dance
practice. I have approached this goal by collecting a novel data set from real
dancers, creating generative models of dance based on this data, and working
with dancers to explore their expectations and ideas for generative AI models of
dance.
Foster [37] deﬁnes a generative AI model as a probabilistic model which
describes how a set of data is generated. By sampling from such a model, we
can generate new data. Prompt-based generative applications for images such as
DALL-E [101] and Midjourney [49] have made it possible to use large-scale image
generation models to create highly detailed images from text prompts through
your browser, and text generation models such as chatGPT [94] are now able to
generate comprehensive text. One of the reasons behind these improvements in
generative AI is the development of deep neural networks and sequence prediction
models trained on vast amounts of data. Deep neural networks are capable
of learning high-level, abstract features from collections of data and sequence
prediction models make it possible to estimate future events based on previously
seen data. The predictive power of deep learning methods has been the main
motivation for choosing this approach in this thesis. While these methods were
applied to many tasks over the last decades, the modelling of artistic data using
deep learning has long been seen as a particular challenge due in part to the
complex and multifaceted nature of artistic pursuits.
Dance exists in many forms and constellations, from solo performances to
pairs and large groups. In this work, the focus has been on individual dancers
and their process of dance creation and improvisation. This narrowing of scope
still presents a large challenge. A complex interweaving of past and present
impressions aﬀect the movements a dance artist conveys during improvisation
and choreographic exploration. One of the eﬀects we have investigated in this
work is the connection between dance and music. While dance can be separate
from, and fully complete without the addition of music, music can prompt
movement in our bodies [41]. From novice to expert dancer, we all move to
music in some way or another. Some might say they can’t dance, yet still tap
their feet or bob their heads when music is playing. In this sense, dancing is
intrinsically human. It is something we can be taught, but it is also something
we do naturally. How we move also conveys information to our surroundings.
The expressive nature of movement is evident in our everyday lives. Many
can relate to the feeling of knowing that a friend or partner is annoyed or stressed
simply by the sound of their footsteps approaching or by how they close a door
behind them. Previous research has shown how simple movement patterns such
as gait [106] and arm movements [98] could be used to identify internal states
such as mood. The expressive power of movement is perhaps even more clear
in dance, with studies showing that the emotional state of the dancer can be
accurately identiﬁed by observers [120, 17]. Improving our understanding of,
and the generation of expressive movement in virtual and physical agents may
also have an impact outside of dance. In a world lived increasingly online, the
need for expressive avatars has also grown [72]. Interacting with virtual agents
2
The subjectivity challenge
and each other online suddenly became the norm in many places on Earth in
early 2020 with the outbreak of the COVID-19 pandemic. Lockdown and social
distancing also meant more people were drawn to video games and online social
events for their downtime. In games such as the massively popular Fortnite [36],
it is common for players to purchase avatar movement sequences, or “emotes”, to
personalize their online character. Being able to emulate expressive movements
will be beneﬁcial in human-robot interaction and the development of virtual
agents [18, 73], as the ability to eﬃciently convey intent may aid in establishing
trust.
The interdisciplinary nature of working with expressive data, artistic practice
and computer science can at times lead to awkward approximations of the
composite motivations and processes involved in art-making.
Assumptions
and simpliﬁcations are however necessary to begin to probe these multifaceted
concepts. To aid in framing the many complexities surrounding AI-generated
art, we attempt to clarify the aims of our work by deﬁning the subjectivity
challenge. The term stems from my own journey of exploring the motivations
and contributions of my work and was ﬁrst presented in my 2021 TEDx talk
[122].
1.1
The subjectivity challenge
Finding an appropriate deﬁnition of creativity, both in humans and machines, is
a complex task and something I will not spend too much time on in this thesis.
But questions of this nature will undoubtedly arise when working with generative
AI. It is not the aim of this thesis to deﬁne a general measure of quality for
creative artefacts, or even for dance in particular. Still, these questions have
guided me in my choice of evaluation methods and deep learning algorithms. I
deﬁne the subjectivity challenge as an umbrella term for:
challenges related to teaching a machine to generate artistic output
that has value.
These challenges can be both philosophical; related to understanding the qualities
and values of art and art creation, and technical; related to the engineering
methods for generating artistic output. This concept builds on work from the
computational creativity ﬁeld, wherein many have contributed to discussions on
the topic of human and machine creativity [11] and developed frameworks to aid
in evaluating the creative merit of AI-generated art [126]. A more comprehensive
overview of this ﬁeld is given in the background chapter of this thesis, but here I
will highlight the two questions in particular that have most directly impacted
my work.
What makes an artistic output "good"?
This question pinpoints the main aspect of the subjectivity challenge. It is one
which, in its nature, is subjective. Diﬀerent people will appreciate artworks
3
1. Introduction
diﬀerently depending on their personal preferences. Attempting to deﬁne a
single, precise metric to decide whether or not an artistic output has value will
likely be either deeply limiting or ﬂawed. This question also reveals several
technical challenges, such as the following; How do we deﬁne a quality metric in
the training of generative models of art?
To train a generative AI model, some deﬁnition of success must be given so that
the model can update its predictions in order to approximate some presumably
correct answer. Deﬁning a single correct goal for a generative AI model will
likely result in output that is mundane to some and too unusual for others.
Ritchie [102] proposes that typicality is an important ﬁrst step in evaluating
the creativity of AI-generated artefacts. The artefacts need to be recognizable
as dance ﬁrst of all. Only then can we move to the next question: is it good?
With the latest research on AI-generated dance, it seems that this ﬁrst goal
may have been met. If we assume that we can build realistic generative models
of dance, then the question which remains is whether we can build generative
models of dance that are useful and interesting. To explore this question we must
acknowledge its subjective nature. When selecting a deep learning architecture
for this project, an important factor was the model’s ability to generate variations
and allow for some randomness. As I wanted to avoid allowing too much of
my own subjective preference to aﬀect the model, it has also been important
to me to restrict the number of constraints put on the model regarding what
movement should look like. When selecting features from the data set and
when visualising the model’s output no limitations were added to the generated
movement sequences. Instead, the model should learn to extract any biological
constraints from the underlying data. Training a model on a data set of examples
which might be considered by many to be high-quality examples of a given genre
should indicate that the model will successfully generate examples which have
an objective level of quality, producing novel output while still conforming to the
norms of the examples in the data set. However, the balance between novelty
and typicality will be judged diﬀerently by individual observers.
What should be the aim of a generative model of art?
Depending on the individual, the answer to this question may vary greatly. We
may begin to ask why we should strive to create good generative AI for art
creation. We may think of art as something humans create compulsively, for
their own enjoyment or due to a need for expression. Why then, would we wish
to outsource this to computers? Who is the generative model for? Depending
on our reasoning around these questions, another technical question becomes
apparent: How do we evaluate the success of the model? This ties into the
previous technical question presented but points more towards how we might
establish that the chosen metric does, in fact, result in output which exhibits
some form of quality. Exactly who should perform this evaluation? Who are the
stakeholders? Deciding on the aim of the generative AI seen through this lens of
who will be using it is an important decision which will shape how we evaluate
the model and its output.
4
The subjectivity challenge
d’Inverno et al. describes two types of AI approaches in art: heroic AI and
collaborative AI [27]. Heroic AI aim to automate the entire creative process
while collaborative AI should aim to contribute to an artist’s creative practice, to
function as a suggestion-giver or provide feedback and reﬂection. By considering
the case of Fortnite dances outlined above as the aim of the model, then the
opinion of dance practitioners may be of little interest compared to the interests
of players. In this case, the interests of dancers may be directly threatened by
the development of autonomous dance creation systems. An apt comparison
may be the current state of text and image generation models. Text generation
models, able to preserve context over several text paragraphs, have improved
chatbots and automatic text generation to the point where many now fear
that regulatory legislation will not be capable of keeping up with the rapidly
improving technology 2. Text indistinguishable from human text can potentially
lead to damage to academic integrity [95] and high-ﬁdelity image generation can
lead to issues of deep fakes [60]. If we instead want to create collaborative AI, the
inclusion of dance practitioners is crucial. However, it can be a time-consuming
and diﬃcult task. Including a human in the training loop to evaluate each output
would cause massive delays (and it would surely be a tedious job). Additionally,
even experts in a creative ﬁeld may disagree in their evaluation of an artefact.
When considering the question of the aim of a generative model within
the scope of this thesis, three main motivations emerge.
The ﬁrst is the
augmentation of human creativity, creating models that function as creative
catalysts or collaborators, oﬀering suggestions and reﬂection of our work [27].
The second is the democratisation of art, building applications that can assist
people that don’t necessarily have artistic training in exploring and succeeding
in creative pursuits [110]. The ﬁnal motivation is perhaps the most elusive
and would be better described as a hope than as a goal. That is the hope
that attempting to build AI that can create interesting pieces of art will reveal
something about ourselves, our biological systems for creativity and art making,
through an analysis-by-synthesis approach. Generative AI could point us in the
direction of a deeper understanding of what mechanisms might be beneﬁcial in
the creation and appreciation of art.
I have held these questions as a guide for the direction and focus of my
research. Subjectivity implies that our opinion of an artefact might change based
on what view we are taking. As such, our suggested solution to the subjectivity
challenge is to attempt to consider multiple views when developing and evaluating
our generative AI dancer to paint a more complete picture. Throughout this work,
I have outlined three main areas of research, or perspectives, on AI-generated
dance: Mappings, control and practice. We examine the model’s ability to
learn the relationship between sound and motion in dance (here referred to as
sound-motion mappings), we focus on the model’s generative power and capacity
for the control of variation after training and ﬁnally we engage with experienced
dancers by implementing AI-dance into their practice. The notions of mappings,
2BBC recently reported on Italy’s ban of the advanced chatbot, ChatGPT. https:
//www.bbc.com/news/technology-65139406
5
1. Introduction
Figure 1.1: This ﬁgure shows how the research questions and thesis title are related
to the six papers that comprise the research contribution of this work. While both
AI-generated dance and aspects of the subjectivity challenge form the motivation for
each of the papers, the papers in the top row have focused on technical aspects of
AI-generated dance and the lower row contains papers where evaluation of the generated
output and exploiting the model’s ability to generate variation are most central.
control and practice are presented and detailed further in the form of the research
questions for the thesis below.
1.2
Research questions and objectives
This thesis consists of six publications. As an initial step in this research, I
examine the notion of sound-motion mappings and how this might manifest
in AI-generated movement and dance. By training a generative model on our
collected data set we begin to explore various methods for aﬀecting the generated
movements. Throughout the work, the aim of understanding how AI-generated
dance might aﬀect and potentially beneﬁt a dancer’s practice has been an ongoing
theme. The sections below describe the research questions in further detail. An
overview of how each research question relates to the included publications and
the two elements of the thesis title can be found in ﬁgure 1.1
RQ1: Mappings: What can AI generated dance reveal about sound-
motion mappings?
When we experience music we often feel the urge to move, and diﬀerent
sounds inspire us to move in diﬀerent ways. Previous work has indicated a
tendency for shared strategies for sound-motion mappings in music-inspired
motion. Are these mappings between sound and movement reproducible
by an artiﬁcial neural network? Or would our individual idiosyncrasies
overshadow any shared tendencies? Paper I formed the initial steps in
examining the ability of an AI model to emulate the tendencies of the
sound-motion mappings observed in the participants of a sound-tracing
6
Research contributions and publications
study. In paper IV this was evaluated qualitatively through a perceptual
judgement survey and quantitatively by comparing motion features from
the output of the model to the training data. Here, participants rate the
musical ﬁt of AI-generated and human dance set to music.
RQ2: Control: How can we inﬂuence the model outcomes?
Inﬂuencing the generative output of a model can be done at various stages,
prior to and during training and during inference. When it comes to the
choice of approach to modelling dance using AI there are a multitude of
variables to consider, from the choice of machine learning model to which
motion and sound features to include. For end users of a generative system,
the most relevant control for adjusting the model comes into play once it
has been trained. Users can inﬂuence the model by adjusting parameters
which aﬀect the output of the model during inference. The eﬀect of altering
the diﬀerent parameters can vary based on training and model choices,
and while some adjusting of these parameters is commonly performed, this
process is not often included in presentations of a generative model. To
understand the limits and aﬀordances of the chosen AI model and data
features papers II and III describe the systematic exploration of diﬀerent
sampling procedures.
RQ3: Practice: What role could an AI dancer play in the creative
practice of dance?
The quantitative measures of evaluating the model’s accuracy and perceived
realism can only reveal part of the picture surrounding generative AI in
dance. To gain a more well-rounded understanding of the promise of this
approach it becomes crucial to explore how dancers themselves perceive
generative AI and its potential use in their practice. Does a generative
model of dance have a place in dance practice? The use of generative AI
as a creative catalyst has been proposed in many related works on AI-
generated dance. Could an AI dancer augment the creative process of dance
composition in some way? These questions are examined by introducing
dancers to AI-generated dance examples through bodystorming, speculation
and integration into their practice. Papers V and VI describe the ﬁndings
of an improvisational workshop and an in-depth user study. These papers
discuss dancers’ perspectives on the potential use case for generative AI in
their work as choreographers and teachers.
1.3
Research contributions and publications
The research questions have been approached in three main stages: Collection
of a motion capture data set of improvised dance performances set to music,
training and parameter tuning of generative models trained on this data set and
ﬁnally evaluation of the models’ output through both qualitative and quantitative
methods. The contributions are summarised in short here.
7
1. Introduction
Improvised Dance data set: DeepDance A data set of dance improvisation
performed to six diﬀerent musical excerpts was recorded as part of this
thesis.
The DeepDance data set contains full-body motion capture
recordings of 30 dancers. In total, the data set contains 9 hours of motion
capture data. The data set is available as an open-access data set in two
versions [123]. Deepdance v.01 contains the subset of the data which has
been used in our publications. This version contains 164 motion capture
ﬁles, six music ﬁles and code for visualisation and processing. Deepdance
v.02 contains the entire 9-hour data set and code for visualisation.
Exploring AI-Generated Movement Exploration of sound-motion mappings is
at the centre of Paper I which focuses on the generation of sound-tracings.
In the following publications, the DeepDance data set is used to train
a deep neural network. Paper IV presents the ﬁndings of a perceptual
judgement survey wherein respondents rate the perceived dance-likeness,
musical ﬁt and expressivity of AI-generated dance. Various methods for
controlling AI-generated movement have been explored in the context of
mixture density recurrent neural networks in Paper II and Paper III as
well as in Paper VI through a simple browsing interface.
Understanding Dancers’ Experience The ﬁnal two publications contribute in
particular to the understanding of the importance of close collaboration
with practitioners when developing generative AI as creative tools. Paper V
and Paper VI document dancers’ experiences with, and reﬂections on, a
generative AI model of dance. Through these reﬂections, we distil promising
avenues for the future development of AI-generated dance which focus on
facilitating surprise and serendipitous moments of discovery by leveraging
the non-human artefacts which emerge from AI.
1.4
Thesis outline
In the following chapter relevant background and related works are presented,
Chapter 3 contains details of data collection and processing of the DeepDance
data set and presents descriptions of the methods I have used in my work.
A summary of the papers included in this thesis is presented in Chapter 4,
followed by Chapter 5 which contains a discussion of my ﬁndings, their scope
and limitations and suggests future avenues of research. The thesis is concluded
in chapter 6. Throughout this document, you will also ﬁnd occasional blue note
boxes. They provide further context on background material outside of the main
scientiﬁc contributions of the thesis. Here is the ﬁrst note box, note 1:
8
Thesis outline
Note 1:
AI vs ML vs DL
The terms AI (Artiﬁcial Intelligence), ML
(Machine Learning), and DL (Deep Learning)
are often used interchangeably, which some-
times leads to confusion. These terms are not
synonymous and refer to diﬀerent concepts.
Artiﬁcial Intelligence refers to the broader
concept of machines being able to carry out
tasks that would normally require human
intelligence. This includes things like speech
recognition, image classiﬁcation, decision-
making, and natural language processing.
Machine Learning is a speciﬁc subﬁeld of AI that focuses on the
development of algorithms that allow a computer to learn from
data, without being explicitly programmed.
Deep Learning is a subﬁeld of Machine Learning that uses ar-
tiﬁcial neural networks with multiple layers to model complex
relationships between inputs and outputs. Deep Learning algo-
rithms are particularly well suited for tasks where the data has a
high degree of complexity.
In this thesis, and my papers, the term AI is used most frequently.
However, deep learning is a more precise description of the method
I have used.
9
Chapter 2
Background
As this thesis is an interdisciplinary work, the background consists of research
from several ﬁelds, including computer science, humanities and art. While this
is not an exhaustive collection of related work, it contains an overview of the
research areas and works that have inﬂuenced my work most directly and formed
the background for my papers.
2.1
Dance and technology
The intersection of dance and computer technology goes back several decades.
In the mid-20th century, avant-garde artists and composers began experimenting
with electronic music and new technologies such as ﬁlm, video, computer graphics
and motion capture. Pioneer choreographers such as Merce Cunningham took
part in the development of software for visualising choreography [108], Phillipa
Cullen experimented with analogue hardware, using pressure-sensitive ﬂoors and
theremins in her work [55] and William Forsythe explored the use of graphical
augmentation in teaching [35]. David Rokeby’s Very Nervous System (1986-
1990) [104] combined cameras, image processing units and synthesizers to create
an interactive space wherein movement created sound. These examples are just
a few of the pioneers that have used new media to create forms of dance that
incorporated elements of light, sound, and motion technologies [26].
With the proliferation of artiﬁcial intelligence and more advanced wearable
sensors in recent years, a new wave of dance technologies has emerged, particularly
focusing on cross-modal interaction. In the project vrengt, Erdem et al. [28]
create a performance with a "shared instrument" which is aﬀected by the dancer’s
movement through Myo armbands recording EMG signals, and a microphone
recording their breath. The sensor information is then processed by a musician
working in Max/MSP to produce sound. Research into AI systems for movement
interaction includes the use of AI in interactive installations using gesture
recognition [65, 76] or mapping movement to the control of various media
such as sound and video, in VR [97] and on stage [31]. This also includes
physical interactions with robotics. Erikson et al. [29] explore the use of ten
drones (named The Aerial Robotic Choir) in an opera performance, wherein
performers control the drones through motion capture sensors. In 2010 Wallis
et al. developed SpiderCrab [124], a large robotic arm that moves within a set
repertoire of interaction strategies based on sensory data from armbands worn
by participants. While the lack of novelty in the robot’s movement was deemed
the cause of eventual disinterest in the dancers, the unfamiliar shape of the robot
was largely intriguing to participants. Several of the dancers interacting with
SpiderCrab reported a perceived friendliness in the robot. Non-human forms in
11
2. Background
robotic dancers have also been explored by Gemeinboeck and Saunders [40]. The
pair develop an autonomous robot with an abstract form that learns to move in
ways that are unique to its own body. This is achieved through motion capture
recordings of dancers wearing the robot shape as a costume, or prosthesis.
The relationship between technology and dance is complex and debated. To
some, computers represent the antithesis of dance, to others computer technology
is seen as a potentially valuable tool [9]. Regardless, computer technology is
pervasive throughout many aspects of the ﬁeld of dance. In addition to the
interactive systems presented in this section, technology has been developed to
support annotation, transcription, teaching and the creation of choreography.
2.1.1
Generating choreography
Note 2:
Rule-based
choreography
and improvisation
Rule-based improvisation forms
an interesting parallel, or juxta-
position, to computer-generated
dance. Terry Riley’s 1964 compo-
sition In C consists of 53 musical
phrases which the musicians play
in order. Each musician can how-
ever choose to repeat each phrase
as many times as they wish before
moving on to the next, as long as
they stay within 5 phrases of their
fellow musicians. In 2021 chore-
ographer and dancer Sasha Waltz
created a series of 53 movement
phrases for her choreography, also
named In C, which follows the
same variable structure and strict
rule set. Both the musical piece
and the choreography were cre-
ated to be always unique, ever-
changing and purposefully unﬁn-
ished.
One of the earliest records of the use of
computer-aided choreography comes from
Michael Noll’s experiments with computer
choreography at Bell Labs in the 1960s
[90]. These systems were predominantly
used by choreographers and functioned by
appending a series of available poses to
construct a choreography, either manu-
ally or by use of randomness as in chance
choreography [24]. Almost 50 years later,
choreographer Wayne McGregor collabo-
rated with Google to create Living archive
[82], a tool for exploring McGregor’s collec-
tion of choreographies as clusters of related
poses. Users can access the archive online
and choose diﬀerent poses through their
browsers. The system then plays back a
new movement sequence by interpolating
between the selected poses. Carlson et al.’s
system Cochoreo [23] uses a genetic algo-
rithm to construct a series of poses. This
approach has also been explored in previ-
ous work by Lapointe et al. in the Danc-
ing Genome Project [67]. Carlson’s ﬁtness
function encompasses heuristic rules based
on the authors’ expertise in contemporary
dance practice.
Users of Cochoreo can
thereby inﬂuence the next generation of
poses by selecting features through an interface. As with Merce Cunningham’s
work with the DanceForms system, these approaches require the choreographer
to make decisions using the computer interface. Berman et al. [6] create an
AI for interactive performance, wherein the AI produces novel movements by
12
Generative AI
randomly traversing a pose map generated by a shallow neural network on the
performers’ movements in real time.
The overview of dance applications given in this chapter is by no means
exhaustive. A supplementary review of systems for supporting various aspects
of dance practice was published by Zhou et al. in 2021 [129]. Bisig’s 2022 review
also gives a structured overview speciﬁcally focused on generative dance systems
[8]. In addition to systems that require human input or follow pre-determined
algorithms to generate movement, there are also data-driven approaches that
utilize larger data sets of dance to learn to generate realistic movement sequences
with little or no human interference using deep learning and generative AI.
2.2
Generative AI
Artiﬁcial neural networks (ANNs) were conceptualised in 1943 with the
perceptron [81], a simpliﬁed version of a biological neuron.
Each artiﬁcial
neuron has one or more inputs with weights associated, a threshold or activation
function and one or more outputs. An ANN is constructed by organising the
artiﬁcial neurons into layers where the output of one layer is fed forward to the
next, from input to output. By measuring the diﬀerence between the output of
the network and the true value an error term is calculated. This error term can
then be propagated back through the connecting layers in a process called back-
propagation, using the gradient (or size of the error) to update the associated
weights of each neuron.
The term generative artiﬁcial intelligence does not refer to a single kind of
ANN architecture or method. Rather it refers to a way of using AI to create,
wherein the AI itself produces an output which is similar to but not identical to
an underlying data set [37]. While our deﬁnition of generative AI is dependent
on the model’s ability to create, this does not mean generative AI is solely
used in creative domains. Generative AI has applications extending to facial
recognition software [52], image up-scaling [70] and voice synthesis [93]. But as
with many technological advances, creators tend to ﬁnd a way of using emergent
technologies in creative pursuits, such as using 3D printers to create sculptures
[50] or using early computers to generate music [112]. Generative AI art is a
form of computer-generated art, deﬁned by Boden et al. as artworks “where the
artwork results from some computer program being left to run by itself, with
minimal or zero interference from a human being.”[12] The complexity of such
a system can vary, from simpler programmatic approaches which use a small
set of rules, to deep neural networks trained on millions of examples. Figure
2.1 shows two examples of generative visual art. The leftmost image shows
a work by Vera Molnár–an artist who, in 1968 began working with the early
programming languages Fortran and Basic to generate algorithmic paintings [85],
the other shows an image created using DALLE-2. Here we will mostly focus on
the somewhat narrower concept of deep learning-based generative AI.
The most common model architectures used in generative AI are deep learning
models such as the Generative Adversarial Networks (GAN) [43], Transformers
13
2. Background
(a) Vera Molnàr Computer-rosace,
74.338-54., 1974
(b) DALL-E 2 via the text prompt: A
1970s abstract artwork titled
"Computer-rosace"
Figure 2.1: Computer-generated visual art has a long history, with pioneers such as
artist Vera Molnàr creating images using early programming languages and a plotter
machine (a). For me to create the image on the right required signiﬁcantly less eﬀort.
The deep learning-based image generation model DALL-E did most of the work here.
[121], VAE (Variational Auto-Encoders) [62] and various Recurrent Neural
Network (RNN) types, perhaps the most common RNN being the LSTM [48].
GANs in particular are often used in image generation tasks [114] as well as
transferring the style of one image to the content of another [57]. VAEs have
been used particularly in style transfer of images and music. Google’s Magenta
team developed the MusicVAE, which allows users to explore the latent space
between encodings of two melodies [103]. Interacting with the MusicVAE allows
users to blend two musical scores to create new melodies. The transformer and
RNN derivatives, however, are used mostly in sequence modelling (though not
exclusively, DALL-E [101] the text-to-image generator from OpenAI, for example,
is based on the Generative Pre-trained Transformer). Many creative tasks can
be modelled as sequence generation problems. The most prevalent are music and
text, which are often modelled as a series of discrete tokens. Computer-generated
music and text sequences have a long history. Examples of automatic music
composition systems go back to the 1950s, with Hiller’s experiments using the
ILLIAC computer at the University of Illinois. By using a random integer
generator paired with arithmetic analogues of compositional rules, this digital
computer produced a four-voice counterpoint piece later entitled the Illiac Suite
for String Quartet [47]. Today the Illiac Suite is considered the ﬁrst example of
computer-generated music. Similarly, ELIZA, a text-parsing program developed
by Weizenbaum in the 1960s became known as the ﬁrst chatbot [125]. In the
21st century, we have also seen sequence prediction models used to generate
real-valued data such as audio waveforms [93], simple sketch drawings [45],
handwriting [44] and dance movement [75]. In the following sections, we look
14
Generative AI
closer at the most prevalent sequence prediction models used in generative AI
for creative tasks currently and in my own work.
2.2.1
Sequence prediction
The recurrent neural network (RNN) is a type of artiﬁcial neural net that has
recurrent connections internally in each neuron. Current internal states are
passed along to the next time step through the recurrent connection, often
called a carry gate.
This internal state is used to generate the output for
that step of the sequence and the recurrent connection allows the state from
the previous time step to aﬀect the current state. The recursive updating of
the neuron’s internal state allows the RNN to capture dependencies between
elements in a sequence, such as the context of a word in a sentence. The ability
of RNNs to hold a short-term memory state makes them well-suited for sequence
modelling and prediction, where previous states inform future predictions.
Note 3:
AI-completeness
The concept of a task being AI-
complete or AI-hard
a is an in-
formal term that refers to prob-
lems where we suspect that AI
would have to reach human-level
– or "general" – intelligence to
solve it.
Perhaps surprisingly,
perhaps not, many of the tasks
that are often considered to be
possibly AI-complete are the ac-
tions that come the most natu-
rally to humans, like music, hu-
mour, conversation and art. We
usually consider these tasks to be
highly creative, requiring the use
of imagination, problem-solving
and skill.
This concept is also
closely linked to “The Argument
from Consciousness” addressed by
Alan Turing in his seminal paper,
Computing Machinery and Intel-
ligence, 1950.
aRaymond, Eric S. (1991, March
22). Jargon File Version 2.8.1
One of the limitations of traditional RNNs
is that they struggle with capturing long-
range dependencies in the sequence, as the
gradients can become very small or very
large as they are passed through many
time steps. In addition to the recursive
connection which passes the previous state
forwards through time, the LSTM cell
[48] has three other gates that aﬀect the
current state: The input gate, the output
gate and the forget gate. The forget gate
allows the cell to disregard information by
clearing itself thereby avoiding the issues
caused by vanishing or exploding gradients.
This makes the LSTM easier to train
and more capable of retaining “memory”
over long sequences. Some of the initial
explorations of generating dance using
deep learning were based on the LSTM.
Crnkovic et al.’s work [25], and later
implementations by Pettee et al. [96] and
Yalta et al. [128] used LSTMs to explore
the generation of novel movements from
data sets of motion capture recordings.
The Transformer model [121] was a
breakthrough in sequence modelling. A
Transformer is a type of deep learning
model that is speciﬁcally designed for
processing time-series data such as text.
It was introduced in the 2017 paper "Attention is All You Need" by Vaswani et
al [121] and has since become a staple in the ﬁeld of natural language processing
15
2. Background
(NLP). The Transformer model is well-suited for sequence prediction tasks
because of its ability to capture both short-range and long-range dependencies in
the input sequence. By introducing the self-attention mechanism the model can
retain information across longer sequences. Earlier sequence prediction models,
such as RNNs, process the input sequence one element at a time, updating
their internal state as they go along. The Transformer, on the other hand, uses
self-attention to weigh the importance of each element in the sequence, allowing
it to attend to diﬀerent parts of the input sequence simultaneously.
The Transformer model consists of an encoder and a decoder. The encoder
takes in the input sequence and produces a sequence of encoded representations.
The decoder then takes in these encoded representations and generates the
output sequence. The self-attention mechanism is applied in both the encoder
and decoder, allowing the model to capture long-range dependencies in the input
sequence. In addition to improvements to text-generation models, the transformer
model brought forth improvements in other sequence prediction tasks such as
music [51] and movement generation [75]. In each of the aforementioned model
architectures, the output is most commonly given as a single-point estimate.
This is not necessarily ideal when working with creative or continuous data where
we may consider several predictions equally valid. Thus, rather than attempting
to predict a single truth, a model for generating creative data should be able to
generate a variety of likely outputs.
Mixture density networks (MDN) [7] are a type of neural network which
constructs mixture density models [83], probability distributions which are
composed of several Gaussian distributions. This allows the MDN to model
complex probability distributions over multiple variables. Unlike traditional
neural networks that output a single point estimate, MDNs output the parameters
of a mixture of probability distributions, giving a more complete representation of
the uncertainty of the predictions. As the MDN learns several distributions of the
data simultaneously, it is further possible to explore each of the learned Gaussian
distributions in isolation. This makes it a good choice for sequences of continuous
data such as bio-signals and movement. MDNs, in combination with various
deep learning models, have been applied to a wide range of problems. Ha et al.
[46] combine an MDN with an RNN to form a memory unit for a problem-solving
virtual agent. As the MDN allows us to sample from a continuous distribution
it has also been used in sequence prediction tasks such as generating sketches
[45] and dance movement [58] where a discrete representation could be highly
limiting.
2.3
Movement computing
The ﬁeld of movement computing encompasses a wide range of research questions
and topics, from gesture recognition [21] to sound-motion mappings of calligraphy
[107]. When recognising certain motion sequences it is often advantageous for
a system to be invariant to the way a movement is performed as the personal
characteristics of users may diﬀer greatly [22]. A good motion recognition software
16
Movement computing
would need to be robust against certain motion features such as temporal, spatial
and stylistic diﬀerences. When the target gesture is known one approach is to
create a robust description or template [87] of the movement with which you
can compare other examples, thereby enabling eﬃcient motion retrieval from
large databases of movement [88].
Movement can be represented in various ways, including position, velocity,
and acceleration. Graphical representations of movement, such as graphs and
diagrams, can be used to illustrate the trajectory and velocity of movement
over time. Mocapgrams [53] for example, present a sequence of movement as
a diagram where the position of markers are mapped to RGB values. Figure
2.2) shows an example of a mocapgram together with a key frame plot of
the same performance. Visual representations of movement can also include
animations and simulations, which can be used to demonstrate the movement
more intuitively. How we choose to present the motion capture data for viewing
also aﬀects the way observers perceive the performance. Recent work by Moura
et al. [86] show the eﬀect of diﬀerent representations in the animation of motion
data on its perceived expressiveness.
When examining human motion, particularly dance, we are interested not
only in what movement is performed but also in how it is performed. In other
words, recognizing speciﬁc gestures, poses or the steps involved in choreography,
but also the stylistic traits, or qualities, that separate one performance from
another, despite how similar the underlying steps may be. Being able to recognise
expressive qualities or transpose a movement quality from one movement sequence
to another is useful in the world of computer animation and also in human-
robot interaction [63]. Movement qualities are often associated with emotional
expression. Even the less purposefully expressive movement patterns of walking
[84, 106] and isolated arm movements [98] could potentially reveal internal
states such as the mover’s mood. Camurri et al. examined audience perception
of emotion in dance [19], concluding that participants were able to detect the
intended emotion transmitted by the dancer. Their following study using decision
trees to classify emotion in the same data showed that the algorithm was able
to achieve above chance-level correct classiﬁcation using features such as the
quantity of movement and body contraction and expansion [20]. Our ability to
detect emotion through movement has also been explored by Burger et al. [17] in
the context of dance as a medium for the emotional content of music. Their study
showed how observers could identify the emotional content of music by observing
only a silent, stick-ﬁgure representation of a dancer’s movements, indicating that
the emotion perception of movement is a multimodal phenomenon.
2.3.1
Sound-motion mappings
Music can aﬀect the way, and how much, we move [117]. Even when purposefully
trying to stand still [54], music, particularly music with a clear repetitive beat,
causes increased movement in the body [42]. How the music moves us is however
idiosyncratic, varying from person to person. Studies on the music-induced
movements of musicians show correlations between the periodic movements of
17
2. Background
(a) Key frame plot showing a skeletal view of a person dancing the macarena.
(b) The mocapgram displays the marker positions of the above performance
mapped to RGB colour space.
Figure 2.2: Key frame plot and mocapgram of a recording of a dancer performing the
macarena [77]. The bobbing of the knees is visible in both visualisations, while the
90° shift in orientation which occurs three times during the recording is most easily
observed in the three vertical blocks that can be seen in the mocapgram.
18
Computational creativity
certain body parts and the tempo of the musical stimuli [117]. However, timbre
and tonal correlations in particular can be more elusive. When examining the
correlations of certain sound features and the dance movement of participants,
Carlson et al. found that clustering performances by individuals was far easier
than clustering performances based on the diﬀerent genres presented as musical
stimuli [22]. This seems to also be the case when looking at mappings between
simpler audio stimuli and more constrained movements. Nymoen et al. performed
several experiments with participants to explore how they would solve a sound-
tracing task using modulated sine waves [91, 92] and Kelkar et al. found some
shared tendencies in participants’ approaches to using hand motions to trace
melodic vocal phrases [59].
Various research has explored the notion of generating dance using musical
features to shape dance movements. Initial attempts revealed the complexity of
this task [38], but improvements in computational models and larger data sets of
music and dance in the last 5 years have caused more successful generative models
of dance to arise, such as Li et al.’s cross-modal transformer model approach [75].
The most common sound features used in music-conditioned dance generation
are MFCCs and various other beat onset and beat strength measures [71, 115],
but tonal and melodic features such as pitch and chord patterns have also been
explored [39]. How music moves us, particularly in dance, is very individual. The
process of developing dance is personal, complex and of course highly creative.
2.4
Computational creativity
The criteria for attributing creativity to a computer, or computer program have
been considered since the computer’s conception, famously by Alan Turing in his
work describing the imitation game, now more commonly known as the Turing
test [119], and by Ada Lovelace, as mentioned at the outset of this thesis. In
this context, it is important to make a clear distinction between measuring the
creative merit of a system and measuring the creative merit of its output. Jürgen
Schmidhuber has written on the topic of what computational mechanisms might
be required to achieve a creative machine [109], emphasizing the importance
of curiosity and surprise and arguing for the use of reinforcement learning and
particularly intrinsic motivation as a promising avenue for fostering creative
problem-solving in robots and virtual agents. Margaret Boden, likewise, has
given practical deﬁnitions of three types of creativity we might observe in both
humans and machines [11], each corresponding to a diﬀerent type of surprise.
The ﬁrst involves unfamiliar combinations of familiar ideas. When we encounter
an artefact which combines familiar concepts or methods in a new way we
experience surprise due to the sudden alienation of familiar concepts. The second
type of creativity involves exploration, where the surprise we experience comes
from seeing an idea in a new conceptual space. The ﬁnal type is associated with
transformation when a new idea reshapes the conceptual space itself. This form
of creativity leaves us with astonishment as it reveals a new idea previously seen
as impossible.
19
2. Background
Boden also reﬂects on what it means to be creative. The word creativity
is itself ill-deﬁned, and diﬃcult to pin down and describe in a precise manner.
Wiggins presents a terminology that allows us to discuss creativity in the realm
of computer science by deﬁning a creative system as “A collection of processes,
natural or automatic, which are capable of achieving or simulating behaviour
which in humans would be deemed creative” [126].
Some might protest the notion of attributing creativity to a machine regardless
of the quality of its artistic output. The argument being that humans alone
can be considered creative by deﬁnition, as long as machines lack consciousness,
preference or values they can not be said to be creative (See note 3 on AI-
completeness). While the issue of ascribing creativity to an entity, machine or
human, is fraught with philosophical and psychological complexity, it is common
to think that creativity is one of the things where "we know it when we see it."
Creativity is a property we consider detectable by observation of behaviour. This
is reﬂected in how most research within the ﬁeld of computational creativity
is presented [127], as most research aimed at determining the creative merit
of a given system is based on the evaluation of the artefacts generated by the
system.
Instead of measuring the creativity of the system itself, the more
pertinent measurements say something about the creativity of the artefacts that
the system produces. A framework is suggested by Graeme Ritchie, in which
three aspects of an artefact are evaluated using a rating scheme that measures
typicality, quality and novelty [102]. Typicality is deﬁned by Ritchie as the
artefact’s similarity to existing examples of the artefact’s type. The notion of
typicality is usually not considered a measure of creativity when evaluating art,
but in the context of computer-generated creative artefacts, it functions as a
baseline indicator that the system is capable of generating examples of a given
set. Ritchie deﬁnes novelty as deviation from the set of existing examples to
which the generated artefact belongs and quality as the perceived value of the
artefact.
While there are examples of systems such as tools for generating stories
[2] being evaluated using Ritchie’s proposed method, the approach is far from
ubiquitous in the ﬁeld. This may be partly due to non-trivial thresholds and
concepts that are required to be deﬁned in the implementation of Ritchie’s criteria.
Ritchie’s method introduces the values α and γ described as the threshold values
used to determine high typicality and high value, respectively. In their work
on applying Ritchie’s criteria to three diﬀerent systems (a poem generator, a
sentence paraphraser and a system for blending visual concepts), Pereira et
al. describe these thresholds as well as the concepts of typicality and value
themselves as “troublesome”. They point out that deﬁning these terms and
thresholds is a subjective process, making Ritchie’s framework challenging to use
as a tool for comparisons between systems or as a benchmarking method. The
SPECS methodology (Standardized Procedure for Evaluating Creative Systems)
[56] aims to oﬀer a more formative evaluation, clarifying the strengths and
weaknesses of a system that can be used to guide development. The method
involves deﬁning what creativity means in the context of your speciﬁc system
and its domain but includes a description of 14 key components of creativity
20
Summary
which the author suggests using to form a baseline. The 14 components are
extracted from a literature review spanning several disciplines, from psychology
to education and computational creativity and include concepts such as Variety,
Divergence and Experimentation, Domain Competence and General Intellect.
How we might assign the rating schemes and threshold values suggested
in these methods, or how we might choose our evaluation methods in general,
is largely aﬀected by the intended purpose of the system in question. The
aforementioned research reveals useful insights into what aspects of an AI-
generated artefact or a generative AI system are important for our perception
of creativity. However, the work presented in this thesis and the accompanying
paper collection is less concerned with determining to what extent the models
used can be said to be creative, instead, we have aimed to explore whether or
not the output generated by the models can be interesting or potentially useful
within dance practice.
2.4.1
Evaluating generative AI in artistic practice
A common motivation stated in generative AI research is to use the generative
system to create inspirational sources for the ideation phase of an artist’s
practice. To determine whether a system is useful in this way, the system must
be evaluated within the context of artistic practice and through feedback from
practitioners themselves. While including in-depth feedback from practitioners
in the evaluation of a system is common in ﬁelds such as human-computer
interaction, where subjective quality assessment plays an important role in
system [89] and interaction design [33] by revealing otherwise elusive aspects of
a system, it is less common in generative AI applications. Sturm et al. argue for
taking their generative model back to music practitioners as part of evaluating
their Folk-RNN model [113], both by publishing example scores online and
through concerts where the AI-generated Irish folk music scores were performed
live by experienced musicians. Accessing feedback from practitioners is also
possible for more nascent technologies and early iterations of systems. While a
given technology may not yet be ready for use in a real-life setting, role-play or
wizard-of-oz methods can be used. Thelle et al. present one such example in their
work with musicians improvising with an “AI” counterpart controlled behind the
scenes by a professional piano player and composer [116]. This approach allows
researchers to examine how the participants experience interacting with a highly
skilled and ﬂexible AI co-creator even though technology of this level does not
yet exist. The insights gained from these interactions are valuable as they reveal
aspects of the generative models that are hard to capture quantitatively. This is
naturally also the case, perhaps even more so, within ephemeral practices such
as improvisation and performance.
2.5
Summary
One of the many places where dance and technology have intersected is AI-
generated dance. To train an artiﬁcial intelligence to generate dance sequences
21
2. Background
there are a multitude of model architectures and movement representations to
choose from. The pursuit of an AI dancer also requires an understanding of
what it means for a generative model to be useful or suitable in the context of
dance practice. Each of the ﬁelds described above forms the background for the
methods implemented in this thesis, which are further described in the following
chapter.
Note 4:
Turk or Tarot
One critique against the appear-
ance of creativity in AI (pre-
sented to me by professor Alan
Blackwell), is that what we are
witnessing is in reality either
a mechanical Turka or a tarot
reading.
In the case of the
mechanical Turk, the “intelli-
gence” springs from a hidden hu-
man intelligence, either through
strict heuristics, programming or
human-in-the-loop methods. If not, it is simply an example of ran-
dom events to which we as observers assign meaning, as in a tarot
reading.
aIllustration of the Mechanical Turk (1789). Image from [100]
22
Chapter 3
Methods
The research questions have been approached in three main stages. The ﬁrst
centred around the selection of an appropriate deep neural network for the task
of dance generation and the collection of a motion capture data set of improvised
dance performances set to music. During the second stage, we use this data set
to train the generative models. The ﬁnal stage includes evaluating the quality
of the generated dance using quantitative and qualitative metrics This chapter
details the collection of the data set and presents the approaches used to train
a deep neural network on the collected data and evaluate the model’s output.
Finally, ethical considerations associated with data collection and generative AI
are described.
3.1
Data set
The studies presented in this thesis use a data set of improvised dance collected
at the RITMO motion capture lab. The data set consists of full-body motion
capture recordings of 30 participants and six one-minute musical excerpts used
during performances. The motion capture and audio data is available online as
an open access data set [123].
3.1.1
Participants
30 dancers (F: 22, M: 8) were recruited from three dance and musical theatre
schools in the greater Oslo area. Their ages range from 19 to 31 (SD: 3.484, mean:
22.167). 13 participants have 10 or more years of experience while 11 participants
are relatively new to dancing with 3 years of training or less.
The main
requirement for participants was that they had some dance background and would
be comfortable with improvisation. The most common dance styles preferred by
participants were contemporary, modern and lyrical jazz. Participants performed
three improvisations to each of the six musical stimuli.
This results in 18
recordings per participant and 540 1-minute recordings in total.
3.1.2
Lab setup and procedure
Participants were recorded using the Qualisys Track Manager 2019.3 and 12
Oqus 300/400 series infrared cameras. Participants entered the lab one at a time
and were asked to ﬁll in a consent form and a short questionnaire to report their
age, gender, contact information, number of years of experience dancing and
details regarding their preferred dance style. They were then dressed in motion
capture suits and 43 reﬂective markers were placed according to the marker
guide shown in Table 3.1 which is based on the Qualisys marker guide.
23
3. Methods
Figure 3.1: During motion capture data collection the dancers wear motion capture
suits aﬃxed with reﬂective markers. The marker setup is based on the locations
suggested by the Qualisys sports marker set.
Participants were given a minimum of 5 minutes to warm up in the
lab space.
During this time the musical stimuli were played as well to let
participants familiarize themselves somewhat with the music before recording.
Each participant was also given a short introduction to the motion capture
technology. Participants were able to see a live stream of the motion capture in
the lab space and were able to explore how certain movements such as lying on
the ﬂoor or moving to the corners of the room would cause markers to drop out.
As the optical motion capture system requires at least two infrared cameras to
have a direct line of sight to any given marker at all times for it to be recorded
participants were asked to imagine a spotlight, around 1.5 meters in diameter,
at the centre of the room and were asked to try to keep within this area during
recording. Participants were given no further instructions about how to move.
The entirety of the recording session including warm-up took approximately 30
minutes for each participant after which they were given a gift card.
3.1.3
Open access data set
The data set has been made available in an open-access repository online in two
versions [123]. The ﬁrst version contains audio ﬁles and the 154 example subset
of the data which has been used in the publications included in this thesis. These
ﬁles have been labelled, gap-ﬁlled, post-processed for deep learning (See section
3.2.1) and exported as .tsv ﬁles which are synced with the musical stimuli. The
second version contains the full data set, 540 1-minute motion capture recordings
of positional data. The data set also contains MATLAB scripts for visualising
the recordings.
24
Preparing the data set for deep learning
Table 3.1: Marker placements and ID numbers.
Marker ID
Location
1
Forehead, above the nose.
2
Top of head, centre.
3,4
Left and right side of the head, just above center of ear.
5
Chest, middle part of the sternum.
6,7
Left/right front of pelvis.
8, 16
Left/right thigh, above the kneecap.
9, 17
Left/right outside of knee.
10, 18
Left/right front of shin.
11, 19
Left/right outside of ankle.
12, 20
Left/right foot, base of toes on the outside.
13, 21
Left/right toe tip.
14, 22
Left/right foot, base of toes on the inside.
15, 23
Left/right back of the heel.
24
Spine top, base of neck.
25, 33
Left/right top of shoulder.
26, 35
Left/right on the outside of the elbow.
27, 36
Left/right wrist outside of the wrist (pinky side)
28, 37
Left/right base of the index ﬁnger.
29, 38
Left/right inside of the wrist (thumb side).
30, 34
Left/right back of the arm.
31, 32
Left/right upper part of the shoulder blade.
39, 40
Left/right below the shoulder blade.
41, 42
Left/right back of the pelvis.
43
Right side middle of back (Only used to indicate right side of body)
3.2
Preparing the data set for deep learning
The data set was split into training and test sets before creating the data tensors.
The training data was further split into one set for training and one for validation.
The models were trained using the Adam optimizer [61] until the loss on the
validation set failed to improve for 10 consecutive epochs.
3.2.1
Movement features
Using the MoCap Toolbox 1.5 [16] the 43 markers a were translated to 22
segments, see Figure 3.2. Small gaps in the data were spline-ﬁlled and a 2nd-
degree Butterworth ﬁlter with a 7.2Hz cutoﬀ was applied to remove any marker
jitter. Recordings in our data set have been normalized so that the root marker
(a weighted average of markers 41, 42, 6 and 7 in Figure 3.2) is centred at the
origin where x, y, and z position is 0. Body segment lengths are averaged across
the dancers ensuring that the data is invariant to global position and individual
body dimensions. The data was captured at 240Hz and downsampled to 30Hz
before model training to reduce the size of each example. The resulting data
tensors consist of 1800 frames (60 seconds at 30Hz) containing the Euclidean
vector pointing from the proximal to the distal joint of each of the 22 segments.
25
3. Methods
Figure 3.2: Placement of the 42 markers worn during data capture and the corresponding
22-segment transformation.
Table 3.2: Description of music stimuli used for dance improvisation data collection.
Sound ID
Instrumentation
Pulse clarity score
Tempo (BPM)
A
Synth, percussion, vocals (melodic)
0.4083
69
B
Zither
0.076405
N/A
C
Synth, Vocals (rhythmic)
0.21989
108
D
Vocals (melodic), percussion
0.32453
149
E
Piano, percussion
0.16784
60
F
Synth, percussion
0.35905
154
3.2.2
Musical features
Six music excerpts were used in data collection. The music pieces varied in terms
of tempo, instrumentation and beat clarity and were selected to be similar in
style and genre to music commonly used by the participants in improvisation
exercises. We extracted two high-level rhythm- and timbre-related features from
the stimuli using the MIR Toolbox [69] to be used in training our generative
model. The ﬁrst was a pulse clarity score, a high-level feature that measures how
clearly the underlying pulse of the music is perceived [68]. A high pulse clarity
score implies that listeners can easily locate the beat, or pulse, of the music
while a low pulse clarity score indicates that the presence of a clear periodic beat
is more diﬃcult to perceive.
The second feature extracted from the stimuli is spectral sub-band ﬂux.
Spectral sub-band ﬂux measures spectral changes in diﬀerent frequency bands of
an audio signal. Alluri et al. [1] found that the sub-band ﬂuctuations in certain
regions in particular correlate to our perception of tonal colour. Their research
indicates that ﬂuctuations in the region between 50 Hz and 200 Hz are related
to the perceived fullness of a musical piece, while ﬂuctuations in the region of
26
Preparing the data set for deep learning
1600 Hz and 6400 Hz were linked to the perceived activity of the piece. These
sub-bands also correspond to activity from rhythmic instruments such as kick
drum and bass guitar for the lower frequency band and hi-hat and cymbals for
the higher range. Table 3.2 shows an overview of the instrumentation and audio
features for each musical piece. We calculate the spectral ﬂux for each musical
stimulus using a sliding window of 5 seconds and a hop size of 0.08 seconds. This
gives us a time series of sub-band ﬂux values for each of the 10 bands wherein
each set of 10 sub-band ﬂux values corresponds to a single frame of the motion
capture data. The pulse clarity score is less meaningful as a frame-wise measure,
so the overall pulse clarity of each musical stimulus is appended as a static value
in each frame. The audio features are appended to each data frame of the motion
capture data to create the tensors used to train the generative models.
3.2.3
Visualisation
When visualising the motion capture data and the model-generated output
for presentations, surveys and interactive applications I have chosen to use a
stick-ﬁgure representation. This approach was chosen for its simplicity, making
it quick to render and avoiding adding additional bias pertaining to dance genre,
perceived gender or other identity markers. This representation of the human
body is sparse, and may remove several factors that can aﬀect viewers’ perception
of the expressivity of the movement [86], but viewers can still discern relatively
subtle characteristics even with simpliﬁed animations [84].
Note 5:
Laban Movement Eﬀorts
The Laban eﬀort factors in La-
ban Movement Analysis (LMA)
are weight, space, time and ﬂow.
Each of the eﬀort factors is con-
sidered a continuum between two
opposites. Weight exists between
light and ﬁrm, space between
ﬂexible and direct and time be-
tween sudden and sustained.
a
In addition, there is ﬂow, which
can be free or bound. Traditionally, LMA has been used in dance
education and analysis and is performed by certiﬁed Laban move-
ment analysts. More recent works also explore how motion capture,
muscle tension and acceleration can be used to recognise Laban
Movement Eﬀorts [32].
aImage from [66]: The eight basic eﬀort actions in relation to the eﬀort
factors time, space and weight.
27
3. Methods
Figure 3.3: In the MDRNN model, the LSTM network learns to estimate the parameters
π (height), µ (location), and σ (width) of each mixture component.
3.3
Models
Two deep learning approaches have been explored throughout the publications
included in this thesis: the MDRNN model, an LSTM-based recurrent neural
network (RNN) in combination with a mixture density model (MDN), and the
transformer. In the following sections, an overview is given of the training and
parameters of the most successful implementations. Several variations have
however been implemented throughout this work. I have implemented both
pure movement generation models and models which generate movement based
on a combination of the previous movement and sound features. Each model
has also been trained over several rounds to tune the models’ hyperparameters
and explore alterations to the architectures, such as the number of layers and
neurons.
The generation of dance is here approached as a multidimensional sequence
prediction task. The assumption is made that the preceding positions of the
body joints are instrumental in predicting the upcoming positions. Each model
is trained using a supervised learning method with back-propagation through
time. The error term used to update the weights of the models is the diﬀerence
between the predicted output and the value of the next frame in the given
training example.
The general problem statement can be expressed in the
following way:
Given a sequence of motion capture frames represented as
X = (x1, . . . , xt)
we generate a sequence of future motion
X′ = (x′
t+1, . . . , x′
t+s)
from time step t + 1 to t + s, where s is the number of frames we
wish to generate.
3.3.1
Training the MDRNN
A mixture density recurrent neural network (MDRNN) is a network which uses
an RNN to learn the parameters of a Gaussian mixture model (GMM). The
MDRNN implemented in this work consists of three layers of LSTM cells as shown
28
Models
in Figure 3.3. In the training phase, the network is fed input data and learns the
parameters, π, µ, and σ, for each of the mixture components. π is the height
of each component (its likelihood), µ describes the mean of each component
and σ describes each component’s standard deviation. During inference, the
mixture density network (MDN) outputs a probability distribution over the
target variables, 3 x 22 joint positions. To optimize an MDN, we minimize
the negative log-likelihood of sampling true values from the predicted GMM
for each example. A probability density function (PDF) is used to obtain this
likelihood value [80]. In our case, the GMM consists of K = 4 n-variate Gaussian
distributions. For simplicity in the PDF, these distributions are restricted to
having a diagonal covariance matrix, and thus the PDF has the form:
p(θ; x) =
K
X
k=1
πkN(µk, σk; x)
(3.1)
where π are the mixing coeﬃcients, µ, the Gaussian distribution centres, σ the
covariance matrices and n is the 66 position values (22 points * 3 dimensions)
contained in each frame.
3.3.2
Training the Transformer
The transformer is a sequence prediction model based on an encoder/decoder
architecture (see Figure 3.4). In my work with transformers I have implemented
an approach similar to that used by the movement model in the work by Li et al.
[75], using a discrete representation of the joint angles. By splitting the space
into a grid of N = 300 possible locations the movement can be encoded in a
similar way as in a language modelling task with a vocabulary size of N. During
training the encoder side of the transformer model encodes the training sequence
using the self-attention mechanism. The self-attention mechanism learns three
vectors: query, key and value. These are in turn used by the decoder to generate
new data. The encoder and decoder each consist of six layers. Each layer has
one dense network containing 256 artiﬁcial neurons and an attention layer with
8 attention heads. A ﬁnal softmax layer on the decoder side gives a probability
distribution across the N possible positions for each of the 3 axes of the 22 joints
that make up the next frame of the movement sequence.
3.3.3
Sampling strategies and temperature
Both the MDRNN and the transformer produce a probability distribution as
their output. During inference, it is possible to adjust this distribution such that
we can sample the output value with more or less randomness. By performing a
reweighting of the learned distribution we can change the likelihood of a value
being selected. The parameter which describes the amount, or strength of this
reweighting is called the temperature parameter. When the temperature is
low the sampling process is more deterministic, resulting in a greedy sampling
strategy. When the temperature is high we create a more uniform distribution,
29
3. Methods
Figure 3.4: The transformer architecture consists of several encoder and decoder
modules. Each module contains a self-attention layer and a feed-forward layer. Image
from Vaswani et al. [121]
making each possible outcome equally likely. This sampling strategy is called
stochastic sampling. Figure 3.5 shows an example of how a simple distribution
of discrete tokens can change when adjusting the sampling temperature. This
adjustment of temperature is common in many generative tasks, as it allows
control of the stochasticity during generation. Our options for reweighting the
distribution are even broader for the MDRNN as it can learn several distributions
simultaneously. In addition to changing the width and height of the mixture
model or its distributions, each distribution can be sampled in isolation instead
of as a mixture.
3.4
Evaluation
As an initial evaluation, we perform a visual inspection of the generated output
to guide in hyperparameter tuning and choice of sampling strategy. To evaluate
30
Evaluation
Figure 3.5:
Example distribution after reweighting with diﬀerent temperatures.
Depending on our sampling strategy, the generated output can exhibit more or less
surprising output. Low temperatures result in more deterministic sampling, while high
temperatures cause more stochastic sampling.
the output further we employ a perceptual judgement survey and user-centered
studies. We have deployed an online perceptual judgment survey to gather
subjective assessments of the perceived dance likeness, musical ﬁt and expressivity
of a selection of AI-generated dance sequences set to music. While online surveys
can allow researchers to attract a larger number of respondents, the control
over experiment conditions like the respondents’ equipment and surrounding
conditions is reduced. However, crowdsourced and remote user studies can still
be reliable [105], given that best practices are followed by handling outliers and
acquiescence responding [34].
Visual inspection by the authors and descriptive statistics from our perceptual
judgement study can give an idea of the perceived realism of the generated dance
and perhaps the subjective preference of the observers. These methods are
however not suﬃcient to provide an understanding of what the potential use case
for dancers might be. To explore the eﬀect and potential of AI dance in dance
practice we facilitated two practice-based studies. In one study we perform a deep
case study involving a single dancer working with a collection of AI-generated
dance clips in his creative practice over several weeks. In the other, we brought
together two groups of dancers in workshop sessions to explore the concept of
an AI dance partner in improvisation through role-play and bodystorming. The
participants of this study take on the role of an imagined AI dancer whose aim
is to inspire a human counterpart as an improvisation partner. This method
applies elements from speculative design to develop a critical discourse about
the future use of generative AI in dance. In speculative design, ﬁction and
imagined potential futures are used to explore emerging technologies [3] and
possible future research areas [79].
When working with dance practitioners we collected data in the form of
discussion transcripts and journal entries.
This data was analysed using a
reﬂexive thematic analysis approach. Thematic analysis refers to a type of
qualitative data analysis that involves engaging with data in an iterative manner
to consolidate themes [14]. This involves coding the data sources over several
rounds and discussing the codes that are prevalent and their meaning. The
31
3. Methods
themes are based on the topics or concepts that are mentioned repeatedly in the
data and are then analyzed in more depth to identify relationships and patterns.
Reﬂexive thematic analysis [13] is a more recent approach to thematic analysis
that incorporates the researcher’s own experiences, biases, and perspectives
into the analysis process. This approach recognizes that the researcher’s own
experiences, values, and beliefs can inﬂuence the way they interpret and analyze
the data.
Note 6:
Dancing Embryo
In collaboration with dancer and choreographer Diego Marín we
construct an interactive application which uses movement detection
to allow Marín to modulate AI-generated dance sequences in
various ways. The application was presented at a Human + AI
Collaborative Performance event organised by the Leverhulme
Center for the Future of Intelligence (LCFI) hosted at the University
of Cambridge. Marín interacts with the online application (available
here) through his movements which are recorded through a webcam.
By doing so, he changes the shape of the avatar and the visual
artefacts that are produced. As we consider the AI to be only in
its initial stages the project was given the name Dancing Embryo.
The visualisation of the AI-generated dance shifts from a cell-like
shape to a more complex being throughout the interaction.
3.5
Ethical considerations
Movement generation research raises several ethical considerations. This section
outlines three main areas of interest and presents the methods used to attempt
to make this work comply with current national and international guidelines for
ethical AI and data handling as well as considerations of the wider implications
of generative AI in dance.
3.5.1
Bias and discrimination
Movement generation algorithms are only as unbiased as the data that they are
trained on. If the data lacks diverse examples then the algorithm may generate
32
Ethical considerations
movements that perpetuate the lack of diversity. Given that regular human-
robot interaction is becoming an increasingly likely future in Western society
[15], this bias and our current understanding of the sentiment associated with
expressive movement will be transferred to these virtual avatars and physical
agents. Our data set contains a relatively homogeneous group sourced from our
local dance schools and professional networks. In working with dancers we ﬁnd
that our participants in most cases come from a contemporary or modern dance
background. As such we acknowledge the limitations of our models in producing
output that reﬂects the vast landscape of dance. We aim to avoid excessive
generalisation regarding the scope of the research as the conclusions drawn from
what the models learn will not apply to all humans or all dancers.
3.5.2
Privacy
When movement generation research involves the collection of personal data,
such as movement patterns, from human subjects, privacy and data security must
be considered. Throughout this work, we have aimed to ensure that the data
collected is stored securely and that published data sets are made anonymous.
Our data collection and studies have been approved by the Norwegian Agency
for Shared Services in Education and Research (SIKT). Motion data is a rich
data source and can potentially be used as indicators of mood, gender, age, and
physical abilities as well as revealing other identifying markers [10]. Participants
in our data collection were informed about the purpose of the research and given
the option to withdraw from the study at any point. During data collection, we
recorded the age and gender of participants in addition to information about
their dance experience and dance genre preference. This information is not on
an individual basis relevant to our research and so only information about the
group as a whole is published, without tying this information to any individual
motion recording.
3.5.3
Unintended consequences
Movement generation research may have unintended consequences that are
diﬃcult to predict.
Dance is a cultural heritage which is often passed to
new generations without tangible material such as notation. Motion capture
technology may allow intangible heritage to be recorded for posterity [99],
but dance generation systems trained on this data could also pose a potential
threat to the livelihood of performers. Similar experiences have been recorded
amongst graphic designers and digital artists as the improvements in text-to-
image applications have made it possible to generate complex, high-deﬁnition
images online [64]. The data which is used to train these models are often sourced
online, making a digital artist’s online portfolio a potential data point. While
no equally accessible system has been developed within dance generation yet, it
is clear that there is a possibility that such a system can become available in the
near future. We hold that it is the AI practitioners’ responsibility to prioritize
the interests and views of the individuals who contribute their performances and
33
3. Methods
artistic output as part of training data by complying to the guidelines that aﬀect
our data set and models.
34
Chapter 4
Summary of papers
This chapter summarises the papers included in this thesis and presents an
overview of how each paper relates to the research questions presented in section
1.2.
Figure 4.1 shows a view of how each research question permeates the
publications and Table 4.1 lists the aims, methods and results of each paper.
The chapter closes with a review of the main ﬁndings and contributions of the
paper collection.
4.1
Overview
The three research questions in this thesis have been approached in a mostly
linear fashion, beginning with RQ1 and ending with RQ3. However, the third
research question stands out as an aspect which intersects with all the papers to
some degree. Papers V and VI focus directly on the question of how AI-generated
dance might ﬁt into dance practice and involves close interaction with dancers.
But the notion of how AI-generated movement might be useful to creators forms
an underlying motivation across all six papers. One way of viewing the research
questions is that control of the AI, as well as its ability to model sound-motion
mappings, falls under the overarching question of how AI might ﬁt into dance
practice. This perspective is presented in Figure 4.1 which gives an overview of
how each paper relates to the research questions.
Figure 4.1: The three research questions addressed in this thesis and the corresponding
papers.
35
4. Summary of papers
Table 4.1: Overview of the aims, methods and results from each of the six papers
included in the thesis.
Aim
Methods
Results
I
MOCO 2019
•Emulate the tendencies
of human movers in
a sound-tracing task
using an MDRNN
•Train an MDRNN on
sound-tracings
•Compare the generated
output to the
original tracings
using an LSTM
classiﬁer
•A higher accuracy was achieved
when classifying the AI-generated
tracings, implying less
variation between
tracings in the generated dataset
compared to the original
tracings.
II
ICCC 2020
•Train an MDRNN to
generate dance movement
using our dataset
•Study the eﬀect
diﬀerent musical input
has on the generated
movements
•Train an MDRNN on
our motion capture
data set of dance
improvisation and music
•Generate movement
sequences using
various sound ﬁles.
•Proof-of-concept for generating
movement based on our dataset
•Sound features from diﬀerent
musical excerpts aﬀected the
AI-generated movements by
adding additional
noise to the
movement sequence
III
EVOMUSART 2021
•Systematically explore
the limits of various
sampling strategies for
generating movement using
the trained MDRNN
•Generate output at
a range of sampling temperatures.
•Sample from each mixture
component in isolation
•Sample using a priming sequence
•Sampling from isolated
mixture components results
in interesting movement
variations.
•The range of viable
sampling temperatures is
small if realism is prioritised.
Higher temperatures add
noise and unpredictable
movement.
IV
C&C 2021
•Understand people’s
perceptions of sound-motion
mappings in dance
•Measure the diﬀerence
between participants’ perception
of these mappings in
AI-generated vs human dance
•Create animations of
8 dance sequences,
4 human, 4 AI-generated
•Host an online survey
to collect ratings of
dance-likeness, musical ﬁt, preference
and expressivity
•Perform post-hoc repeated measures
one-way ANOVA for responses
to each dependant variable
•Mismatched music is less
perceptible in
AI-generated dance
•Examples generated using
music with a clear beat are
rated higher in musical ﬁt,
preference and dance-likeness.
V
C&C 2023
•Engage dance practitioners
in discussions of AI-generated dance
and its potential place
in their practice
•Host two workshop events where
participants role-play
as an imagined AI-dancer
and take part in group discussion
•Use reﬂexive thematic analysis
to formulate themes based on
discussion transcripts
and experiences from
bodystorming sessions
•Suggestions for future work
in interactive AI-dance:
including dancers in development,
leveraging AI-glitches,
balancing surprise
and predictability
VI
•Uncover possible use cases
for glitch in dance practice
•Implement an online browsing
interface for exploring
AI-generated dance
with varying degrees
of glitch.
•Document a dancer’s experience
using the clips
as a creative catalyst
in choreographic exploration
•Use reﬂexive thematic analysis
to formulate themes
based on discussion transcripts,
video-cued recall and diary entries
•Glitch was seen as
particularly promising
in improvisation class.
•Including unrealistic
movements facilitate a positive
loop of surprise,
transformation and breaking
movement patterns
36
Papers
4.2
Papers
This section provides the motivation and summary for each of the papers. Each
paper is attached in full at the end of this thesis.
4.2.1
Paper 1
Tracing from Sound to Movement with Mixture Density
Recurrent Neural Networks
B. Wallace, C.P. Martin, K. Nymoen
In 6th International Conference on Movement and Computing (Doctoral
Consortium), 2019
Figure 4.2: Overview of the generation pipeline from sound and movement features to
generated output from paper I.
Motivation
This paper marks our initial attempts at using the mixture density recurrent
neural network (MDRNN) to model sound-motion mappings. In this work, we
use an MDRNN to learn mappings between simple arm movements and sound
signals using a data set of single point movements (called “sound-tracings”)
performed by 10 participants along with short sounds.
Summary
As part of a study performed in 2010 to observe how people relate movement to
sound, 10 participants were asked to listen to a set of sounds and move a rod
through the air, as though this action was itself making the sound [91]. Using
this data set, we train an MDRNN to generate movement based on preceding
sound and motion features. The MDRNN consists of two layers of LSTM cells
each with 15 hidden units. The outputs of the second LSTM layer are in turn
connected to the mixture density network consisting of 5 mixture components.
We use a second neural network classiﬁer to show that the input sound can
be identiﬁed from generated tracings. An accuracy of 89% was obtained on a
hold-out set of 250 traces. In comparison, when training the classiﬁer on the
original dataset, an accuracy of 75% was obtained on a hold-out set of 62 traces,
similar to the previously reported classiﬁcation accuracy for this dataset using a
Support Vector Machine classiﬁer. This indicates that there is more variability
37
4. Summary of papers
in the original dataset than in the set of generated tracings. When attempting
to classify the generated tracings using the classiﬁer trained on the original
sound-tracing data we achieve an accuracy of only 17%. While it is easy to
make a connection between the tracing and the sound in the original dataset, the
perceptual link between generated tracing and sound is less clear. The MDRNN
in this case seems to learn an "over-generalized" version of the sound-tracing
task.
4.2.2
Paper II
Towards Movement Generation with Audio Features
B. Wallace, K. Nymoen, J. Torresen, C.P. Martin
In 11th International Conference on Computational Creativity, 2020
Figure 4.3: Image from paper II. The sequence generated when the audio features from
a priming example are replaced with features from a song not used in the data set.
Motivation
After completing the data collection stage of my research, an MDRNN was
implemented which took as its input a sequence of motion and sound data
and predicted the following sequence of motion. This work presents initial
experiments training the MDRNN movement generation model on 54 minutes of
our motion capture dataset of improvised dance together with sound features
extracted from the accompanying music. This paper forms a proof of concept
for the idea that high-level rhythmic and timbre features could be used to aﬀect
the movement generated by the MDRNN. It extends the ﬁrst paper by including
more complex movements (full-body motion capture) and music.
Summary
In this paper, three audio features calculated from the set of music pieces used
in data collection are included in training an MDRNN. This experiment aimed
to examine to what extent the model was able to learn a correlation between
audio features and movement and what eﬀect features from diﬀerent musical
excerpts would have on the generated movements. Contemporary works largely
focused on rhythmical, beat-matching aspects when generating dance. While
following the beat is a prominent aspect of many forms of dance, beat-matching
is not the only aspect of music that aﬀects the way we move. Instead of focusing
38
Papers
only on rhythmic features we extracted three high-level sound features from our
musical excerpts: spectral ﬂux of low frequency (50-200 Hz) and high frequency
(1600-6400 Hz) bands as well as a measure of the pulse clarity. Movement
sequences were generated by priming the model on a single full-body motion
capture example and altering sound features. While a near-identical movement
sequence was generated when using the audio features corresponding to the
priming movement, using sound features that were diﬀerent from the priming
example caused the generated movement to vary and contain occasional noise.
When using sound features from a white noise signal instead of music, the
aforementioned noise, or jitter in the movements became very prominent. This
work provided a proof-of-concept for generating movement using the MDRNN
model which would vary depending on the audio features.
4.2.3
Paper III
Exploring the Eﬀect of Sampling Strategy on Movement
Generation with Generative Neural Networks
B. Wallace, K. Nymoen, J. Torresen, C.P. Martin
In 10th International Conference on Artiﬁcial Intelligence in Music,
Sound, Art and Design (EvoMUSART), 2021
(a) 1st
(b) 2nd
(c) 3rd
(d) 4th
Figure 4.4: Figure from paper III. Sampling a sequence from a single mixture of
components in isolation reveals how each distribution learns a diﬀerent movement
shape.
Motivation
This third paper is concerned with exploring the limits of movement generation
with the MDRNN through a systematic exploration of sampling strategies applied
to a trained model. After a model has been trained, there are several ways we
can aﬀect the generated output by changing our sampling strategy. Adjusting
sampling strategies after a model has been trained is a relatively common practice
when working with generative models. Unlike prediction tasks, generative tasks
are most often not concerned with ﬁnding a single correct solution but rather to
39
4. Summary of papers
generate novel variations. For this reason, the output variable with the highest
learned likelihood is not necessarily always the most interesting.
Summary
The mixture density recurrent neural network (MDRNN) produces as its output
a collection of probability distributions or mixture components. In this paper,
we examine the generative power of an MDRNN with four mixture components,
trained on our dance dataset.
We implemented three sampling strategies:
priming, sampling temperature and isolating mixture components. The strategies
are examined systematically to further understand the eﬀects of sampling
parameters on motion generation. This analysis provides evidence that the
choice of sampling strategy signiﬁcantly aﬀects the output of the model. We
conclude by suggesting speciﬁc recommendations for sampling from an MDRNN
to produce novel and realistic movement and make our sampling code, trained
model and video examples available in an open repository online. While the
range of temperature values that support realistic movement is somewhat slim,
the various mixture components seem to produce diﬀerent variations which are
each interesting in their own right. Using primed sampling in combination with
sigma temperature variation also shows promise for generating variations on a
movement theme.
4.2.4
Paper IV
Learning
Embodied
Sound-Motion
Mappings:
Evaluating
AI-Generated Dance Improvisation
B. Wallace, K. Nymoen, J. Torresen, C.P. Martin.
In the 14th ACM Conference on Creativity and Cognition, 2021
Motivation
To evaluate the AI-generated movement sequences further than by our visual
inspection we perform a perceptual judgment experiment wherein 208 respondents
were asked to rate a set of AI-generated dance clips based on four criteria: musical
ﬁt, expressivity, preference and dance-likeness. Our question here was two-fold:
did respondents perceive the AI-generated movements as realistic and having
high quality? And were these aspects related to how well the AI was able to
model the sound-motion mappings? The examples were presented alongside
human performances in random order. In the analysis, particular attention was
given to the sound-motion mappings of the examples. Our motivation for this
study was to examine to what extent an AI can learn to model mappings between
sound and motion in dance, and how we as observers perceive these mappings,
both in humans and AI.
40
Papers
Figure 4.5: Screengrab of survey interface from paper IV.
Summary
Respondents to the survey were presented with 8 videos. Two performances from
our motion capture data set and two performances generated by the MDRNN.
Each performance had a duration of 20 seconds and was shown once with its
intended musical stimulus and once with a diﬀerent musical stimulus. For the
human examples, this means the motion sequence was shown once with the music
that was playing while the data was collected and once with music and once with
diﬀerent music. For the AI-generated motion sequences the clips were displayed
once with the music corresponding to the sound features used to prompt the
model and once with a diﬀerent song. 208 responses were collected during the
3 weeks the survey was available. Signiﬁcance testing was performed on the
4 response variables for each of the video examples to ascertain the main and
interaction eﬀects between the factors of sound-mapping and animation type
(human or AI). We then perform a two-factor repeated-measures ANOVA on the
standardised responses to all questions. If the model was producing examples
that correspond well to the sound motion mappings we expect these examples
should score higher for perceived musical ﬁt when presented with the correct
music. The perceived musical ﬁt of a given human example correlated directly to
the sound mappings with mismatched audio examples scoring lower on musical ﬁt.
The perceived sound-motion mappings of the AI-generated examples remained
somewhat elusive, particularly when compared to examples of human dance.
We did however ﬁnd that in certain aspects related to perceived dance-likeness
41
4. Summary of papers
and expressivity, the model successfully mimicked human dance movement. The
highest-rated AI example was the animation of our high-tempo song set to
its intended music. As early results indicated, the model generally performed
better when generating movements for music with a clear beat. This example
gained the highest ratings across dance-likeness, musical ﬁt and preference. The
dance-likeness ratings did not correlate to sound-mappings, neither for human
nor AI-generated examples. Indicating that what we might perceive as dance
does not always match up with what we feel "suits" the music.
4.2.5
Paper V
Embodying an Interactive AI for Dance Through Movement
Ideation
B. Wallace, C.Hilton, K. Nymoen, J. Torresen, C.P. Martin, R. Fiebrink.
Accepted at the 15th ACM Conference on Creativity and Cognition, 2023
Figure 4.6: Figure from paper V: The dancers pair up and move together in an
improvisation task where one participant (wearing a coloured headband) takes on the
role of an AI dancer, reacting to the input of their partner.
Motivation
In this paper we present the ﬁndings from a series of workshops wherein dancers
took part in embodiment exercises, role-playing as an AI dancer. The experience
was reﬂected on in group discussions with the authors. What do dancers expect
from AI-generated dance? How does it ﬁt into their practice if at all? The
research questions addressed in this paper were motivated by a wish to increase
our understanding of dancers’ fears, hopes and expectations regarding the use of
AI-generated dance as an interactive catalyst for creative exploration.
Summary
Using reﬂexive thematic analysis, we identify three themes from the discussion
transcripts.
Our ﬁrst theme, beyond replica, aims to describe the dancers’
expectations for AI being more than a mere replica of a human dancer. The
non-human potential of AI-generated dance is seen by the participants both
as alienating and intriguing.
The AI dancer might not get tired or make
valuable mistakes that in turn shape ideation. At the same time, the strange
42
Papers
and unfamiliar movements that an AI could produce can inspire novel ideas.
Our second theme, intuiting ﬂow, encompasses the embodied experience of
the dancers and their ongoing interpretations and prediction of their partner’s
moves. These complex and intangible moments of intuition form the basis for
their improvisations and pose a potential challenge for AI-generated dance in
interactive applications. Our ﬁnal theme, building and breaking shared images
lies at the cross-section of our previous themes, describing how the participants
use their lived experiences, associations and interpretive skills to create rapport
with their partner. By surprising their partner’s expectations they shift the
shared image that has been created and breathe new life into the improvisation.
Finally, we explore the practical implications of our ﬁndings for AI-generated
dance. Through this discussion, we argue for the potential value of leveraging AI
glitches to encourage surprise and breaking from realism. We also highlight the
importance of including dancers in the evaluation and development of AI dance
as the ephemeral, embodied nature of dance leads to intangible moments which
do not lend themselves well to algorithmic deﬁnitions and programmable rules.
As such, close collaboration with dancers can facilitate a better understanding of
the potential and challenges associated with bringing generative AI into dance
practice.
4.2.6
Paper VI
Breaking from Realism: Exploring the Potential of Glitch in
AI-Generated Dance
B. Wallace, K. Nymoen, J. Torresen, C.P. Martin
Under review (journal)
Motivation
This ﬁnal paper presents the results of a deep user study collaborating closely
with a modern contemporary dancer on the notion of non-realism, or glitch
in AI-generated movement. Throughout the previous ﬁve publications, the
importance of realism has been an underlying assumption. However, in scientiﬁc
dissemination and presentations as well as in the analysis in paper V, it became
clear that the apparent failings of the AI to generate realistic movement could
be an equally strong, if not stronger, source of curiosity and interest. In this
paper, we were motivated to deploy the output of our model into an artistic
practice to test the utility of glitch.
Summary
Using a browsing interface created for the study, a dancer explored the use
of movement sequences generated by an MDRNN and transformer model in a
choreographic setting as well as in an improvisation class facilitated by the dancer.
43
4. Summary of papers
Figure 4.7: Image from paper VI. Dancer and choreographer Bendik Sundby improvises
with AI-generated movement as a creative catalyst.
The dancer reﬂected on the potential for using the non-realistic movements as
prompts in an improvisation class as these movements forced the dance students
out of their usual movement patterns in an attempt to mimic or translate
the strange and impossible movements using their bodies. Using a reﬂexive
thematic analysis approach we construct a set of themes which describe how the
glitch aﬀected the dancer and students in their improvisation and choreographic
practice. Our themes revealed a process for which glitch is the catalyst: from
surprise as a challenge and beneﬁt, through the transformation of impossible
movement to breaking habitual movement patterns. Our ﬁndings in this study
supported the notion that glitches could function as a practical use case for
AI-generated dance as they motivated the dancer to move in new, unfamiliar
ways. This work supports previous research in creative domains which upholds
the importance of surprise and unfamiliarity in creative catalysts, and supplies a
speciﬁc example of how this manifests in AI-generated dance.
4.3
Contributions
The initial publication for this thesis, paper I, provided a proof of concept for the
project. Using a small dataset, we were able to generate sound-tracings based
on preceding movement and sound features. While the MDRNN presented in
paper I learned a to generate tracings that were less variable than the tracings
of the underlying dataset, the results from this work oﬀered a starting point for
exploring sound motion mappings using more complex movement data. Paper II
presents this further exploration.
44
Contributions
In paper II, we train a similar MDRNN on a subset of our motion capture
dataset of improvised dance. By altering the sound features used during inference,
we attempt to gain insight into how the sound features aﬀect the model’s
predictions.
During these initial steps, the importance of discovering and
emulating the mappings between sound and motion were central aims. This
was motivated in part, as discussed in Section 1.1, by the assumption that
sound-motion mappings play a part in movement to music and would thereby
be important for the perceived realism of an AI-dancer.
The systematic review of parameters presented in paper III is aimed at
uncovering how much variety the model could produce. The kind of output a
model can produce is aﬀected by many variables relevant to the pre-training
phase, such as data features and the model’s architecture and size.
When
interacting with a trained model, however, these variables are ﬁxed and so end
users are bound to the parameters available post-training. In paper III these
post-training parameters are systematically explored, revealing the MDRNN’s
ability to model multiple possible futures simultaneously. This potential for
variation is useful in the development of an AI-dancer that can appeal to multiple
preferences.
In paper IV we ﬁnd that respondents’ perceptions of how well the examples
ﬁt the music did not directly correspond to their preference or whether or not
they judged the example to be dance-like. While the sound-motion mappings of
the human dance examples were mostly clear for respondents, the results from
this survey indicated that sound-motion mappings might not be the whole story
for realism (dance-likeness) or preference.
As can be seen in Figure 4.1, papers I-IV fall under the research questions of
control and mappings. However, with all the publications in this paper collection
an underlying motivation has been to explore and develop AI for use in dance
practice. Mappings have been explored in search of realism, and control has been
explored in search of variation and novelty. In paper V and VI, the technical
limitations and aﬀordances of the models are set aside in order to address the
question of an AI-dancer’s role in dance practice directly. Our ﬁndings from
the workshops and case study reveal that there is interest in dance communities
for using an AI-dancer as part of their creative practice. AI-generated dance
which breaks from our expectations of realism can be powerful creative prompts,
helping dancers to escape ingrained movement patterns.
From this paper collection we can identify some key ﬁndings related to the
two questions presented as part of the subjectivity challenge in Section 1.1: When
considering how we measure quality, the ﬁndings of these papers point towards
the importance of a holistic approach to evaluation. From the ﬁnal two papers in
this collection, we can begin to pinpoint the aim of an AI-dancer. In particular,
we ﬁnd evidence for the importance of surprise and a level of unpredictability
when using AI as a creative catalyst. These ﬁndings are discussed in further
detail in the following chapter.
45
Chapter 5
Discussion
Here, the ﬁndings and conclusions from the included papers are presented in the
wider context of the research questions. Throughout the publications submitted
as part of this thesis, I have explored the research questions from multiple
perspectives. Notions of mappings and control have been examined through
data collection, implementation of methods for generating dance using deep
neural networks and systematic exploration of the model’s ability to produce
dance. Through close collaboration with dancers as groups and individuals,
our work provides a more in-depth understanding of the potential role of an
AI dancer in dance practice. This chapter begins by revisiting the research
questions. Limitations are then discussed, before describing avenues for future
research and the impact that the subjectivity challenge has had on this work.
5.1
Research questions
Mappings: What can AI generated dance reveal about sound-motion
mappings?
Understanding how sound aﬀects the way we move is closely tied to the idea of
embodied cognition which states that our bodies play a large role in shaping our
cognitive processes, including our experience of the world. In embodied music
cognition research we ﬁnd examples of how timbre and rhythm are experienced
not only in the mind but through the lens of our bodies. Exploring how AI
might be used to model these complex interactions between music and movement
revealed how the deep learning approach tended to over-generalise the tendencies
present in their human equivalents. This question was addressed ﬁrst in Paper I.
Here we found that the MDRNN was able to generate sound-tracings that
followed some characteristics of the original gesture, but generalising to the point
where the generated tracings for each sound were more distinct from each other
than the human tracings were. Paper II also approached this question in the
context of dance. By alternating between sound features from diﬀerent musical
stimuli we could observe the eﬀect these features had on the output of the model.
We returned to this research question in Paper IV where a perceptual
judgement survey was performed to gauge the perceived sound-motion mappings
in AI-generated dance movement compared to human examples. In this study,
we found that the perceived mismatch of sound and movement was easier to
discern when rating human dance than the AI-generated counterparts. We found
that throughout this work, the MDRNN displayed a tendency to produce an
over-generalised approximation of the data, averaging out the idiosyncrasies
found in the participants of the data set. While simpler movements such as those
found in the sound-tracing data set allow the MDRNN to learn clearer diﬀerences
47
5. Discussion
between tracings to diﬀerent sounds, attempting to extend this to the more
complex context of sound-motion mappings in dance revealed the limitations of
the generator. This complements existing knowledge regarding the individuality
of movement patterns [22] and the intricate nature of embodied music cognition
by providing additional evidence that generalising across many human bodies
runs the risk of overshadowing individual idiosyncrasies. The mixture density
network does however provide a more complex output distribution than most
other machine learning approaches, and increasing the variance and complexity
of the model could provide less generalised output.
Control: How can we inﬂuence the model outcomes?
When it comes to the choice of approach for modelling dance using AI there are
a multitude of variables to consider, from the choice of machine learning model
to which motion and sound features to include. We also have several options
for adjusting the model once it has been trained. In Paper II this was explored
by altering musical stimuli when prompting our trained model. By training the
MDRNN to generate movement based on a preceding sequence of sound and
movement from our dataset we can experiment by altering the sound features
when generating and examining the generated movements.
The limits of prompting the trained MDRNN was the focus of Paper III.
Including a temperature parameter which is used to reweight the learned
probability distribution is a common way to generate variations on model
output. This is often done by experimenting with sampling from the probability
distribution with more or less “randomness”, often referred to as stochastic
sampling, until the output is found to be satisfactory. If the model consistently
outputs the most likely prediction, the output can become repetitive. Conversely,
a ﬂat probability distribution where all outcomes are equally likely would
be equivalent to sampling at random, invariant to the learned distribution.
Reweighting the distribution allows the model to generate less likely predictions
some of the time. Allowing for occasional unexpected predictions by implementing
stochastic sampling will in many cases improve the realism of the output and elicit
more interesting predictions thereby improving the perceived creative capacity
of the model. However, there are many possible temperature values and ﬁnding
the sweet spot can be challenging. As the MDN learns several distributions of
the data simultaneously, it is further possible to explore each of the learned
Gaussian distributions in isolation. Our ﬁndings show that each distribution
models its own version of a dance sequence, some more human-looking than
others, but each with a distinct nature which could be interesting to explore.
Our research into the various methods of control for the MDRNN provides
examples of potentially interesting parameters to make available to users when
interacting with a trained model. The ﬁndings of these studies gave us conﬁdence
that the MDRNN model could generate a suﬃcient variety of movements for
study in the human-centred studies in Paper IV and Paper VI.
48
Limitations and future work
Practice: What role could an AI dancer play in the creative practice of
dance?
This question was addressed in Paper V through the use of a role-playing setting
and discussions with six dancers over two workshop events. The discussions
revealed the participants’ current understanding and envisioning of AI-generated
dance. The fear of aiming for a “perfect” dancer, unable to make mistakes or
carry the personal history and internal experiences which the participants use to
build rapport with their improvisation partner became clear. When participants
were surprised, or challenged, by their partner this prompted the improvisation
to continue, and in discussions with the dancers, the concept of an AI dancer
which did not necessarily conform to a humanoid shape or actions sparked an
interest. The ideas which formed through this experience encouraged us to look
deeper into the potential of the non-realistic movement.
Paper VI centres around the use of AI glitches as a creative catalyst in a
dancer’s practice, both in teaching and in dance creation. The dancer involved
in this study had a well-established creative workﬂow from several years of
experience as a choreographer.
While using the AI-generated examples as
choreographic exploration was an interesting task, they noted that the notion of
integrating something like this into their usual practice would likely be redundant.
However, when using the interface in an improvisation class, the dancer reported
that the AI-generated movements allowed his students to challenge themselves to
move in unfamiliar ways. This supports ﬁndings in related work such as cochoero
[23] and AI_am [5], which shows the positive eﬀect that can be achieved when
artists are confronted with unexpected stimuli. Through our studies involving
dance practitioners, we suggest that the future role of generative AI in dance could
centre concepts of the non-human and of surprise while leveraging the potential
for large models to learn multiple interpretations of a dancer’s movement. As
mentioned in the overview of papers in the previous chapter (section 4.1), the
question of how an AI dancer might contribute to a dancer’s practice in many
ways permeates each of the papers. The work presented here adds to a growing
collection of research on generative AI dance in performance, improvisation and
dance creation which shows that co-creation and ideation using AI is something
that many dancers are interested in exploring as part of their practice. While our
approach might fall into the category of heroic AI as described by d’Inverno et al.
[27], our work shows that these methods can still integrate into dance practice
in a way more akin to collaborative AI by augmenting creativity, particularly
when prioritising variation and glitches over creating high-ﬁdelity replicas.
5.2
Limitations and future work
On the topic of generative models in artistic ﬁelds, it is crucial to work at the
cross-section of multiple views, both technical and based in the practices which
the generative AI is attempting to model. While working in an interdisciplinary
way allows us to view complex problems from various angles, it can be challenging
to go particularly deep within any speciﬁc ﬁeld. In the following section, I will
49
5. Discussion
outline the limitations of my work and the most promising avenues for future
work.
Note 7:
Future You by Universal Everything
Future you is an interactive artwork by Universal Everything, ﬁrst
exhibited at The Barbican in 2019. The installation consists of a
digital mirror that reﬂects a synthetic version of the person standing
in front of it. The viewer’s movements are reﬂected through various
strange shapes (47,000 possible variations in total)a.
aImage from Universal Everything [30]
5.2.1
Approach and methods
As presented in the background chapter of this thesis, there exist a multitude of
generative models of which I have explored two, the MDRNN and the transformer.
Both these models use a supervised learning approach. The inbuilt assumption
in supervised learning is that there exists a correct future state that can be
predicted based on the current state of the model and preceding events. This
assumption may be challenged in several ways in the context of generative models
of dance and generative AI art in general. One might challenge the assumption
that there exists only a single correct mapping from the current position of the
body to the next position. While the MDRNN does predict the components
of a mixture distribution, allowing us to model an array of possible outcomes
rather than a single position, it is still trained using supervised learning and as
such the distance between the predicted output and the actual, ground truth
position of the body found in the training sample is used to update the models’
internal weights. In this way, the model is trained to mimic the dancers in
the training data. As discussed in the background section on computational
creativity, the ability to mimic is not necessarily bad, it may indeed be a requisite
ﬁrst step in learning to create novel pieces of art. There are, however, other
generative deep learning models which could be explored for comparison. In
50
Limitations and future work
particular, generative adversarial networks (GAN) and reinforcement learning
(RL) approaches. GAN models are constructed to allow two networks to train
adversarially, mimicking an artist vs. critic dynamic. RL approaches such as
hierarchical reinforcement learning which implements intrinsic motivation or
curiosity have been suggested as promising models for several creative tasks. As
discussed in our exploratory work on dance and reinforcement learning [118], if
a suﬃcient environment and novelty term can be deﬁned, we can reward the
model based on the amount of novelty that is achieved when it generates new
examples. This may lead to output that conforms more closely to our perception
of interestingness [4].
Secondly, there may be other stimuli than a dancer’s previous position that
aﬀect where their movements are going next. Music is often an important aspect
of dance. In evaluating the generated dance movements, beat matching is a
common quality metric [74, 75]. Moving in time with a beat is a typical aspect of
recreational dancing and many choreographies are created using a count which
is often closely tied to the beats and bars of an accompanying musical piece. In
my work, I have attempted to use high-level features such as perceived pulse
clarity and sub-band spectral ﬂux, but a combination of high and low-level
features could result in more robust models capable of generating movements
that correspond to genre-speciﬁc musical styles. Li et al. [75] for example, use a
set of 35 musical features for every movement frame, including a 12-dimension
MFCC and one-hot encoded beat and peak indicators. However, it is important
to acknowledge that dance is not intrinsically intertwined with the auditory
perception of music. Looking outside of the sound-movement relationship we
may ﬁnd equally valuable features to include in training an AI dancer. In dance
improvisation, for example, students are often given a prompt in the form of
a concept or visual imagery to imagine as a starting point. Imagery, mood
or intention could all be viable alternatives or additions to the creation of an
AI dancer and may reveal connections between these factors and the learned
movement qualities.
The focus in this work has been on individual dance, excluding the added
complexities that result from group and pair dance settings. While dance is
perhaps most commonly performed and often also created in a group setting,
this choice has allowed us to narrow the scope somewhat and focus on individual
improvisation and choreographic exploration. When selecting movement features
for training the generative models we have made a conscious eﬀort to represent
the movements of the dancers in such a way that biological constraints are not
applied to the training data or the model’s output. This may complicate the
movement generation task as the model needs to learn these features from the
data in addition to the more subtle qualities such as sound-motion mappings.
Approaching dance generation through alternate movement features such as
ﬂuidity, the quantity of movement, and acceleration or annotating the data set
with Laban movement qualities instead or in addition to positional data might
aid in fostering more expressive movements.
51
5. Discussion
5.2.2
Future work
We are at the cusp of seeing how generative AI will truly aﬀect our lives and
artistic practices. Building on the work in this thesis by performing longitudinal
studies, analysing the perception of generative AI in a creative practice such
as dance as it changes over the next few years could potentially reveal insights
that would be transferable to other areas as well. In my meetings with various
dance practitioners, I have throughout this work been confronted with views on
AI and tech in dance which have challenged me to internalise a broader view
of what dance is. Notions of ownership and protection of a dancer’s creative
output have expanded my understanding of the potential negative impact an
AI dancer could have on a community. Participation in interdisciplinary forums
where an exchange of knowledge is encouraged and supported will further help
ensure that the technology is contributing something of value and is in a position
to integrate into existing practices by facilitating collaboration and artist-led
research.
Throughout this work, we have gained insight into the potential beneﬁt
of using AI-glitch and non-realism to break habitual movement patterns. It
is, however, interesting to consider the limits of this notion. How non-human
can we make an AI-dancer before it crosses the boundary into being perceived
only as noise? This assumption that virtual and physical agents beneﬁt from
having a somewhat human-like shape lies at the centre of many robotics projects,
particularly when the robot is intended to interact with humans [111]. This
assumption is however less obvious within artistic practice (see for example the
Future You installation mentioned in note 7). Investigating this boundary in
both an artistic and human-AI interaction setting could shine additional light
on preferences regarding AI interaction and perception.
We delve into concepts in our work that could be further explored beyond the
realm of generative AI. The data set collected as part of this thesis, for example,
could be explored through the lens of embodied music cognition research as it
contains recordings of dance improvisation synchronised to music. The data set,
model and code for generating sequences using diﬀerent sampling strategies is
also open for use in artistic practice. Perhaps the most multifaceted concept
touched upon in this thesis is our understanding of creativity, the facilitation of
creative work and aesthetic factors, contributing to something being considered
pleasing or interesting. These questions have been explored by various disciplines,
ranging from neuroscience to psychology and philosophy of mind. Consequently,
there are probably viewpoints and discoveries from these ﬁelds that we have not
been able to incorporate into our research. We hope, however, that in addition to
our contributions to a better understanding of how generative AI might integrate
into dance practice, our work can oﬀer some new perspectives to future work in
this sprawling landscape surrounding artistic practice, perception and creativity.
52
The impact of subjectivity
5.3
The impact of subjectivity
As dance is inherently expressive and interpretive, its evaluation is often rooted in
human emotions, cultural contexts, and artistic interpretations. This subjective
nature makes it challenging to devise a universal framework for training AI
models of dance and evaluating their output. Unlike traditional machine learning
tasks with quantiﬁable metrics, creativity deﬁes precise measurement. If our
model aims to predict weather patterns, for example, surprising and unrealistic
results would be highly problematic. But in the case of a model intended to
generate paintings, music or dance, some level of unpredictability is beneﬁcial.
The subjectivity challenge has played a large role in two aspects of this work
in particular: technical choices surrounding the AI models and the selection of
evaluation methods.
Throughout this work, choices have been made regarding model architectures
and data features to allow for variation and surprise. When exploring how we can
control the output of the model, we aim to maximise the variability of its output.
By thoroughly examining the parameters available during model inference, we
can generate both realistic sequences and sequences that are glitchy, noisy
and unpredictable. By looking into sound-motion mappings, we ﬁnd potential
correlations between movement features and sound, bringing attention to how
musical features aﬀect our assumptions of corresponding dance movements and
vice-versa. This knowledge can be used to either reinforce or contradict these
expectations, adding further to the perceived realism or unpredictability of the
generated sequences.
Due to the subjective nature of dance and generative models of creative
artefacts evaluation is naturally also impacted by notions of subjectivity. As
such, the methods of evaluating generated output have been selected to gain
access to multiple viewpoints, both audience, performer and choreographer.
Further, the collection of papers presented here combines quantitative metrics,
such as movement ﬂuidity and model accuracy with qualitative evaluations,
involving human feedback and practice-based approaches. This approach not
only embraces diversity in creativity but also fosters a deeper connection between
technology and the arts by encouraging collaborative exploration between artists
and researchers. The integration of AI models into artistic processes can catalyze
new conversations about the relationship between technology and humanity.
The juxtaposition of art and AI encouraged participants in our work to ponder
questions of identity, consciousness, and the boundaries of human creativity in
an increasingly technologically-driven world.
By acknowledging the inherent subjectivity of artistic practice through
centring variability and surprise and embracing a holistic evaluation approach,
future work can contribute to a more nuanced understanding of generative AI’s
place within creative pursuits.
53
Chapter 6
Conclusions
The main objective of the thesis has been to explore the creation and control
of generative dance, what AI dance can tell us about sound and movements
and how such a model might be applied in dance practice. The ﬁeld itself,
generative AI and generative AI for dance creation has evolved alongside this
work. As new methods have emerged and improved, the research questions of
this thesis have also been updated to reﬂect the current state of the art. There
are, for example, today several implementations of deep learning methods which
generate dance based on musical features. As these methods have evolved the
questions which remain are more related to exactly how this technology could
beneﬁt the community that has contributed their artistic output to the training
of these models. The later stages of my work have focused on this important
consideration.
The papers in the thesis demonstrate the use of generative deep-learning
methods in dance, both from a technological perspective and from the perspective
of dance practitioners. The main contributions of the thesis can be summarised
as follows:
Open-access data set of improvised dance:
A motion capture data set of improvised dance and synchronised music
has been collected and made available online as an open-access data set
for future research into sound-motion mappings, dance generation and
reproducibility of the work presented here.
Systematic overview of sampling strategies for the MDRNN model:
The generative potential of the use of the MDRNN model in movement
generation is presented in paper III. We oﬀer a systematic overview of the
eﬀects of various sampling strategies together with examples and sampling
code, available as an online resource.
Insight into AI sound-motion mappings:
Papers I and II provide examples of the use of MDRNNs in modelling
sound-motion mappings in sound-tracing and dance. Through a perceptual
judgement survey, we have shown that the perceived sound-motion
mappings of human dancers are more easily perceived than the AI-generated
equivalent and presented connections to related research within embodied
music cognition.
Distillation of dancers perspectives into generative AI for dance:
By implementing embodied, practice-based approaches in papers V and VI
we corroborate previous research on the importance of surprise and making
55
6. Conclusions
strange when designing a creative catalyst. The ﬁndings are presented as
concrete suggestions for the future development of AI dance.
Potential use-case in improvised dance:
In paper VI we present the use of AI-glitch as a potential use-case in
improvised dance with tests in a real-world scenario of improvisation
training. We further identify new perspectives on how an AI dancer might
ﬁt into a dancer’s practice which centres on the concepts of shared imagery,
novelty and embodiment.
Figure 6.1: Dancer and choreographer Diego Marín and 6A9 productions created a
video for the "Dancing Embryo" project using clips generated by our model. See full
video here.
Before concluding this thesis we return brieﬂy to the questions of the subjectivity
challenge outlined in the introduction. The concept of synergy arising between
humans and machines when one augments the ability of the other is a central
part of my own journey in marrying artistic work and computer science which
culminated in the TEDx talk I presented in 2021. In preparing to present my
research and ideas to a large audience the questions of the subjectivity challenge
began to take center stage. The process forced me to view the work from a
perspective outside of academia, outside of dance practice and outside of myself.
This became an important turning point for my work, where my interest in
generative AI began to shift somewhat from a practical, problem-solving mindset,
to what this potential synergy can mean to people.
In lieu of proposing a distinctive metric for “what makes an artistic output
"good"” my research has led me to the conclusion that a more compelling focus
is creating works that are subjectively interesting in favour of objectively good.
This implies creating models and facilitating interactions that have plasticity
and can generate a range of viable output from highly realistic to highly abstract.
As an AI practitioner interested in creativity, it can be tempting to look at
generative AI as a hammer and everything else as a nail. So when considering
the second question; “What should be the aim of a generative model of art?”
I have attempted to consider the views of the dance practitioners themselves.
Focusing their experiences and thoughts when suggesting future avenues for
56
research. While AI may never fully replace artists in any ﬁeld, it has and will
continue to aﬀect the way we perceive art.
Generative AI is developing quickly in many ﬁelds.
The eﬀect of this
technology on the creative communities on which they are modelled can pose
challenges but also foster discovery and transformation. As AI practitioners and
researchers, we are in a position to ensure that the use and development of these
technologies also beneﬁt these communities. Once generative tools are in the
hands and intention of artists we begin to see how these new tools can aﬀect
the world of art and performance. In the context of AI-generated dance, this
may take the form of using the unreal aspects of an AI dancer to exaggerate,
interpret or extrapolate a dancer’s movement, forming interesting duets between
AI and human dancers or allowing dancers to explore their own and others’
movement languages. However, I acknowledge that my above predictions may
be limited. The impact of AI-generated dance in the future may take forms that
completely escape me at present, as artists continue to incorporate, challenge
and manipulate the development and use of generative AI. Either way, I look
forward to being surprised.
57
Bibliography
[1]
Alluri, Vinoo and Toiviainen, Petri. “Exploring perceptual and acoustical
correlates of polyphonic timbre”. In: Music Perception: An Interdisci-
plinary Journal vol. 27, no. 3 (2010), pp. 223–242.
[2]
Alvarado, Juan and Wiggins, Geraint A. “Meta-level Evaluation and
Transformational Creativity; An analysis of MEXICA”. In: Proceedings
of the Eleventh International Conference on Computational Creativity.
Coimbra, Portugal, 2020, pp. 173–176.
[3]
Auger, James. “Speculative design: crafting the speculation”. In: Digital
Creativity vol. 24, no. 1 (2013), pp. 11–35.
[4]
Berlyne, Daniel E. “Novelty, complexity, and hedonic value”. In: Percep-
tion & psychophysics vol. 8, no. 5 (1970), pp. 279–286.
[5]
Berman, Alexander and James, Valencia. “Kinetic dialogues: enhancing
creativity in dance”. In: Proceedings of the 2nd International Workshop
on Movement and Computing. Vancouver, British Columbia, Canada,
2015, pp. 80–83.
[6]
Berman, Alexander and James, Valencia. “Learning as performance:
Autoencoding and generating dance movements in real time”. In:
Proceedings of the 7th International Conference on Computational
Intelligence in Music, Sound, Art and Design: EvoMUSART. Springer.
Parma, Italy, 2018, pp. 256–266.
[7]
Bishop, Christopher M. Mixture density networks. Tech. rep. Aston
University, Birmingham, UK, 1994.
[8]
Bisig, Daniel. “Generative Dance-a Taxonomy and Survey”. In: Proceed-
ings of the 8th International Conference on Movement and Computing.
Chicago, Illinois, USA, 2022, pp. 1–10.
[9]
Blades, Hetty. “Creative computing and the re-conﬁguration of dance
ontology”. In: Proceedings of Electronic Visualisation and the Arts.
London, UK, 2012, pp. 221–228.
[10]
Bläsing, Bettina E and Sauzet, Odile. “My action, my self: Recognition
of self-created but visually unfamiliar dance-like actions from point-light
displays”. In: Frontiers in psychology vol. 9 (2018), pp. 110–118.
[11]
Boden, Margaret A. The creative mind: myths and mechanisms. 1991.
[12]
Boden, Margaret A and Edmonds, Ernest A. “What is generative art?”
In: Digital Creativity vol. 20, no. 1-2 (2009), pp. 21–46.
59
Bibliography
[13]
Braun, Virginia and Clarke, Victoria. “Reﬂecting on reﬂexive thematic
analysis”. In: Qualitative research in sport, exercise and health vol. 11,
no. 4 (2019), pp. 589–597.
[14]
Braun, Virginia and Clarke, Victoria. Thematic analysis. American
Psychological Association, 2012.
[15]
Broekens, Joost, Heerink, Marcel, Rosendal, Henk, et al. “Assistive social
robots in elderly care: a review”. In: Gerontechnology vol. 8, no. 2 (2009),
pp. 94–103.
[16]
Burger, Birgitta and Toiviainen, Petri. “MoCap Toolbox-A Matlab toolbox
for computational analysis of movement data”. In: Proceedings of the 10th
Sound and Music Computing Conference. Stockholm, Sweden, 2013.
[17]
Burger, Birgitta and Toiviainen, Petri. “See How It Feels to Move:
Relationships Between Movement Characteristics and Perception of
Emotions in Dance”. In: Human Technology vol. 16, no. 3 (2020).
[18]
Burton, Sarah Jane, Samadani, Ali-Akbar, Gorbet, Rob, and Kulić, Dana.
“Laban movement analysis and aﬀective movement generation for robots
and other near-living creatures”. In: Dance Notations and Robot Motion
(2016), pp. 25–48.
[19]
Camurri, Antonio, Lagerlöf, Ingrid, and Volpe, Gualtiero. “Recognizing
emotion from dance movement: comparison of spectator recognition and
automated techniques”. In: International journal of human-computer
studies vol. 59, no. 1-2 (2003), pp. 213–225.
[20]
Camurri, Antonio, Mazzarino, Barbara, Ricchetti, Matteo, Timmers,
Renee, and Volpe, Gualtiero. “Multimodal analysis of expressive gesture
in music and dance performances”. In: Proceedings of Gesture-Based
Communication in Human-Computer Interaction: 5th International
Gesture Workshop. Genova, Italy, 2003, pp. 20–39.
[21]
Caramiaux, Baptiste, Montecchio, Nicola, Tanaka, Atau, and Bevilacqua,
Frédéric. “Adaptive Gesture Recognition with Variation Estimation for
Interactive Systems”. In: ACM Transactions on Interactive Intelligent
Systems vol. 4, no. 4 (2014), pp. 1–34.
[22]
Carlson, Emily, Saari, Pasi, Burger, Birgitta, and Toiviainen, Petri.
“Dance to your own drum: Identiﬁcation of musical genre and individual
dancer from motion capture using machine learning”. In: Journal of New
Music Research (2020), pp. 1–16.
[23]
Carlson, Kristin, Pasquier, Philippe, Tsang, Herbert H, Phillips, Jordon,
Schiphorst, Thecla, and Calvert, Tom. “Cochoreo: A generative feature in
idanceForms for creating novel keyframe animation for choreography”. In:
Proceedings of the Seventh International Conference on Computational
Creativity. Paris, France, 2016, pp. 380–387.
[24]
Cohen, Selma Jeanne. “Avant-Garde Choreography”. In: Criticism vol. 3,
no. 1 (1961), pp. 16–35.
60
Bibliography
[25]
Crnkovic-Friis, Luka and Crnkovic-Friis, Louise. “Generative Choreogra-
phy using Deep Learning”. In: Proceedings of the Seventh International
Conference on Computational Creativity. Paris, France, 2016, pp. 272–277.
[26]
deLahunta, Scott. “Periodic convergences: Dance and computers”. In: S.
Dinklaand M. Leeker (eds.), Dance and Technology: Moving Towards
Media Productions, Alexander Verlag, 2002.
[27]
d’Inverno, Mark and McCormack, Jon. “Heroic versus collaborative AI
for the arts”. In: Proceedings of the 24th International Conference on
Artiﬁcial Intelligence. Buenos Aires, Argentina, 2015, pp. 2438–2444.
[28]
Erdem, Cagri, Schia, Katja Henriksen, and Jensenius, Alexander Ref-
sum. “Vrengt: A Shared Body–Machine Instrument for Music–Dance
Performance”. In: Proceedings of the International Conference on New
Interfaces for Musical Expression. Porto Alegre, Brazil, 2019, pp. 186–191.
[29]
Eriksson, Sara, Unander-Scharin, Åsa, Trichon, Vincent, Unander-
Scharin, Carl, Kjellström, Hedvig, and Höök, Kristina. “Dancing with
drones: Crafting novel artistic expressions through intercorporeality”. In:
Proceedings of the 2019 Conference on Human Factors in Computing
Systems. Glasgow, Scotland, UK, 2019, pp. 1–12.
[30]
Everything, Universal. Future You. 2019. url: https : / / www .
universaleverything.com/artworks/future-you (visited on 03/23/2023).
[31]
Fdili Alaoui, Sarah. “Making an interactive dance piece: Tensions in
integrating technology in art”. In: Proceedings of the 2019 Conference
on Designing Interactive Systems. San Diego, California, USA, 2019,
pp. 1195–1208.
[32]
Fdili Alaoui, Sarah, Françoise, Jules, Schiphorst, Thecla, Studd, Karen,
and Bevilacqua, Frédéric. “Seeing, sensing and recognizing Laban
movement qualities”. In: Proceedings of the Conference on Human Factors
in Computing Systems. Denver, Colorado, USA, 2017, pp. 4009–4020.
[33]
Fiebrink, Rebecca, Trueman, Daniel, Britt, N Cameron, Nagai, Michelle,
Kaczmarek, Konrad, Early, Michael, Daniel, MR, Hege, Anne, and
Cook, Perry R. “Toward understanding human-computer interaction
in composing the instrument”. In: Proceedings of the 2010 International
Computer Music Conference. New York, USA, 2010, pp. 135–142.
[34]
Fischer, Ronald and Milfont, Taciano L. “Standardization in psychological
research.” In: International Journal of Psychological Research vol. 3, no. 1
(2010), pp. 88–96.
[35]
Forsythe, William and Kaiser, Paul. “Dance geometry”. In: Performance
Research vol. 4, no. 2 (1999), pp. 64–71.
[36]
Fortnite. [video game]. Epic Games, 2017.
[37]
Foster, David. Generative deep learning: teaching machines to paint, write,
compose, and play. O’Reilly Media, 2019.
61
Bibliography
[38]
Françoise, J, Pasquier, P, Alemi, O, Françoise, J, and Pasquier, P.
“GrooveNet: real-time music-driven dance movement generation using
artiﬁcial neural networks”. In: Proceedings of SIGKDD 2017 Workshop
on Machine Learning for Creativity. Nova Scotia, Canada, 2017.
[39]
Fukayama, Satoru and Goto, Masataka. “Music content driven automated
choreography with beat-wise motion connectivity constraints”. In:
Proceedings of the 12th Sound and Music Computing Conference.
Maynooth, Ireland, 2015, pp. 177–183.
[40]
Gemeinboeck, Petra and Saunders, Rob. “Movement matters: How a
robot becomes body”. In: Proceedings of the 4th international conference
on movement Computing. London, UK, 2017, pp. 1–8.
[41]
Godøy, Rolf Inge. “Gestural aﬀordances of musical sound”. In: Musical
Gestures. Routledge, 2010, pp. 115–137.
[42]
Gonzalez-Sanchez, Victor E, Zelechowska, Agata, and Jensenius, Alexan-
der Refsum. “Correspondences between music and involuntary human
micromotion during standstill”. In: Frontiers in psychology vol. 9 (2018),
p. 1382.
[43]
Goodfellow, Ian, Pouget-Abadie, Jean, Mirza, Mehdi, Xu, Bing, Warde-
Farley, David, Ozair, Sherjil, Courville, Aaron, and Bengio, Yoshua.
“Generative adversarial nets”. In: Advances in neural information
processing systems vol. 27 (2014).
[44]
Graves, Alex. “Generating sequences with recurrent neural networks”. In:
arXiv preprint arXiv:1308.0850 (2013).
[45]
Ha, David and Eck, Douglas. “A neural representation of sketch drawings”.
In: arXiv preprint arXiv:1704.03477 (2017).
[46]
Ha, David and Schmidhuber, Jürgen. “Recurrent world models facilitate
policy evolution”. In: Advances in Neural Information Processing Systems.
2018, pp. 2450–2462.
[47]
Hiller Jr, Lejaren A and Isaacson, Leonard M. “Musical composition with
a high-speed digital computer”. In: Journal of the Audio Engineering
Society vol. 6, no. 3 (1958), pp. 154–160.
[48]
Hochreiter, Sepp and Schmidhuber, Jürgen. “Long Short-term Memory”.
In: Neural computation vol. 9 (1997), pp. 1735–80.
[49]
Holz, David. Midjourney. 2022. url: https://www.midjourney.com/
(visited on 10/11/2022).
[50]
Horčicovà, Monika. Memento Mori. 2013. url: https://monikahorcicova.
wordpress.com/prace/#jp-carousel-283 (visited on 12/02/2023).
[51]
Huang, Cheng-Zhi Anna, Vaswani, Ashish, Uszkoreit, Jakob, Shazeer,
Noam, Hawthorne, Curtis, Dai, Andrew M, Hoﬀman, Matthew D, and
Eck, Douglas. “Music Transformer: Generating Music with Long-Term
Structure”. In: arXiv preprint arXiv:1809.04281 (2018).
62
Bibliography
[52]
Huang, Rui, Zhang, Shu, Li, Tianyu, and He, Ran. “Beyond face rotation:
Global and local perception gan for photorealistic and identity preserving
frontal view synthesis”. In: Proceedings of the IEEE international
conference on computer vision. 2017, pp. 2439–2448.
[53]
Jensenius, Alexander Refsum, Skogstad, Ståle, Nymoen, Kristian, Tørre-
sen, Jim, and Høvin, Mats. “Reduced displays of multidimensional motion
capture data sets of musical performance”. In: Proceedings of the 7th
Triennial Conference of European Society for the Cognitive Sciences of
Music. 2009.
[54]
Jensenius, Alexander Refsum, Zelechowska, Agata, and Gonzalez Sanchez,
Victor Evaristo. “The musical inﬂuence on people’s micromotion when
standing still in groups”. In: Proceedings of the 14th Sound & Music
Computing Conference. Espoo, Finland, 2017, pp. 195–200.
[55]
Jones, Stephen. “Philippa Cullen: dancing the music”. In: Leonardo Music
Journal vol. 14, no. 1 (2004), pp. 65–73.
[56]
Jordanous, Anna. “A standardised procedure for evaluating creative
systems: Computational creativity evaluation based on what it is to be
creative”. In: Cognitive Computation vol. 4 (2012), pp. 246–279.
[57]
Karras, Tero, Laine, Samuli, and Aila, Timo. “A style-based generator
architecture for generative adversarial networks”. In: Proceedings of the
IEEE/CVF conference on computer vision and pattern recognition. Long
Beach, California, USA, 2019, pp. 4401–4410.
[58]
Kaspersen, Esbern Torgard, Górny, Dawid, Erkut, Cumhur, and Palamas,
George. “Generative choreographies: the performance dramaturgy of the
machine”. In: Proceedings of the 15th International Joint Conference
on Computer Vision, Imaging and Computer Graphics Theory and
Applications. Valletta, Malta, 2020, pp. 319–326.
[59]
Kelkar, Tejaswinee and Jensenius, Alexander Refsum. “Analyzing Free-
Hand Sound-Tracings of Melodic Phrases”. In: Applied Sciences vol. 8,
no. 1 (2018).
[60]
Kietzmann, Jan, Lee, Linda W, McCarthy, Ian P, and Kietzmann, Tim C.
“Deepfakes: Trick or treat?” In: Business Horizons vol. 63, no. 2 (2020),
pp. 135–146.
[61]
Kingma, Diederik P and Ba, Jimmy. “Adam: A method for stochastic
optimization”. In: arXiv preprint arXiv:1412.6980 (2014).
[62]
Kingma, Diederik P and Welling, Max. “Auto-Encoding Variational
Bayes”. In: stat vol. 1050 (2014), p. 1.
[63]
Knight, Heather and Simmons, Reid. “Expressive motion with x, y and
theta: Laban eﬀort features for mobile robots”. In: Proceedings of the
23rd IEEE International Symposium on Robot and Human Interactive
Communication. Edingburgh, Scotland, UK, 2014, pp. 267–273.
63
Bibliography
[64]
Ko, Hyung-Kwon, Park, Gwanmo, Jeon, Hyeon, Jo, Jaemin, Kim, Juho,
and Seo, Jinwook. “Large-scale text-to-image generation models for
visual artists’ creative works”. In: Proceedings of the 28th International
Conference on Intelligent User Interfaces. Sydney New South Wales,
Australia, 2023, pp. 919–933.
[65]
Kumar, Meha, Long, Duri, and Magerko, Brian. “Creativity Metrics for
a Lead-and-Follow Dynamic in an Improvisational Dance Agent”. In:
Proceedings of the Eleventh International Conference on Computational
Creativity. Coimbra, Portugal, 2020, pp. 276–283.
[66]
Laban, Rudolf. Modern educational dance. 1975.
[67]
Lapointe, François-Joseph and Époque, Martine. “The dancing genome
project: generation of a human-computer choreography using a genetic
algorithm”. In: Proceedings of the 13th annual ACM international
conference on Multimedia. 2005, pp. 555–558.
[68]
Lartillot, Olivier, Eerola, Tuomas, Toiviainen, Petri, and Fornari, Jose.
“Multi-Feature Modeling of Pulse Clarity: Design, Validation and
Optimization”. In: Proceedings of the Ninth International Conference
on Music Information Retrieval. Kobe, Japan, 2008, pp. 521–526.
[69]
Lartillot, Olivier and Toiviainen, Petri. “A Matlab toolbox for musical
feature extraction from audio”. In: Proceedings of the 10th edition of
the International Conference on Digital Audio Eﬀects. Bordeaux, France,
2007, pp. 237–244.
[70]
Ledig, Christian et al. “Photo-realistic single image super-resolution using
a generative adversarial network”. In: Proceedings of the IEEE conference
on computer vision and pattern recognition. Honolulu, Hawaii, USA, 2017,
pp. 4681–4690.
[71]
Lee, Hsin-Ying, Yang, Xiaodong, Liu, Ming-Yu, Wang, Ting-Chun, Lu,
Yu-Ding, Yang, Ming-Hsuan, and Kautz, Jan. “Dancing to music”. In:
Proceedings of the 33rd International Conference on Neural Information
Processing Systems. Vancouver, Canada, 2019, pp. 3586–3596.
[72]
Lee, Jieun, Heo, Jeongyun, Kim, Hayeong, and Jeong, Sanghoon.
“Fostering Empathy and Privacy: The Eﬀect of Using Expressive Avatars
for Remote Communication”. In: Proceedings of the 23rd International
Conference on Human-Computer Interaction. Virtual, 2021, pp. 566–583.
[73]
Lee, Jina and Marsella, Stacy. “Nonverbal Behavior Generator for
Embodied Conversational Agents”. In: Proceedings of the 6th International
Conference on Intelligent Virtual Agents. Marina Del Rey, California, USA,
2006, 243–255.
[74]
Li, Jiaman, Yin, Yihang, Chu, Hang, Zhou, Yi, Wang, Tingwu, Fidler,
Sanja, and Li, Hao. “Learning to generate diverse dance motions with
transformer”. In: arXiv preprint arXiv:2008.08171 (2020).
64
Bibliography
[75]
Li, Ruilong, Yang, Shan, Ross, David A, and Kanazawa, Angjoo. “Ai
choreographer: Music conditioned 3d dance generation with aist++”. In:
Proceedings of the IEEE/CVF International Conference on Computer
Vision. Montreal, Canada, 2021, pp. 13401–13412.
[76]
Liu, Lucas, Long, Duri, Gujrania, Swar, and Magerko, Brian. “Learning
movement through human-computer co-creative improvisation”. In:
Proceedings of the 6th International Conference on Movement and
Computing. Tempe, Arizona, USA, 2019, pp. 1–8.
[77]
Los del Rio. Macarena - The Bayside Boys Remix. Single. 1995.
[78]
Lovelace, Ada Augusta. “Notes by AAL [Augusta Ada Lovelace]”. In:
Taylor’s Scientiﬁc Memoirs vol. 3 (1843), pp. 666–731.
[79]
Mancini, Clara, Rogers, Yvonne, Bandara, Arosha K, Coe, Tony,
Jedrzejczyk, Lukasz, Joinson, Adam N, Price, Blaine A, Thomas, Keerthi,
and Nuseibeh, Bashar. “Contravision: exploring users’ reactions to
futuristic technology”. In: Proceedings of the SIGCHI conference on human
factors in computing systems. Atlanta, Georgia, USA, 2010, pp. 153–162.
[80]
Martin, Charles Patrick and Tørresen, Jim. “An Interactive Musical
Prediction System with Mixture Density Recurrent Neural Networks”.
In: Proceedings of the International Conference on New Interfaces for
Musical Expression. Porto Alegre, Brazil, 2019, pp. 260–265.
[81]
McCulloch, Warren S and Pitts, Walter. “A logical calculus of the ideas
immanent in nervous activity”. In: The bulletin of mathematical biophysics
vol. 5 (1943), pp. 115–133.
[82]
McGregor, Studio Wayne. "Living Archive: Creating Choreography with
Artiﬁcial Intelligence" Arts & Culture Google. 2020. url: https : / /
experiments.withgoogle.com/living-archive-wayne-mcgregor (visited
on 10/11/2022).
[83]
McLachlan, Geoﬀrey J and Basford, Kaye E. Mixture models: Inference
and applications to clustering. Vol. 38. M. Dekker New York, 1988.
[84]
Michalak, Johannes, Troje, Nikolaus F, Fischer, Julia, Vollmar, Patrick,
Heidenreich, Thomas, and Schulte, Dietmar. “Embodiment of sadness
and depression—gait patterns associated with dysphoric mood”. In:
Psychosomatic medicine vol. 71, no. 5 (2009), pp. 580–587.
[85]
Molnar, Vera. “Toward Aesthetic Guidelines for Paintings with the Aid
of a Computer”. In: Leonardo vol. 8, no. 3 (1975), pp. 185–189.
[86]
Moura, N, Fonseca, P, Goethel, M, Oliveira-Silva, P, Vilas-Boas, J, and
Serra, S. “The impact of visual display of human motion on observers’
perception of music performance”. In: Plos one vol. 18, no. 3 (2023).
[87]
Müller, Meinard and Röder, Tido. “Motion templates for automatic
classiﬁcation and retrieval of motion capture data”. In: Proceedings of the
2006 ACM SIGGRAPH/Eurographics symposium on Computer animation.
Vienna, Austria, 2006, pp. 137–146.
65
Bibliography
[88]
Müller, Meinard, Röder, Tido, and Clausen, Michael. “Eﬃcient content-
based retrieval of motion capture data”. In: Proceedings of the 2005 ACM
SIGGRAPH. Los Angeles, California, USA, 2005, pp. 677–685.
[89]
Ni, Pengpeng, Eichhorn, Alexander, Griwodz, Carsten, and Halvorsen,
Pål. “Fine-grained scalable streaming from coarse-grained videos”. In:
Proceedings of the 18th international workshop on Network and operating
systems support for digital audio and video. Braunschweig, Germany, 2009,
pp. 103–108.
[90]
Noll, A Michael. “Early Digital Computer Art at Bell Telephone
Laboratories, Incorporated”. In: Leonardo vol. 49, no. 1 (2016), pp. 55–65.
[91]
Nymoen, Kristian, Caramiaux, Baptiste, Kozak, Mariusz, and Torresen,
Jim. “Analyzing sound tracings: a multimodal approach to music informa-
tion retrieval”. In: Proceedings of the 1st international ACM workshop on
Music information retrieval with user-centered and multimodal strategies.
Scottsdale, Arizona, USA, 2011, pp. 39–44.
[92]
Nymoen, Kristian, Godøy, Rolf Inge, Jensenius, Alexander Refsum, and
Torresen, Jim. “Analyzing correspondence between sound objects and
body motion”. In: ACM Transactions on Applied Perception (TAP) vol. 10,
no. 2 (2013), pp. 1–22.
[93]
Oord, Aaron van den, Dieleman, Sander, Zen, Heiga, Simonyan, Karen,
Vinyals, Oriol, Graves, Alex, Kalchbrenner, Nal, Senior, Andrew, and
Kavukcuoglu, Koray. “WaveNet: A Generative Model for Raw Audio”.
In: Proceedings of the 9th ISCA Speech Synthesis Workshop. Sunnyvale,
California, USA, 2016, pp. 125–125.
[94]
OpenAI. “GPT-4 Technical Report”. In: arXiv preprint arXiv:2303.08774
(2023).
[95]
Perkins, Mike. “Academic Integrity considerations of AI Large Language
Models in the post-pandemic era: ChatGPT and beyond”. In: Journal of
University Teaching & Learning Practice vol. 20, no. 2 (2023), p. 07.
[96]
Pettee, Mariel, Shimmin, Chase, Duhaime, Douglas, and Vidrin, Ilya.
“Beyond Imitation: Generative and Variational Choreography via Machine
Learning”. In: Proceedings of the 10th International Conference on
Computational Creativity. Charlotte, North Carolina, USA, 2019, pp. 196–
203.
[97]
Plant, Nicola, Hilton, Clarice, Gillies, Marco, Fiebrink, Rebecca, Perry,
Phoenix, González Díaz, Carlos, Gibson, Ruth, Martelli, Bruno, and
Zbyszynski, Michael. “Interactive Machine Learning for Embodied
Interaction Design: A tool and methodology”. In: Proceedings of the
Fifteenth International Conference on Tangible, Embedded, and Embodied
Interaction. Salzburg, Austria, 2021, pp. 1–5.
[98]
Pollick, Frank E, Paterson, Helena M, Bruderlin, Armin, and Sanford,
Anthony J. “Perceiving aﬀect from arm movement”. In: Cognition vol. 82,
no. 2 (2001), pp. 51–61.
66
Bibliography
[99]
Poveda Yánez, Jorge and Fraisse, Amel. “It is not Dance, is Data: Gearing
Ethical Circulation of Intangible Cultural Heritage practices in the Digital
Space”. In: LREC 2022 Joint Workshop on Legal and Ethical Issues
in Human Language Technologies and Multilingual De-Identiﬁcation of
Sensitive Language Resources within the 13th Language Resources and
Evaluation Conference. Marseille, France, 2022, pp. 84–91.
[100]
Racknitz, Joseph Friedrich zu. Ueber Den Schachspieler Des Herrn Von
Kempelen Und Dessen Nachbildung. Leipzig [u.a.]: Breitkopf. 1789. url:
https://www.digi-hub.de/viewer/image/BV041097321/65/.
[101]
Ramesh, Aditya, Dhariwal, Prafulla, Nichol, Alex, Chu, Casey, and Chen,
Mark. “Hierarchical text-conditional image generation with clip latents”.
In: arXiv preprint arXiv:2204.06125 (2022).
[102]
Ritchie, Graeme. “Some empirical criteria for attributing creativity to
a computer program”. In: Minds and Machines vol. 17, no. 1 (2007),
pp. 67–99.
[103]
Roberts, Adam, Engel, Jesse, Raﬀel, Colin, Hawthorne, Curtis, and
Eck, Douglas. “A hierarchical latent vector model for learning long-term
structure in music”. In: Proceedings of the Thirty-ﬁfth International
Conference on Machine Learning. Stockholm, Sweden, 2018, pp. 4364–
4373.
[104]
Rokeby, David. A Very Nervous System. 1986-1990. url: http://www.
davidrokeby.com/vns.html (visited on 04/20/2023).
[105]
Sabet, Saeed Shaﬁee, Midoglu, Cise, Hassan, Syed Zohaib, Salehi, Pegah,
Baugerud, Gunn Astrid, Griwodz, Carsten, Johnson, Miriam, Riegler,
Michael Alexander, and Halvorsen, Pål. “Comparison of Crowdsourced
and Remote Subjective User Studies: A Case Study of Investigative Child
Interviews”. In: Proceedings of the 14th International Conference on
Quality of Multimedia Experience. Lippstadt, Germany, 2022, pp. 1–6.
[106]
Satchell, Liam, Morris, Paul, Mills, Chris, O’Reilly, Liam, Marshman, Paul,
and Akehurst, Lucy. “Evidence of big ﬁve and aggressive personalities
in gait biomechanics”. In: Journal of nonverbal behavior vol. 41, no. 1
(2017), pp. 35–44.
[107]
Schacher, Jan and Wei, Lia. “Gesture-Ink-Sound: Linking Calligraphy
Performance with Sound”. In: Proceedings of the 6th International
Conference on Movement and Computing. Tempe, Arizona, USA, 2019,
pp. 1–8.
[108]
Schiphorst, Thecla. “Merce Cunningham: Making dances with the
computer”. In: Merce Cunningham: Creative elements (2013), pp. 79–98.
[109]
Schmidhuber, Jürgen. “Developmental robotics, optimal artiﬁcial curiosity,
creativity, music, and the ﬁne arts”. In: Connection Science vol. 18, no. 2
(2006), pp. 173–187.
[110]
Schröter, Jens. “Artiﬁcial intelligence and the democratization of art”. In:
AI Critique| Volume (2019), p. 297.
67
Bibliography
[111]
Smith, Gina. Why Build Robots with Faces? 2018. url: https : / /
www.hansonrobotics.com/why-build-robots-with-faces/ (visited on
05/23/2022).
[112]
Strachey, Christopher. Earliest known recording of computer generated
music. 1951. url: https://www.bl.uk/collection-items/earliest-known-
recording-of-computer-generated-music (visited on 12/02/2023).
[113]
Sturm, Bob L and Ben-Tal, Oded. “Taking the models back to music
practice: evaluating generative transcription models built using deep
learning”. In: Journal of Creative Music Systems vol. 2, no. 1 (2017).
[114]
Tan, Wei Ren, Chan, Chee Seng, Aguirre, Hernán E, and Tanaka, Kiyoshi.
“ArtGAN: Artwork synthesis with conditional categorical GANs”. In:
Proceedings of the International Conference on Image Processing. Beijing,
China, 2017, pp. 3760–3764.
[115]
Tang, Taoran, Jia, Jia, and Mao, Hanyang. “Dance with melody: An
LSTM-autoencoder approach to music-oriented dance synthesis”. In:
Proceedings of the 26th ACM international conference on Multimedia.
Seoul, Republic of Korea, 2018, pp. 1598–1606.
[116]
Thelle, Notto JW and Fiebrink, Rebecca. “How do Musicians Experience
Jamming with a Co-Creative “AI”?” In: Proceedings of the Workshop on
Machine Learning for Creativity and Design at the Thirty-sixth Conference
on Neural Information Processing Systems (2022).
[117]
Toiviainen, Petri, Luck, Geoﬀ, and Thompson, Marc R. “Embodied
meter: hierarchical eigenmodes in music-induced movement”. In: Music
Perception vol. 28, no. 1 (2010), pp. 59–70.
[118]
Toverud Ruud, Markus, Hisdal Sandberg, Tale, Johan Vedde Tranvaag,
Ulrik, Wallace, Benedikte, Mojtaba Karbasi, Seyed, and Torresen,
Jim. “Reinforcement Learning Based Dance Movement Generation”.
In: Proceedings of the 8th International Conference on Movement and
Computing. Chicago, Illinois, USA, 2022, pp. 1–5.
[119]
Turing, Alan M. “I.—COMPUTING MACHINERY AND INTELLI-
GENCE”. In: Mind vol. LIX, no. 236 (1950), pp. 433–460.
[120]
Van Dyck, Edith, Vansteenkiste, Pieter, Lenoir, Matthieu, Lesaﬀre,
Micheline, and Leman, Marc. “Recognizing induced emotions of happiness
and sadness from dance movement”. In: PloS one vol. 9, no. 2 (2014),
e89773.
[121]
Vaswani, Ashish, Shazeer, Noam, Parmar, Niki, Uszkoreit, Jakob, Jones,
Llion, Gomez, Aidan N, Kaiser, Łukasz, and Polosukhin, Illia. “Attention
is all you need”. In: Proceedings of the Thirty-ﬁrst Conference on Neural
Information Processing Systems. Long Beach, California, USA, 2017,
pp. 5998–6008.
[122]
Wallace, Benedikte. What Happens When Art Meets AI? TEDxArendal,
2021. url: https://www.ted.com/talks/benedikte_wallace_what_
happens_when_art_meets_ai (visited on 02/02/2023).
68
Bibliography
[123]
Wallace, Benedikte, Nymoen, Kristian, Martin, Charles P., and Tør-
resen, Jim. DeepDance: Motion capture data of improvised dance.
https://doi.org/10.5281/zenodo.5838179. [Dataset]. 2019.
[124]
Wallis, Mick, Popat, Sita, McKinney, Joslin, Bryden, John, and Hogg,
David C. “Embodied conversations: performance and the design of a
robotic dancing partner”. In: Design Studies vol. 31, no. 2 (2010), pp. 99–
117.
[125]
Weizenbaum, Joseph. “ELIZA—a Computer Program for the Study
of Natural Language Communication between Man and Machine”. In:
Communications of the ACM vol. 9, no. 1 (1966), 36–45.
[126]
Wiggins, Geraint A. “A preliminary framework for description, analysis
and comparison of creative systems”. In: Knowledge-Based Systems vol. 19,
no. 7 (2006), pp. 449–458.
[127]
Wiggins, Geraint A. “Computational Creativity and Consciousness:
Framing, Fiction and Fraud”. In: Proceedings of the Twelfth International
Conference on Computational Creativity. México City, México, 2021,
pp. 182–191.
[128]
Yalta, Nelson, Watanabe, Shinji, Nakadai, Kazuhiro, and Ogata, Tetsuya.
“Weakly-supervised deep recurrent neural networks for basic dance step
generation”. In: Proceedings of the International Joint Conference on
Neural Networks. Budapest, Hungary, 2019, pp. 1–8.
[129]
Zhou, Qiushi, Chua, Cheng Cheng, Knibbe, Jarrod, Goncalves, Jorge,
and Velloso, Eduardo. “Dance and Choreography in HCI: A Two-Decade
Retrospective”. In: Proceedings of the 2021 CHI Conference on Human
Factors in Computing Systems. Virtual, 2021, pp. 1–14.
69
Papers
Paper I
Tracing from Sound to Movement
with Mixture Density Recurrent
Neural Networks
B. Wallace, C.P. Martin, K. Nymoen
Published in 6th International Conference on Movement and Computing
I
73
Tracing from Sound to Movement with Mixture Density
Recurrent Neural Networks
Benedikte Wallace
University of Oslo, Norway
RITMO, Department of Informatics
benediwa@ifi.uio.no
Charles P. Martin∗
University of Oslo, Norway
RITMO, Department of Informatics
charlepm@ifi.uio.no
Kristian Nymoen
University of Oslo, Norway
RITMO, Informatics & Musicology
kristian.nymoen@imv.uio.no
LSTM
MDN
Sound
Movement
Generated movement
Figure 1: Generating sound-tracings with mixture density recurrent neural network (MDRNN). Sound features and previous
movements are inputs to the network and the outputs are predicted movements.
ABSTRACT
In this work, we present a method for generating sound-tracings
using a mixture density recurrent neural network (MDRNN). A
sound-tracing is a rendering of perceptual qualities of short sound
objects through body motion. The model is trained on a dataset
of single point sound-tracings with multimodal input data and
learns to generate novel tracings. We use a second neural network
classifier to show that the input sound can be identified from gener-
ated tracings. This is part of an ongoing research effort to examine
the complex correlations between sound and movement and the
possibility of modelling these relationships using deep learning.
CCS CONCEPTS
• Computing methodologies → Neural networks; • Applied
computing → Sound and music computing; • Information
systems → Multimedia and multimodal retrieval.
KEYWORDS
Sound-Tracing, Mixture-Density Networks, Recurrent Neural Net-
works, Music Information Retrieval
ACM Reference Format:
Benedikte Wallace, Charles P. Martin, and Kristian Nymoen. 2019. Tracing
from Sound to Movement with Mixture Density Recurrent Neural Networks.
In 6th International Conference on Movement and Computing (MOCO ’19),
October 10–12, 2019, Tempe, AZ, USA. ACM, New York, NY, USA, 4 pages.
https://doi.org/10.1145/3347122.3371376
∗Also with Australian National University, Canberra.
Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
MOCO ’19, October 10–12, 2019, Tempe, AZ, USA
© 2019 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-7654-9/19/10.
https://doi.org/10.1145/3347122.3371376
1
INTRODUCTION
Our perception of sound is intrinsically entangled to our perception
of movement [6]. Hearing a sound involves not only bottom-up
processing of sonic features but also a top-down understanding of
what sort of objects and sound-producing actions were involved
in creating the sound. Inputs to different sensory modalities have
common references in metaphoric language (e.g., bright, soft, high,
energetic), and such cross-modal metaphors have been studied
extensively in order to understand human perception. One such
approach is to study gestural renderings of short sounds, what
Godøy et al. [7] refer to as sound-tracing, noting that people connect
sonic shapes to gestural shapes.
The research presented in this paper is motivated by studies of
sound-tracing. We ask to what extent an artificial neural network
is able to mimic humans in gestural rendering of sounds. This task
is particularly challenging for several reasons. First, because the
neural network needs to model both human movement constraints
and auditory perception, as well as the bridge between the two
modalities, and secondly, because humans do not seem to use any
single strategy for coupling auditory features to movement features.
Rather, a variety of strategies for mapping between sound and
movement have been identified [3, 11, 17]. To account for a range
of equally valid strategies for rendering gesture from sound, we
employ a mixture density network (MDN) architecture whereby
the outputs of an artificial neural network are interpreted to be
the parameters of a Gaussian mixture model (GMM). A GMM can
potentially model these multiple strategies simultaneously and
be sampled to provide a number of valid predictions. Since both
sound and movement are unfolding in time, we use a recurrent
neural network (RNN) with long short-term memory (LSTM) units
which make predictions based on the network’s state at previous
time steps, making it possible for the network to learn temporal
dependencies.
The mixture density recurrent neural network (MDRNN) em-
ployed in our work learns to generate sound-tracings using multi-
modal training data. We further experiment with sampling from
the probability distribution of the MDRNN with different degrees of
MOCO ’19, October 10–12, 2019, Tempe, AZ, USA
Benedikte Wallace, Charles P. Martin, and Kristian Nymoen
randomness. By way of visual inspection of the tracings generated
by this model, we find that the results produce tracings similar to
those found in the training data. Further, we find that in training
a classifier to identify which of the sounds the generated tracings
correspond to we achieve an accuracy of approximately 89% indi-
cating that there are clear differences between, and commonalities
within tracings conditioned on each sound.
In the following section, we provide a brief overview of relevant
background. In Section 3 we present the dataset and method applied
in our project. In Section 4 we present and discuss the results for
the generated sound-tracings before concluding and looking ahead
in Section 5.
2
BACKGROUND
2.1
Sound-tracing
Initial exploratory studies of tracing sounds were performed by
Godøy et al. in 2006, with participants tracing on a 2D tablet [7].
Küssner and Leech-Wilkinson [12] later documented that musical
training had an effect on the choice of strategy used for mapping
between sound and shape in tablet drawings. In following years
several studies have been performed using motion capture technol-
ogy in order to record tracings in three dimensions [3, 11, 16, 17].
Although participants move freely, most participants apply a lim-
ited selection of strategies to solve the task. Taking the tracing
of musical pitch as an example, Kelkar and Jensenius identified a
total of 6 different strategies for tracings of melodic contour [11]; a
broader picture than several previous studies that focused on the
high correlation between pitch/brightness and vertical movement
[5, 16, 18]. Others have also shown a relation between pitch and
hand distance, strategies with both strongly positive and strongly
negative correlation between the two were found [11, 17].
2.2
Sequence prediction
The challenge addressed in our paper is a sequence prediction
problem—predicting future time steps based on previous and cur-
rent events. Previous research on movement synthesis have applied
Restricted Boltzmann Machines [1], Convolutional Autoencoders
[10], and recurrent neural networks (RNN) [4]. We have chosen an
RNN architecture, which is a suitable choice for sequence predic-
tion tasks which require that the temporal aspects of the data are
represented. The recurrent connections between neurons in such
networks allow the model to retain information between time steps
in its internal state. Thereby, the output of the RNN at each time
step is affected by the information learned at previous time steps.
This makes RNNs a suitable choice for many sequence prediction
tasks such as drawings [9], chord progressions [13], text [19], and
movement.
Mixture density networks (MDNs) [2] treat the outputs of a neu-
ral network as the parameters of a Gaussian mixture model, which
can be sampled to generate real-valued predictions. This approach
has the advantage of both generating real-valued predictions, as
well as control over the diversity and “randomness” of sampling,
and control over the number of mixture components that allow
training to account for situations where multiple predictions could
be considered equally suitable. An RNN can be combined with an
(a) Tracing of sound 7
(b) Tracing of sound 4
Figure 2: Sound-tracing examples from the original sound-
tracing experiment [16] which constitutes our training data.
Table 1: Dataset description
Data type
No. Examples
Variables
Representation
Sound files
10
Pitch, Timbre, Loudness
128 bin Mel-spectrograms.
Frame rate 100 Hz
Motion capture
410, each paired
with one sound file
X, Y, Z position
Differentiated X, Y, Z data.
Frame rate 100 Hz
MDN to form an MDRNN that can make real-valued predictions
based on a sequence of inputs.
To optimize an MDN, we minimize the negative log-likelihood
of sampling true values from the predicted GMM for each example.
The probability density function (PDF) is used to obtain this likeli-
hood value. In our case, the GMM consists of K n-variate Gaussian
distributions. For simplicity in the PDF, these distributions are re-
stricted to having a diagonal covariance matrix, and thus the PDF
has the form:
p(θ;x) =
K
Õ
k=1
πkN(µk, Σk;x)
(1)
Where π are the mixing coefficient, µ, the Gaussian distribution
centres and Σ the covariance matrices. The loss function in our
system is calculated by the keras-mdn-layer [14] Python package
which makes use of Tensorflow’s probability distributions package
to construct the PDF.
MDRNNs are becoming well-established tools in the generation
of creative data. They have previously been applied to musical
sketches in two dimensions as part of a smartphone app [15], to
sketches [9], and handwriting [8]. While an MDRNN has previously
been applied to motion capture data [4], until now, it has not been
employed to model sound-to-gesture relationships.
3
METHODS AND MATERIALS
3.1
Dataset
The dataset used to construct training data for the MDRNN is
summarized in Table 1. It consists of 10 sound files and 410 motion
capture recordings of 15 subjects moving a rod through the air in
order to trace each sound [16]. The motion capture recordings were
time-synchronised to the sound playback. A NaturalPoint Optitrack
motion capture system with 7 Flex V100 cameras was used to track
the 3D position of the tip of the rod at a frame rate of 100 Hz. Figure
2 displays examples of sound-tracings recorded for sound 4 and 7.
3.2
Pre-processing
The motion recordings contained 50 data frames prior to the sound
file playback. The recordings were trimmed to only contain those
Tracing from Sound to Movement with Mixture Density Recurrent Neural Networks
MOCO ’19, October 10–12, 2019, Tempe, AZ, USA
15
units
15
units
[mel spectrogram,
      dx, dy, dz ]
LSTM-RNN Layers
  Mixture 
Distribution
π
μ
δ
[dx, dy, dz] t
t - 1
i
i
i
Figure 3: The MDRNN structure, two layers of LSTM units
are followed by the MDN layer. The resulting GMM can be
sampled to produce the next movement.
data frames where sound was played back. To avoid starting posi-
tion bias we used the first finite difference between motion capture
frames as training data.
The sound files, synthesized in Max/MSP, range in duration
from 2 to 4 seconds, and vary in pitch, timbre and loudness. For
the purpose of this experiment, mel-scaled spectrograms with 128
Mel bands were extracted from each sound. The sound files have a
sample rate of 44.1 kHz, by selecting a hop length of 441 samples
the time scale of the spectrograms correspond to the 100 fps motion
capture recordings.
3.3
Experiment design
Two versions of the MDRNN were produced to experiment with
predicting the movement features present in the dataset. The first
version took only movement as input; while the second took both
movement and sound features. More precisely, for each time step
t, the movement-only network attempts to map (dxt,dyt,dzt ) →
(dxt+1,dyt+1,dzt+1). The sound-and-movement network maps
(m1, . . . ,m128,dxt,dyt,dzt ) → (dxt+1,dyt+1,dzt+1) where themi
are 128 mel-scaled frequency weights obtained through a STFT
procedure. The aims of our experiments were:
(1) To determine whether convincing movement traces can be
obtained using the movement-only network and our dataset.
(2) To determine whether the addition of sound features allow
movements to be controlled by conditioning prediction on a
certain sound.
For validation of the sound-and-movement network, an auto-
mated classification of generated traces was performed by training
a neural network that attempts to identify which sound was used to
generate a trace. This classifier can tell us whether different sound
inputs lead to recognisable traces. The network comprises three
layers of 30 LSTM units and a dense layer with softmax activation
(19030 parameters) and operates on 100-step long sequences of
movement data. Two versions of the classifier were trained, one
on the original sound-tracing dataset and one on a set of 1000
generated tracings—100 for each of the 10 sounds.
3.4
MDRNN and Training
The generative RNN used in this work consists of two layers of
LSTM cells each with 15 hidden units. The outputs of the second
LSTM layer are in turn connected to a Mixture-Density network.
The model learns to estimate the mean, standard deviation and
weight of 5 normal distributions. Figure 3 shows an overview of
the MDRNN architecture. This configuration corresponds to 3,560
Figure 4: Examples of tracings generated by the MDRNN
trained only on motion capture data with no sound features
parameters for the movement-only model, and 11,800 parameters
for the sound-and-movement model.
Both models were trained using the Adam optimizer and on
epochs of shuffled data until the loss on a validation set (10% of
the dataset) failed to improve for five consecutive epochs. The
movement-only network trained for 50 epochs with a final valida-
tion loss of 2.96 and the sound-and-movement network trained for
40 epochs with a final validation loss of 4.49.
4
RESULTS AND DISCUSSION
4.1
Movement-only tracings
As a starting point for this exploration, the model is trained on the
movement of the rod without including any information pertaining
to the sounds the subjects were moving to. During training, the mo-
tion capture recordings are segmented into overlapping sequences
of 100 time steps, with each time step having a corresponding tar-
get value consisting of the position variation in the following time
step. Figure 4 shows examples of tracings generated by this model.
The examples show that the network generates reasonable motion
sequences with some variability.
4.2
Sound-and-Movement Tracings
To generate movement using sound and motion features in com-
bination, the 128 Mel bands are appended to the three position
variables associated with the current time step of each training
example. Further, these motion and sound examples are segmented
into overlapping sequences of 100 time steps, equivalent to 1 sec-
ond. The target values used in training the network are the same as
when generating movement from only motion data, the 3 position
values for the next time step. Thus, the model learns to predict
the upcoming position of the rod given only the current sound
features and its preceding position. When sampling a prediction
from the output of our MDN layer we can choose to increase the
width and/or height of the Gaussian distributions, the mixture com-
ponents of our model. By scaling σ, the standard deviations, or
π, the mixing weights, we affect the probability of sampling from
certain regions of the probability distribution for our three position
variables. These values affect the smoothness of generated tracings.
Figures 5 and 6 show tracings generated when using sound 7 and
sound 4 to condition the predictions and sampling with low and
high randomness respectively. The sounds were chosen arbitrarily
to give a comparison between original and generated tracings and
the effect of changes in sampling temperature.
The tracings generated by our model are remarkably similar
within a group of tracings to the same sound input. There are
MOCO ’19, October 10–12, 2019, Tempe, AZ, USA
Benedikte Wallace, Charles P. Martin, and Kristian Nymoen
(a) Tracing of sound 7
(b) Tracing of sound 4
Figure 5: Tracings generated by the MDRNN sampled at low
temperature σ = 0.0, π = 1.0
(a) Tracing of sound 7
(b) Tracing of sound 4
Figure 6: Tracings generated by the MDRNN sampled at σ =
0.5, π = 2.5
also quite clear differences between groups that were generated
to different sounds. To assess the within-group similarities and
between-group differences we perform an automated classifica-
tion of the generated sound-tracings using an LSTM classifier. An
accuracy of 89% was obtained on a hold out set of 250 traces. In
comparison, when training the classifier on the original dataset, an
accuracy of 75% was obtained on a hold-out set of 62 traces, similar
to the previously reported classification accuracy for this dataset
using a Support Vector Machine classifier [16]. This indicates that
there is more variability in the original dataset than in the set of
generated tracings.
Although the classifier is able to distinguish between the sound-
tracings, the perceptual connection between sound-tracing to the
sonic input is vaguer. When attempting to classify the generated
tracings using the classifier trained on the original sound-tracing
data we achieve an accuracy of only 17%. While it is easy to make
a connection between the tracing and the sound in the original
dataset, the perceptual link between generated tracing and sound
is less clear. The lack of such a link may come down to a number
of things, such as the size of dataset or the data representation, and
indicates that there is still a way to go before our model mimics
human cognition of sound and movement.
5
CONCLUSIONS AND FUTURE WORK
This paper presents a novel system for generating sound-tracings
using multi-modal input consisting of sound and movement. The
proposed method involves training an MDRNN on motion cap-
ture data and sound from a sound-tracing experiment. The trained
model generates new instances of sound-tracings using the sound
features of mel-scaled spectrograms. Previous work has shown that
humans can express movements related to sounds in a reliable way.
This work contributes new evidence that similar couplings can be
learned and generated by an ML system. In future work, we will
examine additional metrics for evaluating the tracings generated
by the model. We wish to examine more closely the similarities
between tracings relating to the same sound, and also what sound-
tracing features are shared between the original data and the gener-
ated tracings. Further, we intend to perform additional experiments
on generating movement from sound using more complex data
such as choreography using full-body motion capture and music.
ACKNOWLEDGMENTS
This work was partially supported by the Research Council of
Norway through its Centres of Excellence scheme, project number
262762.
REFERENCES
[1] Omid Alemi, Jules Françoise, and Philippe Pasquier. 2017. GrooveNet: Real-
time music-driven dance movement generation using artificial neural networks.
networks 8, 17 (2017), 26.
[2] Christopher M. Bishop. 1994. Mixture density networks. Technical Report. Aston
University, Birmingham, UK. http://publications.aston.ac.uk/373/
[3] Baptiste Caramiaux, Frédéric Bevilacqua, and Norbert Schnell. 2010. Towards a
Gesture-Sound Cross-Modal Analysis. In Gesture in Embodied Communication and
Human-Computer Interaction, Stefan Kopp and Ipke Wachsmuth (Eds.). Springer,
Berlin Heidelberg, 158–170.
[4] Luka Crnkovic-Friis and Louise Crnkovic-Friis. 2016. Generative choreography
using deep learning. In Proceedings of the Seventh International Conference on
Computational Creativity. 272–277.
[5] Zohar Eitan and Roni Y. Granot. 2006. How Music Moves: Musical Parameters
and Listeners’ Images of Motion. Music Perception 23, 3 (2006), pp. 221–248.
[6] Rolf Inge Godøy. 2018. Sonic Object Cognition. In Springer Handbook of Systematic
Musicology, Rolf Bader (Ed.). Springer, Berlin, Heidelberg, 761–777.
[7] Rolf Inge Godøy, Egil Haga, and Alexander Refsum Jensenius. 2006. Exploring
music-related gestures by sound-tracing: A preliminary study. In Proceedings
of the COST287-ConGAS 2nd International Symposium on Gesture Interfaces for
Multimedia Systems (GIMS2006). 27–33.
[8] Alex Graves. 2013. Generating sequences with recurrent neural networks. arXiv
eprints arXiv:1308.0850 (2013).
[9] David Ha and Douglas Eck. 2017. A neural representation of sketch drawings.
arXiv eprints arXiv:1704.03477 (2017).
[10] Daniel Holden, Jun Saito, Taku Komura, and Thomas Joyce. 2015. Learning
motion manifolds with convolutional autoencoders. In SIGGRAPH Asia 2015
Technical Briefs. ACM, 18.
[11] Tejaswinee Kelkar and Alexander Refsum Jensenius. 2018. Analyzing Free-Hand
Sound-Tracings of Melodic Phrases. Applied Sciences 8, 1 (2018).
[12] Mats B. Küssner and Daniel Leech-Wilkinson. 2014. Investigating the influence
of musical training on cross-modal correspondences and sensorimotor skills in a
real-time drawing paradigm. Psychology of Music 42, 3 (2014), 448–469.
[13] Hyungui Lim, Seungyeon Rhyu, and Kyogu Lee. 2017. Chord Generation from
Symbolic Melody Using BLSTM Networks. In Proceedings of the 18th International
Society for Music Information Retrieval Conference. 621–627.
[14] Charles Martin. 2018. keras-mdn-layer: Python Package.
https://doi.org/10.
5281/zenodo.1482348
[15] Charles Patrick Martin and Jim Torresen. 2018. RoboJam: A musical mixture
density network for collaborative touchscreen interaction. In Int’l. Conference on
Computational Intelligence in Music, Sound, Art and Design. Springer, 161–176.
[16] Kristian Nymoen, Kyrre Glette, Ståle Skogstad, Jim Torresen, and Alexander R
Jensenius. 2010. Searching for Cross-Individual Relationships between Sound and
Movement Features using an SVM Classifier. In Proceedings of the International
Conference on New Interfaces for Musical Expression. 259–262.
[17] Kristian Nymoen, Rolf Inge Godøy, Alexander Refsum Jensenius, and Jim Torre-
sen. 2013. Analyzing correspondence between sound objects and body motion.
ACM Transactions on Applied Perception 10, 2 (2013), 9:1–9:22.
[18] Elena Rusconi, Bonnie Kwan, Bruno L. Giordano, Carlo Umiltà, and Brian Butter-
worth. 2006. Spatial representation of pitch height: the SMARC effect. Cognition
99, 2 (2006), 113 – 129.
[19] Ilya Sutskever, James Martens, and Geoffrey E Hinton. 2011. Generating text with
recurrent neural networks. In Proceedings of the 28th International Conference on
Machine Learning (ICML-11). 1017–1024.
Paper II
Towards Movement Generation
with Audio Features
B. Wallace, C.P. Martin, J. Torresen, K. Nymoen
Published in 11th International Conference on Computational Creativity
II
79
Towards Movement Generation with Audio Features
Benedikte Wallace1, Charles P. Martin2, Jim Torresen1 and Kristian Nymoen1
1RITMO Center, University of Oslo
2Australian National University
benediwa@iﬁ.uio.no
Abstract
Sound and movement are closely coupled, particularly
in dance. Certain audio features have been found to
affect the way we move to music. Is this relationship
between sound and movement something which can be
modelled using machine learning? This work presents
initial experiments wherein high-level audio features
calculated from a set of music pieces are included in a
movement generation model trained on motion capture
recordings of improvised dance. Our results indicate
that the model learns to generate realistic dance move-
ments which vary depending on the audio features.
Introduction
Expressive movement is an intrinsic part of human life.
Hand gestures, body language as well as dance can efﬁ-
ciently convey an emotional state. Simple movement pat-
terns such as gait or arm movement can allow us to detect
characteristics such as gender, personality or mood (Micha-
lak et al., 2009; Pollick et al., 2001; Satchell et al., 2017). As
such, a better understanding of body motion, and the anal-
ysis and generation of motion data is important to further
develop ﬁelds such as human-robot interaction and human
activity recognition. For dance movement in particular, gen-
erative models have potential as artistic tools for animation
and choreography.
Research in embodied music cognition has identiﬁed sev-
eral audio features that are relevant to how we move to mu-
sic. Burger et al.’s (2013) work suggests that several map-
pings exist between different aspects of music and music-
induced movement. The presence of a clear beat, for exam-
ple, was shown to translate to faster movements of head and
hands.
The work presented here is part of an ongoing research
effort to examine how deep learning can be used to capture
salient features of human movement, and especially dance
movement, using full-body motion capture data and sound.
As part of this work, we have collected a dataset of motion
capture recordings of dance improvisation performed to six
different musical stimuli. The improvisations are performed
by experienced dancers and use contemporary music styles.
Here, we present the results of training a generative mix-
ture density recurrent neural network (MDRNN) on our mo-
tion data and audio features which have been shown to affect
Figure 1: The 43 reﬂective markers translated to 22 points
certain aspects of movement to music. Without the inclusion
of audio features, the MDRNN is able to generate sequences
of movement which are (subjectively) realistic variations of
the underlying training data. The results presented here indi-
cate that the model retains this ability to produce movement
variations when audio features are added. Our ﬁndings sug-
gest that the model additionally learns that different audio
features affect the way the body moves.
Motion Capture Data
Our dataset contains 54 one minute motion capture record-
ings of improvised dance performed by three experienced
dancers. Each dancer performs three one minute improvi-
sations to six different musical stimuli.
The dataset was
recorded using a Qualisys optical motion capture system
with 12 Oqus 300/400 series cameras which capture 43 re-
ﬂective markers worn by the dancers. Figure 1 shows how
the 43 marker positions were reduced to a 22 point skeleton
representation using the MoCap Toolbox 1.5 (Burger and
Toiviainen, 2013). Small gaps in the data were spline-ﬁlled
using Qualisys Track Manager 2019.3 and a 2nd degree But-
terworth ﬁlter with a .03Hz cutoff was applied to remove any
marker jitter.
Recordings in our dataset have been normalized so that
the root marker (a weighted average of markers 41, 42, 6 and
7 in Figure 1) is centred at the origin. Body segment lengths
are averaged across the three dancers ensuring that the data
is invariant to global position and individual body dimen-
sions. The data was captured at 240Hz and downsampled to
30Hz before model training to reduce the size of each ex-
ample. The resulting 54 data tensors consist of 1800 frames
(60 seconds at 30Hz) with 3-dimensional positions for each
of the 22 points.
Two full motion capture recordings were withheld for
testing while the remaining 52 examples were split into two
sets, 80% were used for training and 10% for validation.
Each example has been sliced into overlapping sequences
of 300 frames and the spatial dimensions of each of the 22
points are scaled using min-max normalisation. The input to
our model thereby consists of 78000 overlapping sequences
of 300 frames with their corresponding audio features. The
model performs a sequence-to-sequence mapping between
the training examples (including audio features) and the
shifted sequence of motion capture frames (excluding audio
features – the model only predicts motion).
Representing sound
There are several aspects to consider when selecting ap-
propriate audio features to represent the music examples
to which the dancers were improvising. Several previous
works (Fukayama and Goto, 2015; Lee et al., 2019; Seo
et al., 2013) have largely focused on the rhythmical, beat-
matching aspects when generating dance.
Although the presence of a clear beat can affect our urge
to move, moving in sync with a beat is only one of sev-
eral ways musical features inﬂuence the way we move. For
the experiments presented here, we have chosen to use two
high-level rhythm- and timbre-related features: pulse clarity
and sub-band spectral ﬂux. Previous work by Burger et al.
(2013) has shown connections between pulse clarity (Lar-
tillot et al., 2008) and overall body movement, as well as
sub-band spectral ﬂux and movement of the head and hands.
Pulse clarity is a high-level feature which measures how
clearly the underlying pulse of the music is perceived. Pulse
clarity is estimated using the overall entropy of the en-
ergy distribution of the frequency spectrum within a musical
piece. We calculate a series of pulse clarity values for each
musical stimuli using a sliding window of 5 seconds and a
hop size of 0.08 seconds. This gives us a time series wherein
each value corresponds to a single frame of the motion cap-
ture data.
Spectral sub-band ﬂux measures spectral changes in dif-
ferent frequency bands of an audio signal. Alluri and Toivi-
ainen (2010) found that the sub-band ﬂuctuations in the re-
gion between 50 Hz and 200 Hz are related to the perceived
“fullness” of a musical piece, while ﬂuctuations in the region
of 1600 Hz and 6400 Hz were linked to the perceived “ac-
tivity” of the piece. These sub-bands also correspond to ac-
tivity from rhythmic instruments such as kick drum and bass
guitar for the lower frequency band and hi-hat and cymbals
for the higher range.
We extract two frequency bands, one low-frequency band
(50 Hz - 100 Hz) and one high-frequency band (3200 Hz -
6400 Hz) from the six musical stimuli. The spectral ﬂux is
then calculated using the same window and hop size as for
the pulse clarity values resulting in a single sub-band ﬂux
value for each of the two bands for every frame of motion
Figure 2: Sampling from the MDRNN. One frame of mo-
cap data and audio features is sent through the model. The
MDRNN outputs the parameters of a mixture distribution
which is sampled to generate the next frame.
capture data. The audio features are appended to each data
frame of the motion capture data to create the tensors used
to train the generative mixture density recurrent neural net-
work.
Mixture Density Recurrent Neural Networks
Mixture density networks (MDNs) (Bishop, 1994) treat the
outputs of a neural network as the parameters of a Gaus-
sian mixture model (GMM), which can be sampled to gen-
erate real-valued predictions. A GMM can be derived us-
ing the mean, weight and standard deviation of each com-
ponent. The number of components needed to accurately
represent the data is not known and is treated as a hyper-
parameter for our model. For the study outlined here, we
have used 4 components. By combining a recurrent neural
network (RNN) with an MDN to form an MDRNN we can
make real-valued predictions based on a sequence of inputs.
Figure 2 shows the model architecture of the MDRNN used
in this work. The RNN consists of three layers of LSTM
cells (Hochreiter and Schmidhuber, 1997). The three LSTM
layers contain 1024, 512 and 256 hidden units respectively.
The outputs of the third LSTM layer are in turn connected
to an MDN. The LSTM layers learn to estimate the mean
(µ), standard deviation (σ) and weight (π) of the 4 Gaus-
sian distributions of the MDN. This approach has the advan-
tage of control over the diversity and “randomness” of sam-
pling, and control over the number of mixture components
that allow training to account for situations where multiple
predictions could be considered equally suitable. MDRNNs
have previously been applied to various other tasks such as
sketches (Ha and Eck, 2017), handwriting (Graves, 2013),
and music control generation (Martin and Torresen, 2019).
To optimize an MDN, we minimize the negative log-
likelihood of sampling true values from the predicted GMM
for each example. A probability density function is used to
obtain this likelihood value. This conﬁguration corresponds
to 8.5M parameters. The loss function in our system is cal-
culated by the keras-mdn-layer Martin (2018) Python
package which makes use of Tensorﬂow’s probability distri-
butions package to construct the PDF. The model is trained
using the Adam optimizer (Kingma and Ba, 2014) until the
loss on the validation set failed to improve for 10 consecu-
tive epochs.
(a) The original movement sequence used to prime the model.
(b) Movement generated using the priming sequence with corresponding audio features. Movements are less smooth and expressive than the
original recording, but the generated movement follows the priming example nicely.
(c) Sequence generated when the audio features from the priming example are replaced with features from a song not used in the dataset. The
movements are more unstable and shaky.
(d) Here, audio features are replaced with features calculated from a white noise signal. While the overall sequence is similar to the priming
example, the movements contain more variation between frames, causing jittery movement.
Figure 3: These ﬁgures show the trajectories of hand and toe markers over time (left to right). When generating motion using
audio which was not used in training (3c) or white noise (3d) the movements become more unstable.
Altering Movement Using Audio Features
When the MDRNN is trained on movement without the ad-
dition of audio features it is able to generate movements
which are, under visual inspection, realistic. In this section,
we examine the effect of altering the audio input for a model
trained on both movement and audio data.
To examine to what extent the model has learned a corre-
lation between the audio features and the movement we gen-
erate motion using a priming technique. When using prim-
ing the input to the model is taken from one of the examples
which were withheld during training. At each time step, the
input consists of the 3D positions of the 22 points and the
corresponding set of audio features for that time step. The
model then predicts the positions of the 22 points at the next
time step. Thereby, the model always predicts the next pose
using the values from the priming sequence. By altering
the audio features of the priming example the output can be
evaluated to determine the effect which different audio fea-
tures have on the generated output. We examine three such
cases here. First, we investigate a sequence generated using
the audio features associated with the priming example it-
self, that is, features calculated from the musical stimuli the
dancer was improvising to when the priming example was
recorded. Secondly, we replace this audio with an excerpt
from a song which was not part of the training data. Finally,
features calculated from a white noise signal is used to re-
place the original audio features. Figure 3 show keyframes
from the 3 generated movement sequences as well as the
motion sequence used to prime the model.
Discussion
When generating movement using the audio features be-
longing to the priming sequence (Figure 3b), the model gen-
erates movement which largely follows the example (Figure
3a). While the overall movement sequence is similar, some
expressiveness seems to have been lost. The generated mo-
tion could be said to resemble a “dead pan” performance of
the original sequence, as trajectories of arms and legs are
to some extent muted in comparison to the original. This
may be due to the model generalising movement across the
training data.
Figure 3c shows the sequence generated when the original
audio features are replaced with features calculated from a
song not included in the training data. The model produces
a movement sequence which matches the priming sequence
well, indicating that the model is able to predict the next
frame of the motion data even with unseen audio features.
Still, additional noise is visible in this example (when com-
pared to 3b), suggesting that the model has not fully learned
to generate smooth movements when unseen audio features
are used.
In the ﬁnal ﬁgure, 3d, the audio features are replaced with
features calculated from a white noise signal. Here, trajec-
tories are decidedly affected by the audio. As with ﬁgure
3c and 3b the model still predicts reasonable positions for
the 22 markers at every frame, but with a larger variation
between frames, causing the resulting sequence to display
jittery movement. This indicates that the model does rely
on structured audio to generate realistic movements at the
micro (if not macro) scale.
Conclusions and Future Work
These results indicate that the MDRNN model could be used
to explore how music and audio features affect the way the
dancers move, and how this manifests itself in the move-
ments generated by a deep neural network.
Much work
remains to obtain a comprehensive understanding of how
MDRNNs can model cross-modal interactions like those be-
tween sound and motion. An important aspect is how we can
best evaluate the performance of this model. Finding good
qualitative and quantitative ways to evaluate creative data
generated by models such as this one will be a central ques-
tion in our future work. Going forward, we will focus on
systematically exploring metrics to evaluate the generated
movements, train the model on a larger dataset and experi-
ment with alternate audio representations.
References
Alluri, V., and Toiviainen, P. 2010. Exploring perceptual and
acoustical correlates of polyphonic timbre. Music Percep-
tion: An Interdisciplinary Journal 27(3):223–242.
Bishop, C. M. 1994. Mixture density networks. Technical
report, Aston University, Birmingham, UK.
Burger, B., and Toiviainen, P. 2013. Mocap toolbox-a mat-
lab toolbox for computational analysis of movement data.
In 10th Sound and Music Computing Conference, SMC
2013, Stockholm, Sweden. Logos Verlag Berlin.
Burger, B.; Thompson, M.; Luck, G.; Saarikallio, S.; and
Toiviainen, P. 2013. Inﬂuences of rhythm- and timbre-
related musical features on characteristics of music-
induced movement. Frontiers in Psychology 4:183.
Fukayama, S., and Goto, M. 2015. Music content driven
automated choreography with beat-wise motion connec-
tivity constraints. Proceedings of SMC 177–183.
Graves, A. 2013. Generating sequences with recurrent neu-
ral networks. arXiv preprint arXiv:1308.0850.
Ha, D., and Eck, D. 2017. A neural representation of sketch
drawings. arXiv preprint arXiv:1704.03477.
Hochreiter, S., and Schmidhuber, J. 1997. Long short-term
memory. Neural computation 9:1735–80.
Kingma, D. P., and Ba, J.
2014.
Adam: A method for
stochastic optimization. arXiv preprint arXiv:1412.6980.
Lartillot, O.; Eerola, T.; Toiviainen, P.; and Fornari, J. 2008.
Multi-feature modeling of pulse clarity: Design, valida-
tion and optimization. In ISMIR, 521–526. Citeseer.
Lee, H.-Y.; Yang, X.; Liu, M.-Y.; Wang, T.-C.; Lu, Y.-D.;
Yang, M.-H.; and Kautz, J.
2019.
Dancing to music.
In Advances in Neural Information Processing Systems,
3581–3591.
Martin, C. P., and Torresen, J. 2019. An interactive musical
prediction system with mixture density recurrent neural
networks. In Proc. NIME ’19, 260–265.
Martin, C. 2018. keras-mdn-layer: Python Package.
Michalak, J.; Troje, N. F.; Fischer, J.; Vollmar, P.; Heiden-
reich, T.; and Schulte, D. 2009. Embodiment of sadness
and depression—gait patterns associated with dysphoric
mood. Psychosomatic medicine 71(5):580–587.
Pollick, F. E.; Paterson, H. M.; Bruderlin, A.; and Sanford,
A. J. 2001. Perceiving affect from arm movement. Cog-
nition 82(2):B51–B61.
Satchell, L.; Morris, P.; Mills, C.; O’Reilly, L.; Marshman,
P.; and Akehurst, L. 2017. Evidence of big ﬁve and ag-
gressive personalities in gait biomechanics.
Journal of
nonverbal behavior 41(1):35–44.
Seo, J.-H.; Yang, J.-Y.; Kim, J.; and Kwon, D.-S. 2013. Au-
tonomous humanoid robot dance generation system based
on real-time music input. In RO-MAN, 2013 IEEE, 204–
209. IEEE.
Paper III
Exploring the Effect of Sampling
Strategy on Movement Generation
with Generative Neural Networks
B. Wallace, C.P. Martin, J. Torresen, K. Nymoen
Published in Artiﬁcial Intelligence in Music, Sound, Art and Design: 10th
International Conference, EvoMUSART
III
85
Exploring the Eﬀect of Sampling
Strategy on Movement Generation
with Generative Neural Networks
Benedikte Wallace1(B)
, Charles P. Martin2
, Jim Tørresen1
,
and Kristian Nymoen1(B)
1 RITMO Centre for Interdisciplinary Studies in Rhythm,
Time and Motion, Department of Informatics, University of Oslo, Oslo, Norway
{benediwa,krisny}@ifi.uio.no
2 Australian National University, Canberra, Australia
Abstract. When using generative deep neural networks for creative
applications it is common to explore multiple sampling approaches. This
sampling stage is a crucial step, as choosing suitable sampling parame-
ters can make or break the realism and perceived creative merit of the
output. The process of selecting the correct sampling parameters is often
task-speciﬁc and under-reported in many publications, which can make
the reproducibility of the results challenging. We explore some of the
most common sampling techniques in the context of generating human
body movement, speciﬁcally dance movement, and attempt to shine a
light on their advantages and limitations. This work presents a Mixture
Density Recurrent Neural Network (MDRNN) trained on a dataset of
improvised dance motion capture data from which it is possible to gen-
erate novel movement sequences. We outline several common sampling
strategies for MDRNNs and examine these strategies systematically to
further understand the eﬀects of sampling parameters on motion genera-
tion. This analysis provides evidence that the choice of sampling strategy
signiﬁcantly aﬀects the output of the model and supports the use of this
model in creative applications. Building an understanding of the relation-
ship between sampling parameters and creative machine-learning outputs
could aid when deciding between diﬀerent approaches in generation of
dance motion and other creative applications.
Keywords: Mixture density networks · Movement generation ·
Generative networks · Creative prediction
1
Introduction
Expressive movement is an intrinsic part of human life. Simple movement pat-
terns such as gait or arm movement can eﬃciently convey an emotional state and
This work was partially supported by the Research Council of Norway through its
Centres of Excellence scheme, project number 262762.
c
⃝ Springer Nature Switzerland AG 2021
J. Romero et al. (Eds.): EvoMUSART 2021, LNCS 12693, pp. 344–359, 2021.
https://doi.org/10.1007/978-3-030-72914-1_23
Exploring Sampling Strategy with Generative MDRNN
345
may allow us to detect characteristics such as personality or mood [15,18,19]. As
such, motion analysis and generation have the potential to improve human-robot
interaction, human activity recognition and artiﬁcial agent design [12]. In this
paper, we present a deep recurrent neural network trained on dance movement
and systematically examine a series of common sampling strategies in order to
explore their eﬀect on the movement generation process.
While the objective criteria of success for a predictive model will vary depend-
ing on the application, one would usually expect a model intended for the gen-
eration of creative data to be able to generate novel and varied outputs, while
still ensuring a reliable level of realism. This gives rise to several challenges. For
any predictive system, a loss function would typically include some metric indi-
cating the distance from the model output to some ground truth. This approach
produces good results when generating basic human motion such as gait [1].
However, when generating dance movements, or indeed any creative data, there
is no single correct answer or ground truth to which the output can be compared
as we may consider several predictions equally valid. Thus, rather than attempt-
ing to predict a single truth a good model for generating creative data should be
able to generate a variety of likely outputs. Mixture density networks (MDNs)
[2] have been used for various creative prediction tasks previously, as they are
particularly suited for modelling these kinds of data. The MDN treats the out-
puts of a neural network as the parameters of a Gaussian mixture model (GMM),
which can be sampled to generate real-valued predictions and allow us to explore
several strategies for generating variations on the model’s predictions. For the
experiments outlined here, we will focus on three strategies: priming, isolation
of mixture components and temperature adjustments.
Including a temperature parameter which is used to reweight the learned
probability distribution is a common way to approach the challenge of generating
variations on model output. This is often done by experimenting with sampling
from the probability distribution with more or less “randomness”, often referred
to as stochastic sampling, until the output is found to be satisfactory [6–8,14].
If the model consistently outputs the most likely prediction, the output can
become repetitive. Conversely, a ﬂat probability distribution where all outcomes
are equally likely would be equivalent to sampling at random, invariant to the
learned distribution. Reweighting the distribution allows the model to generate
less likely predictions some of the time. Allowing for this occasional unexpected
predictions by implementing stochastic sampling, will in many cases improve the
realism of the output and elicit more interesting predictions thereby improving
the perceived creative capacity of the model. However, there are many possible
temperature values and ﬁnding the “sweet spot” can be challenging.
As the MDN learns several distributions of the data simultaneously, it is fur-
ther possible to explore each of the learned Gaussian distributions in isolation.
This idea has been explored by Ellefsen et al. [6] in the context of world mod-
els [9], who found evidence to support the theory that each component will model
diﬀerent stochastic events. In this work, we examine the distributions learned
by a model trained with 4 mixture components and discuss the potential of this
346
B. Wallace et al.
approach as a means of simultaneously modelling several possible outcomes for
a given input sequence.
Another common approach to vary the output of a trained model is to prime
the model on diﬀerent unseen examples. Priming consists of running an unseen
sequence through the model before generating a prediction for the next time
step. Graves [7] demonstrated interesting results using this strategy to generate
handwriting examples in various styles by priming a model on examples by
diﬀerent subjects. With our priming experiments, we aim to examine to what
extent the results obtained for handwriting are transferable to full-body motion.
While variation is a central aspect of generating creative data, there has
been little focus on the sampling strategies and how they aﬀect variation in
the generated data. We aim to systematically explore the eﬀect of applying
the three sampling strategies outlined above: adjusting temperature, priming,
and isolating mixture components. The code used to sample from the trained
model has been made available in the interest of reproducibility. The following
sections describe the training parameters and motion capture recordings used to
train the MDRNN. Section 5 presents the results of sampling from the trained
MDRNN using each of the aforementioned strategies. The results are further
discussed in Sect. 6 where we present our recommendations for applying the
diﬀerent strategies in creative applications.
2
Motion Data
Working with expressive human movement presents a series of challenges. In the
section below, we have outlined some considerations relevant to working with
creative data and, more speciﬁcally, dance. In the following section, details of
the data collection process and post-processing of the motion capture data are
presented.
2.1
Quantifying Movement
Movement of a human body may be represented as real-valued multi-dimensional
time-series data unfolding in space and time. High-precision recordings of move-
ment may be recorded with marker-based optical motion capture technology,
where each marker put on the body provides a 3D position vector over time.
Dance is a particularly complex variant of human movement, with subtle fea-
tures which can communicate important information. As such our network must
be able to retain the nuances in our dance recordings as real-valued time-series.
While other creative data sources such as music, text and images can be simpli-
ﬁed into a discrete representation, it is clear that doing so would severely reduce
the expressiveness of human movement.
In examining the output of a model which generates creative data the results
are often evaluated according to whether or not they are typical examples of their
type, be it a piece of music perhaps belonging to a speciﬁc genre [20], or a painting
in a certain style [21], or in our case a realistic sequence of movement. Realism
Exploring Sampling Strategy with Generative MDRNN
347
is naturally a quality that has some degree of importance when generating any
type of data, but perhaps especially so for movement; any deviation from what
is physically possible for a human to achieve is instantly recognisable. Another
important aspect to consider is whether the model is able to produce novel
output, diﬀerent from the examples used in training.
In order to determine the novelty or realism of a generated motion sequence,
it is useful to deﬁne a notion of similarity. However, the seemingly simple concept
of movement similarity becomes quite complicated. In many cases, it may be suf-
ﬁcient to consider motions as similar if they only diﬀer with respect to global
rotation or spatial and temporal scaling [16], but for the purpose of comparing
dance, such a deﬁnition may be unsatisfactory. Consider, for example, one dance
sequence where the left arm is raised, and another the right arm. The semantic
similarity between the two would not be reﬂected in metrics like the euclidean
distance between global positions or between joint angles in consecutive frames.
Further exploration of movement similarity is outside the scope of this article.
Instead of comparing generated motion sequences to the motion capture record-
ings by use of distance metrics, we will describe the generated motion sequences
in reference to the conceptual class of artefacts that this data belongs to, namely
dance, and evaluate them by visual inspection.
Fig. 1. Marker setup worn by the dancers during motion capture data collection
2.2
Dataset
Our dataset contains 54 one minute motion capture recordings of improvised
dance performed by three female dancers. Their average age is 23 and each has
more than 10 years experience in modern, jazz and ballet. Each dancer performs
three one minute improvisations to six diﬀerent musical stimuli which vary in
terms of tempo, tonality and beat clarity. The dataset was recorded using a
Qualisys optical motion capture system with 12 Oqus 300/400 series cameras
348
B. Wallace et al.
which capture 43 reﬂective markers worn by the dancers. Figure 1 shows the
front and back of a dancer wearing the motion capture suit with markers. Using
the MoCap Toolbox 1.5 [3] these 43 markers are translated to 22 joint positions,
see Fig. 2. Small gaps in the data were spline-ﬁlled using Qualisys Track Manager
2019.3 and a 2nd degree Butterworth ﬁlter with a 7.2Hz cutoﬀ was applied to
remove any marker jitter. Recordings in our dataset have been normalized so
that the root marker (a weighted average of markers 41, 42, 6 and 7 in Fig. 2) is
centred at the origin where x, y, and z position is 0. Body segment lengths are
averaged across the three dancers ensuring that the data is invariant to global
position and individual body dimensions. The data was captured 240 Hz and
downsampled 30 Hz before model training to reduce the size of each example. The
resulting 54 data tensors consist of 1800 frames (60 s 30 Hz) with 3-dimensional
positions for each of the 22 points.
Two full motion capture recordings are withheld for testing while the remain-
ing 52 examples are split into two sets, 80% are used for training and 10% for
validation. Each example is sliced into overlapping sequences of 256 frames and
the spatial dimensions of each of the 22 points are scaled using min-max normal-
isation. The input to our model thereby consists of 80288 overlapping sequences
of 256 frames. The target value of each training example is the 3D position of
each point in the following frame.
Fig. 2. The 43 reﬂective markers are translated to 22 points
3
Movement Prediction with MDRNNs
Mixture density networks (MDNs) [2] treat the outputs of a neural network as
the parameters of a Gaussian mixture model (GMM), which can be sampled to
generate real-valued predictions. MDRNNs are becoming well-established tools
Exploring Sampling Strategy with Generative MDRNN
349
in the generation of creative data. They have previously been applied to musical
sketches in two dimensions as part of a smartphone app [14], to sketches [8], and
handwriting [7]. MDRNNs have also previously been applied to motion capture
data [5,17,22] leading to promising results.
Figure 3 shows a simpliﬁed mixture distribution with 4 components. A GMM
can be derived using the mean, weight and standard deviation of each compo-
nent. The number of components needed to accurately represent the data is not
known and is treated as a hyperparameter for our model. For the study outlined
here, we have used 4 components. We can interpret these components as each
representing diﬀerent possible future movement. By combining a recurrent neu-
ral network (RNN) with an MDN to form an MDRNN we can make real-valued
predictions based on a sequence of inputs. Figure 4 shows the model architecture
of the MDRNN used in this work. The RNN consists of three layers of LSTM
cells [10], known to be eﬀective in modelling temporal sequences such as music,
text and speech. The three LSTM layers contain 1024, 512 and 256 hidden units
respectively. The outputs of the third LSTM layer are in turn connected to an
MDN. The LSTM layers learn to estimate the mean, standard deviation and
weight of the 4 Gaussian distributions.
Fig. 3. Simpliﬁed mixture distribution example. The MDRNN presented in this work
learns the mean (μ), standard deviation (σ) and weight (π) of 4 components, each
having 66 dimensions, one for each of the 22 3D points.
To optimize an MDN, we minimize the negative log-likelihood of sampling
true values from the predicted GMM for each example. A probability density
function (PDF) is used to obtain this likelihood value. In our case, the GMM
consists of K = 4 n-variate Gaussian distributions. For simplicity in the PDF,
these distributions are restricted to having a diagonal covariance matrix, and
thus the PDF has the form:
p(θ; x) =
K

k=1
πkN(μk, Σk; x)
(1)
where π are the mixing coeﬃcients, μ, the Gaussian distribution centres, Σ the
covariance matrices and n is the 66 position values (22 points * 3 dimensions)
350
B. Wallace et al.
contained in each frame. This conﬁguration corresponds to 8,540,692 parame-
ters. The loss function in our system is calculated by the keras-mdn-layer [13]
Python package which makes use of Tensorﬂow’s probability distributions pack-
age to construct the PDF. The model is trained using the Adam optimizer [11]
until the loss on the validation set failed to improve for 10 consecutive epochs.
Fig. 4. Sampling from the MDRNN. A sequence of preceding frames is sent through
the model which outputs the parameters of a mixture distribution. By sampling from
this distribution the next frame is generated.
4
Sampling from the Trained Model
In the following sections, we explore how the output of the trained MDRNN
can be aﬀected using diﬀerent sampling strategies. There are, of course, count-
less parameter settings and combinations to examine. Here, we have focused
on outlining the extremes as well as the threshold points wherein the eﬀect of
each strategy changes. Three diﬀerent strategies for examining the creative rep-
resentational capacity of the MDRNN and their eﬀect on the model output are
explored:
Temperature Adjustment: When sampling from our trained model we can
choose to alter the value of two temperature parameters, the π-temperature and
the σ-temperature. The σ-temperature is used to scale the covariance matrix
of each mixture component’s multivariate normal distribution. Adjusting the σ-
temperature aﬀects the width of each mixture component. A high σ-temperature
allows for samples further from the learned mean of each mixture component to
be selected. For our experiments, we explored a range of σ values between 0 and
1 to locate at which temperatures the eﬀect on the output changes.
The π-temperature adjusts the probability of sampling from individual mix-
ture components. High π-temperatures reweight the probability of sampling from
each component in such a way that each component is an equally likely choice,
while suﬃciently low temperatures will ensure that only a single component is
selected. Here, we examine π-temperatures between 0 and 10. By selecting a
π-temperature close to or higher than 1.0 we create a higher likelihood of sam-
pling from a diﬀerent mixture component at every time step. Sampling with
Exploring Sampling Strategy with Generative MDRNN
351
π-temperature close to 0, on the other hand, ensures that only the component
which has the highest learned weight will be sampled from. This is equivalent to
isolating this component and sampling from it at every time step.
Isolating Mixture Components: One of the advantages of the MDRNN archi-
tecture is its inherent ability to model a range of possible outcomes simultane-
ously. The MDN used in this work builds a GMM from 4 components, in the
following experiments we isolate each of them by enforcing that sampling can
happen from only a single component. This allows us to further examine what
kind of movement is learned by each component. While the most realistic move-
ment sequences would most probably be gained from sampling from the compo-
nent with the highest π value, the other components contain variations on the
predicted movement which might also produce interesting results.
Primed Sampling: Being able to generate motion in a certain style is useful
both in the context of animation and as a creative tool for choreography. By
feeding the model an unseen example we can evaluate to what extent the model
is able to accurately predict a continuation of the sequence. Carlson et al. [4]
have recently shown how the characteristics of a participant’s individual style
of movement are suﬃciently unique to allow for classiﬁcation using machine
learning and as such, it is intriguing to investigate whether priming a model
on an example may allow the model to generate movements in the style of a
speciﬁc participant or performance. Previous work using MDRNNs to generate
handwriting [7] show that it is possible to produce examples which display the
characteristics of a particular writer when the model is primed on an example.
In the experiments presented below, two motion sequences are used to prime the
model during sampling. The ﬁrst contains slow and ﬂowing movements while the
second was performed to an uptempo song and contains faster and more move-
ment overall. By comparing the generated movements we examine the perceived
likeness between the primer sequence and the model predictions.
5
Results
The code used to sample from the trained model can be found at our working
repository together with video examples.1
5.1
Adjusting Temperature
Sampling with σ-temperature close to or higher than 1 results in movement
sequences that change rapidly between frames. The movements are “shaky” and
jump in unnatural ways between time steps. We observe that this shaking eﬀect
becomes noticeable with σ > 0.05. Conversely, a σ-temperature closer to 0 allows
for smoother motions. An example of the extremes can be seen in Figures 5a and
1 https://github.com/benediktewallace/Motion-MDRNN.
352
B. Wallace et al.
5b. These ﬁgures show the trajectories of hand and toe markers, with horizontal
displacement indicating time evolution. Figure 5c shows a sequence wherein the
temperature rises over time. σ-temperatures between 10−7 and 10−4 cause small
variations in the motion sequence while remaining realistic. Higher values result
in sequences which contain a fair amount of noise which can also distort the
form of the body, while lower values contain less overall movement.
Figure 6b shows a sequence generated with a π-temperature close to 0. Figure
6c shows how the output changes as the π-temperature is increased over time.
At temperatures around 1.0 the component with the highest weight is still sam-
pled from most frequently but, we also intermittently sample from the other 3
components according to their respective weights. Figure 6a shows the result
of sampling with a high π-temperature. Here, we sample from a diﬀerent com-
ponent at almost every time step. Shifting between mixture components when
generating motion sequences causes abrupt changes in the position of the body,
alluding to the notion that each component may learn a slightly diﬀerent move-
ment sequence. We examine this more closely in the upcoming section.
5.2
Isolating Mixture Components
For these experiments, we disregard the π-temperature and instead manually
select which of the 4 mixture components to sample from. This ensures that
each new frame is sampled from a single component. We observed in the previ-
ous section that the entire position of the body changed as we sampled with a
higher π-temperature, indicating that individual components emphasise diﬀerent
features.
In order to examine this more closely the σ-temperature is kept at a low value
to make certain that we sample close to the mean of each component and each
sequence is given the same starting position. Figure 7 shows the 100th frame
from sequences sampled from each of the 4 components using the same starting
frames and temperature parameters. These ﬁgures show how each component
has predicted a diﬀerent outcome, with component 1 being the most dissimilar
to the other components. Sequences sampled from component 1 contain almost
no movement at all, while component 2 and 3 produces examples which show the
highest amount of realism. The third component consistently produces the most
realistic output and is the component with the highest learned π-value. Thereby
making it the component which is most likely sampled from when no adjustments
are made using the π-temperature parameter. Sampling from component 1 and
4 shows little overall movement and a fair amount of distortion in the body, as
joints are placed at unnatural angles. From these experiments, it seems that each
component learns a slightly diﬀerent movement pattern. Thereby, selecting to
sample from a single component when generating a movement sequence ensures
that certain learned patterns appear in the predicted movement.
Exploring Sampling Strategy with Generative MDRNN
353
(a) Movement generated with σ=1.0. Movements are noisy, positions of points jerk
between frames.
(b) Movement generated with σ = 10−9 Movements are smooth and realistic.
(c) Movement generated with σ-temperature rising over time.
Fig. 5. Sequences generated using diﬀerent σ-temperatures.
5.3
Primed Sampling
When generating motion with priming, a movement sequence which has not been
used in training is given as input to the model. The next frame is then gener-
ated and the process is repeated. The model always predicts the next frame for
a previously unseen real sequence, as opposed to non-primed sampling, wherein
354
B. Wallace et al.
(a) Movement generated with π=10. Positions of the body jerk between frames as the
we alternate between mixture components.
(b) Movement generated with π = 10−9 Movements are smooth and realistic.
(c) Movement generated with π-temperature rising over time.
Fig. 6. Sequences sampled with diﬀerent π-temperatures.
the models’ previous predictions become part of the sequence used to generate
the following frame. We explore the eﬀect of priming on two performances by
diﬀerent individuals using the examples that were withheld during training. The
ﬁrst example hereafter referred to as primer A, was performed to rhythmical
musical stimuli with a strong beat presence. The second example, primer B,
was performed to slow, non-rhythmic musical stimuli. As such, the two prim-
Exploring Sampling Strategy with Generative MDRNN
355
(a) 1st
(b) 2nd
(c) 3rd
(d) 4th
Fig. 7. 100th frame from sequences generated by 4 diﬀerent mixture components. Keep-
ing all other parameters the same, each component generates a diﬀerent movement
sequence.
ing examples have diﬀering characteristics. Primer A contains more movement
overall and a higher speed of movement, while primer B consists of slower move-
ments. For these experiments, π and σ-temperatures are both set to values close
to zero ensuring that we sample from close to the mean of the component with
the highest weight, component 3.
Figure 8a shows a series of frames from primer A. Figure 8b shows the corre-
sponding movement sequence generated using primer A. As can be seen in the
ﬁgure, the generated movement to a large degree follows the movement of the
priming sequence. Movement aspects such as the speed of motion and the overall
amount of movement are similar to the priming sequence as well. Similar results
were also found for primer B.
6
Discussion
We have implemented and systematically explored common sampling strategies
which have previously been used in various generative tasks. However, the impor-
tance of realism is perhaps more important in the generation of movement than
in generating other creative data. Even slight deviations from what is feasible
for the human form breaks the perceived realism of the generated movement.
When sampling from only the component with the highest weight, either
by suﬃciently lowering the π-temperature or by isolating the component with
the highest learned π value, we ensure that the model outputs smooth and
realistic motion sequences. While stochastic sampling of mixture components is
possible, our results indicate that this approach, at least for movement, does not
necessarily improve the output as the movements modelled by each component
are suﬃciently dissimilar. Thereby, intermittent changes in the choice of mixture
component cause abrupt changes and more jagged movement.
Sampling from a single mixture component for a given sequence and altering
the amount of deviation we allow away from the mean using the σ-temperature
instead can give varied and realistic results. While larger adjustments to σ-
temperature result in noisy and irregular predictions, slight adjustments allow
356
B. Wallace et al.
(a) Frames from priming sequence A.
(b) Motion sequence generated when priming on priming sequence A.
Fig. 8. Priming sequence and corresponding output. The sequence generated by the
model shows similar motion unfolding over time.
for some interesting variations to occur. This could have useful applications as
a user-controlled parameter, both in a creative tool and for animation.
When examining the output generated by each mixture component in isola-
tion it is apparent that the components which achieve the highest learned weight
also produce the most realistic movement sequences, as would be expected. By
examining each component individually, however, we found that two of the four
components were able to produce interesting sequences which also achieved rel-
atively high realism. It is possible that training on a larger dataset would cause
the other components, which for our experiments show distorted body poses with
little movement, to learn more realistic and useful features. This would then be
an interesting parameter in a creative tool in order to create variations on a
theme. Examining the output from each component also shows us the capabil-
ities of this particular model to produce several possible futures with diﬀerent
degrees of variation and realism.
From our experiments with priming, we found that the output of the model
was able to continue the motion of unseen examples with some variation. This
indicates that the style of movement is indeed continued when sampling with
Exploring Sampling Strategy with Generative MDRNN
357
priming. Priming the model on unseen data could be useful when using the
model as a creative tool to assist choreographers and animators, as it allows for
priming the model with their own data and achieve predictions in a desirable
style closer to their own. The goal of this research is to critically examine sam-
pling of a motion generation MDRNN with a view towards the production of
novel and realistic data for creative applications. From our ﬁndings, we suggest
the following recommendations for applying the diﬀerent types of sampling we
have explored in creative motion generation:
Temperature Adjustment. Adjusting π and σ during sampling allow varia-
tions to be produced within a narrow band of useful values. These need to
be determined by trial and error.
Isolating Mixture Components. Generating motion from individual mixture
components is a particularly useful tool. As we found, some mixture compo-
nents with lower learned weights can produce useful outputs. Isolating these
components can ensure that they are not overwhelmed by more likely parts
of the mixture model.
Primed Sampling. Using new movement sequences to prime the model can
ensure that the generated sequence follows the over-all movement shape
and style of the priming example. By further combining priming and σ-
temperature adjustments, one could produce several variations on a theme.
7
Conclusion
While MDRNNs have previously been applied to creative processes including
motion generation, the process of sampling, or generating creative outputs, has so
far not been critically examined. In this research, we have presented results from
exploring three common sampling techniques (temperature adjustment, isolating
mixture components, and primed sampling) that can be used when generating
output from MDRNNs. We have analysed how these techniques aﬀect generated
dance movement sequences.
Our ﬁndings show that the priming technique allows the output to be shaped
by an unseen example, indicating that the model could be used to generate move-
ment sequences in a certain style. Further, the results show that changing the
learned probability distribution by including temperature parameters has the
potential to greatly aﬀect the output. However, the range of viable parameter
values is small, as any unnatural movement is easily detected by an observer.
As such, temperature adjustment may be less suited for human motion than for
other creative data. Alternating between mixture components when generating
a motion sequence results in unnatural shifts in body positions between consec-
utive frames. Still, the components with a lower learned weight may also contain
interesting movement sequences. Therefore, we believe this approach warrants
further exploration and analysis.
Evaluations of the results of the diﬀerent sampling strategies have been quali-
tative. Arguably, one might prefer a more quantitative approach to the evaluation
358
B. Wallace et al.
of machine learning results, however, this is an inherently challenging aspect of
creative computing in general, and particularly so for data representing human
behaviour. In future work, we will conduct a perceptual study in order to quan-
tify the qualitative evaluations conducted in this paper.
We feel that the ﬁndings of the present research, in conﬁrming and exploring
the capacity of generative MDRNN models of human motion to create a variety
of outputs will assist with the production of more novel and more realistic motion
sequences for use in artistic and scientiﬁc work.
References
1. Alemi, O., Pasquier, P.: WalkNet: a neural-network-based interactive walking con-
troller. IVA 2017. LNCS (LNAI), vol. 10498, pp. 15–24. Springer, Cham (2017).
https://doi.org/10.1007/978-3-319-67401-8 2
2. Bishop, C.M.: Mixture density networks. Technical report, Aston University, Birm-
ingham, UK (1994). http://publications.aston.ac.uk/373/
3. Burger, B., Toiviainen, P.: MoCap toolbox-a Matlab toolbox for computational
analysis of movement data. In: 10th Sound and Music Computing Conference,
SMC 2013, Stockholm, Sweden. Logos Verlag Berlin (2013)
4. Carlson, E., Saari, P., Burger, B., Toiviainen, P.: Dance to your own drum: identi-
ﬁcation of musical genre and individual dancer from motion capture using machine
learning. J. New Music Res. 49, 1–16 (2020)
5. Crnkovic-Friis, L., Crnkovic-Friis, L.: Generative choreography using deep learn-
ing. In: Proceedings of the Seventh International Conference on Computational
Creativity (2016)
6. Ellefsen, K.O., Martin, C.P., Torresen, J.: How do mixture density RNNs predict
the future? arXiv preprint arXiv:1901.07859 (2019)
7. Graves, A.: Generating sequences with recurrent neural networks. arXiv preprint
arXiv:1308.0850 (2013)
8. Ha, D., Eck, D.: A neural representation of sketch drawings. arXiv preprint
arXiv:1704.03477 (2017)
9. Ha, D., Schmidhuber, J.: Recurrent world models facilitate policy evolution. In:
Advances in Neural Information Processing Systems. pp. 2450–2462 (2018)
10. Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural Comput. 9,
1735–80 (1997). https://doi.org/10.1162/neco.1997.9.8.1735
11. Kingma, D.P., Ba, J.: Adam: a method for stochastic optimization. arXiv preprint
arXiv:1412.6980 (2014)
12. Lee, J., Marsella, S.: Nonverbal behavior generator for embodied conversational
agents. In: Gratch, J., Young, M., Aylett, R., Ballin, D., Olivier, P. (eds.) IVA
2006. LNCS (LNAI), vol. 4133, pp. 243–255. Springer, Heidelberg (2006). https://
doi.org/10.1007/11821830 20
13. Martin, C.P.: Keras-MDN-layer: Python Package, November 2018. https://doi.org/
10.5281/zenodo.1482348
14. Martin, C.P., Torresen, J.: RoboJam: a musical mixture density network for collab-
orative touchscreen interaction. In: Liapis, A., Romero Cardalda, J.J., Ek´art, A.
(eds.) EvoMUSART 2018. LNCS, vol. 10783, pp. 161–176. Springer, Cham (2018).
https://doi.org/10.1007/978-3-319-77583-8 11
Exploring Sampling Strategy with Generative MDRNN
359
15. Michalak, J., Troje, N.F., Fischer, J., Vollmar, P., Heidenreich, T., Schulte, D.:
Embodiment of sadness and depression-gait patterns associated with dysphoric
mood. Psychosom. Med. 71(5), 580–587 (2009)
16. M¨uller, M.: Information Retrieval for Music and Motion, vol. 2. Springer, Heidel-
berg (2007). https://doi.org/10.1007/978-3-540-74048-3
17. Pettee, M., Shimmin, C., Duhaime, D., Vidrin, I.: Beyond imitation: generative and
variational choreography via machine learning. In: 10th International Conference
on Computational Creativity (2019)
18. Pollick, F.E., Paterson, H.M., Bruderlin, A., Sanford, A.J.: Perceiving aﬀect from
arm movement. Cognition 82(2), B51–B61 (2001)
19. Satchell, L., Morris, P., Mills, C., O’Reilly, L., Marshman, P., Akehurst, L.: Evi-
dence of big ﬁve and aggressive personalities in gait biomechanics. J. Nonverbal
Behav. 41(1), 35–44 (2017)
20. Sturm, B., Santos, J.F., Korshunova, I.: Folk music style modelling by recurrent
neural networks with long short term memory units. In: 16th International Society
for Music Information Retrieval Conference (2015)
21. Tan, W.R., Chan, C.S., Aguirre, H.E., Tanaka, K.: ArtGAN: artwork synthesis
with conditional categorical GANs. In: 2017 IEEE International Conference on
Image Processing (ICIP), pp. 3760–3764. IEEE (2017)
22. Wallace, B., Martin, C.P., Torresen, J., Nymoen, K.: Towards movement genera-
tion with audio features. In: Proceedings of the 11th International Conference on
Computational Creativity (2020)
Paper IV
Learning Embodied Sound-Motion
Mappings: Evaluating
AI-Generated Dance Improvisation
B. Wallace, K. Nymoen, J. Torresen, C.P. Martin
Published in The 14th ACM Conference on Creativity and Cognition
IV
103
Paper V
Embodying an Interactive AI for
Dance Through Movement Ideation
B. Wallace, C.Hilton, K. Nymoen, J. Torresen, C.P. Martin, R.
Fiebrink
Accepted at the 15th ACM Conference on Creativity and Cognition.
V
113
Embodying an Interactive AI for Dance Through Movement
Ideation
Benedikte Wallace
University of Oslo
Oslo, Norway
benediwa@ifi.uio.no
Clarice Hilton
Goldsmiths University of London
UK
Kristian Nymoen
University of Oslo
Oslo, Norway
Jim Torresen
University of Oslo
Oslo, Norway
Charles Patrick Martin
Australian National University
Canberra, Australia
Rebecca Fiebrink
Creative Computing Institute,
University of the Arts London
UK
ABSTRACT
What expectations exist in the minds of dancers when interacting
with a generative machine learning model? During two workshop
events, experienced dancers explore these expectations through im-
provisation and role-play, embodying an imagined AI-dancer. The
dancers explored how intuited flow, shared images, and the concept
of a human replica might work in their imagined AI-human inter-
action. Our findings challenge existing assumptions about what
is desired from generative models of dance, such as expectations
of realism, and how such systems should be evaluated. We further
advocate that such models should celebrate non-human artefacts,
focus on the potential for serendipitous moments of discovery, and
that dance practitioners should be included in their development.
Our concrete suggestions show how our findings can be adapted
into the development of improved generative and interactive ma-
chine learning models for dancers’ creative practice.
CCS CONCEPTS
• Applied computing → Performing arts; • Human-centered
computing;
KEYWORDS
generative AI, embodiment, reflexive thematic analysis, dance
ACM Reference Format:
Benedikte Wallace, Clarice Hilton, Kristian Nymoen, Jim Torresen, Charles
Patrick Martin, and Rebecca Fiebrink. 2023. Embodying an Interactive AI for
Dance Through Movement Ideation. In Creativity and Cognition (C&C ’23),
June 19–21, 2023, Virtual Event, USA. ACM, New York, NY, USA, 11 pages.
https://doi.org/10.1145/3591196.3593336
1
INTRODUCTION
Generative artificial intelligence (AI) has over the last five years seen
substantial improvements in creative fields such as visual arts [26]
and music [14]. Today, generative AI models like DALL-E 2 [47] and
This work is licensed under a Creative Commons Attribution International
4.0 License.
C&C ’23, June 19–21, 2023, Virtual Event, USA
© 2023 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0180-1/23/06.
https://doi.org/10.1145/3591196.3593336
GPT-3 [9] have made it possible to generate detailed visual artworks
or write a lengthy text on a topic of one’s choice without requiring
prior skill or knowledge with AI, visual art or creative writing.
While the potential of these models can be viewed as a means
of democratizing artistic fields, there also exist concerns among
the creative practitioners in these fields regarding their future job
security [30]. Dance communities have so far not been affected
in the same way, but the availability of large datasets of dance
movements [35] and recent improvements in generative AI indicate
that the generation of highly realistic and versatile dance movement
without the aid of animators or dancers is not far behind. However,
exactly how dancers themselves might want to use generative AI is
not yet so clear. In this work, we employ a practice-based approach
with elements drawn from speculative design to examine what a
group of dance practitioners and choreographers might expect and
want from working with a generative model of dance.
Including creators in the development and evaluation of genera-
tive models is an important aspect of creating useful and interesting
creative AI [56]. But as of today, there are still various computa-
tional bottlenecks that make real-time interaction with a generative
movement model impractical. These include post-processing of
motion capture data, the inference of the generative model as well
as visualisation of the output. State-of-the-art movement models
are still developing when it comes to generating output that can
achieve the realism and expressivity of human dance [35].
To examine the potential future of generative AI as a tool for
dancers in the process of dance creation, we therefore chose to
forego the use of any AI model in our study, instead utilizing role-
play and generalising the concept of a potential generative AI with
the term AI-dancer. In our workshops, the participants themselves
take on the role of an AI-dancer and are paired with a human coun-
terpart for whom they are asked to act as a creative catalyst in
an improvisation task. The workings of such a dance model are
intentionally left mostly open to interpretation by the participants
beyond the idea that the model can observe the movements of a
dancer and respond with movements of its own. The participants’
embodiment of this imagined AI-dancer allows us to bring about
discussions with participants around what the aim of a movement
generation model for dance should be without limiting their reflec-
tions to a specific technology or approach. When it comes to bodily
practices, insights are partly embodied and can be challenging to
put into words [18]. Having participants embody the AI themselves
by taking on the role of an imagined AI-dancer in collaboration
C&C ’23, June 19–21, 2023, Virtual Event, USA
Benedikte Wallace, Clarice Hilton, Kristian Nymoen, Jim Torresen, Charles Patrick Martin, and Rebecca Fiebrink
with a human partner can illuminate which aspects of AI in dance
should be prioritized in the future development of interactive and
generative tools.
This paper presents the findings from two workshop events de-
signed to explore what expectations, hopes and fears exist in the
minds of the participants regarding the use of generative AI in the
process of dance creation. While building on an extensive back-
ground in the study of technology in dance, this work presents
to our knowledge the first use of an embodied variant of specula-
tive design in exploring emerging generative AI methods in dance
practice. This work provides insight into an embodied view of gen-
erative AI for dance that exposes the importance of the concepts
of intuition, shared images and surprise. In the following sections,
we present our workshop design including bodystorming sessions
and discussions with participants. Through our reflexive thematic
analysis, we shed light on elusive moments of ideation, identifying
challenges that might arise when human dancers translate their
practices with other people into interactions with AI. We then
present suggestions as to how these idea-inspiring interactions
between participants during improvisation might be approximated
or augmented using AI, describing how some of the dancers’ expe-
riences and goals might be translated into new types of technical
formulations for dance AI systems that go beyond simple mimicry
of human motion patterns.
2
RELATED WORK
Human-computer interaction in dance goes back several decades,
with pioneers such as choreographer Merce Cunningham taking
part in the development of software for visualising dance in the
1980s [52]. With the proliferation of machine learning in more
recent years, we have seen growth in the field of computational
support for dance [62]. Research into AI systems in the context of
movement practices shows great promise for the use of AI in inter-
active installations using gesture recognition [28, 32, 36], analysis
and classification of movement qualities [17] or mapping movement
to the control of various media such as sound and video [16, 45].
In this work, we will focus on generative AI. Generative AI is not
a single kind of neural network architecture or method. Rather it
refers to a way of using AI to create, wherein the AI itself produces
an output which is similar, but not identical to its training data or
learning environment. Specifically, we will focus on data-driven
generative AI. In this way we aim to differentiate between program-
matic generation of dance [43] and models which learn to generate
dance from dance datasets, with little or no interference from a
human being. It is this data-driven approach to generative AI which
has seen immense advancements in image and text generation over
the last 5 years and recently also within dance generation, from
the work of Crnkovic et al. [13] in 2016 to the recent publication
by Li et al. [35] presenting a generative model for dance which can
produce realistic dance movement in a variety of styles.
One of the benefits of data-driven deep learning models is their
ability to learn complex correlations in the data without the devel-
oper of the system having to decide on explicit mappings between
the model’s input and what it generates. Instead, the model learns
these mappings, or an approximation of them, from the data. While
these data-driven dance generation systems are currently some-
what slow (compared to the near instantaneous responsiveness of a
human dancer) and require large datasets, we are likely on the cusp
of seeing data-driven generative models in real-time interactive
applications.
Many generative movement models are imagined to be used
to automatically generate choreography for virtual avatars such
as Hatsune Miku [41] or to produce expressive emotes in games
[34, 35]. Another commonly cited future goal for AI-generated
dance is for use in dance creation and performance [5]. However,
the ways that different dance communities might themselves wish
to use such methods, and the criteria against which they would
evaluate an AI-dancer’s quality and usefulness, are not yet clear.
The most widely adopted methods for evaluation of generative
movement models consist mainly of quantitative metrics such as
preference ratings [60], beat synchronicity and comparisons to
other models [35]. While these methods shed light on the models’
abilities to generate realistic dance movements, a closer engagement
with dance practitioners is needed to better understand generative
AI’s potential use in a dancer’s creative practice. Previous work
on technology’s effect on kinaesthetic creativity [27] (the body’s
ability to enact alternate future possibilities through movement)
provides insights into the importance of awareness around how
different aspects of a technological paradigm can affect the users’
creative flow [11]. In Sturm and Ben-Tal’s work [56] on bringing
generative music models back to practitioners, they argue for a
multifaceted approach to evaluating a generative model for use in
co-creation. Having creative practitioners engage with emerging
technologies through experimentation and discussions [19] can
lead to unexpected insights into the opportunities and challenges
presented by AI systems that quantitative measures can not capture.
As there exist a multitude of methods, datasets, and models—as
well as few if any suitable real-time generative models to work
with—we propose that a practice-based role-playing scenario using
methods from speculative design might allow us to probe dancers’
experience without the limitations of the current state-of-the-art
models. In speculative design, fiction and potential futures are used
to explore emerging technologies [2] and possible future research
areas [37]. By engaging participants of our study in embodiment ex-
ercises and discussion we employ elements from speculative design
to develop a critical discourse about the future use of generative AI
in dance.
3
METHODS AND MATERIALS
3.1
Participants
We held two workshops, the first in London, England and the sec-
ond in Oslo, Norway. Six dancers (4 female, 2 male; average age
29.83) took part in the workshops, three in each event. Participants
were recruited through several online channels such as mailing
lists and our personal network. They were not compensated for
participation but were supplied with food and beverages during the
event. Prior to the workshop participants filled in a form describing
their age, gender, years of dance experience and preferred dance
style. Participants all have extensive experience in dance, both as
performers and dance composers (average years of experience 17;
Embodying an Interactive AI for Dance Through Movement Ideation
C&C ’23, June 19–21, 2023, Virtual Event, USA
standard deviation 11.37). All participants noted an interest in con-
temporary dance, but several also reported various other styles
such as Turkish folk dance, jazz, and dance theatre. Participants
were also asked to what degree improvisation plays a role in their
creative practice, to which all responded it was an important part.
The participants’ responses are detailed in Table 1 along with the
participant ID number used throughout this paper.
3.2
Workshop structure
During each workshop, participants took part in an introduction
to the workshop and generative AI models for dance, and two
bodystorming sessions with discussion sessions in between. Each
is detailed further in the following sections. The workshops took
place over approximately 4 hours each with 5-10 minute breaks
between bodystorming sessions and a 30 minute break for lunch.
3.2.1
Introduction session. The workshops began with a 30-minute
presentation describing the first author’s previous work on move-
ment generation models. Participants were shown how the gen-
erative movement models were trained and what their generated
output may look like (see Figure 1). This allowed for a shared base-
line of understanding of the current state of generative models in
dance before participants began the bodystorming activities. Specif-
ically, participants were shown examples of output generated by a
mixture density recurrent neural network (MDRNN) trained using a
supervised learning approach on an open access dataset containing
164 one-minute motion capture recordings of improvised dance
[61]. The MDRNN is a sequence prediction model which combines
a recurrent neural network consisting of LSTM cells [25] and a
Mixture Density Network (MDN) [4]. This modelling approach
allows for a large variety in the generated data and has previously
been applied to musical sketches in two dimensions as part of a
smartphone app [40], to sketches [24], handwriting [23], as well as
motion capture data [59].
The introduction session focused on giving participants a general
understanding of how an AI-dancer can be constructed by learn-
ing to estimate future movement based on a dataset of examples
and observations of the current input. It was not a requirement
for participants to understand the inner workings of an MDRNN
or any other data-driven approach to movement generation. As
part of this general overview, we also discussed what the AI-dancer
actually “sees” when learning to generate a movement sequence.
As dance involves multifactorial sensing of the environment, the
space, other bodies or objects, sounds, and one’s biological affor-
dances and limitations, a complex interweaving of past and present
impressions affect the movements a dance artist conveys during
improvisation and choreographic exploration. The current state-
of-the-art movement generation models naturally do not have this
complex sensory capability or inner life to draw inspiration from.
Instead, most movement generation models receive a series of joint
positions or similar positional information created using motion
capture data or extracted from video as their input. Such a model
can not “see” facial expressions, skin or clothes. All it has access
to is the movements of a skeletal representation of the dancer. By
discussing the view of an AI-dancer’s “perspective”, we invited the
dancers to connect with how the sensory, somatic aspects of their
experience impact their improvisation.
3.2.2
Bodystorming session. In the bodystorming session, dancers
were invited to explore the process of improvisation in the context
of dance creation through embodiment exercises. The participants
were asked to take turns taking on the role of the AI, receiving input
in the form of movement from their partner and responding in the
way they might expect or hope a generative AI would respond
to inspire and further innovation. The participant currently role-
playing as the AI was given a coloured headband to represent their
role as the AI-dancer, and the other took on the role of the human
input to the AI. As there were three participants in each workshop,
one would observe while the other two were moving. When the
current improvisation ended the observer would take the place of
one of the pair. This way each person experienced both roles, AI-
dancer and human input, in each of the two bodystorming sessions.
Improvisation plays a large role in the creation process for many
contemporary choreographers [49] and is a familiar practice for
the participants. As opposed to other improvisation settings, the
dancers here have two clear roles. The role of AI implies that the
dancer should produce movements to provide inspiration and con-
tribute to the ongoing improvisation led by their partner. We ex-
plored this dynamic of embodying AI using two exercises, both
intended to mimic common ways we imagine AI being used by
dancers and choreographers in the ideation stage of dance compo-
sition. We also asked participants to imagine the AI-dancer as a
visualisation projected onto a wall or on to a movable screen instead
of having a physical body. While using a free exploration approach
to the bodystorming exercises could reveal interesting views on
the form and use of an AI-dancer, this more structured approach
allows us to root the exercises in the not-so-distant future.
The first exercise was a motion continuation exercise. Here, each
participant acting as the AI would observe a short movement se-
quence produced by their partner and would attempt to complete
the movement sequence. This exercise imitates one way in which
the movement repertoire of a trained model might be used, by
prompting a trained model using a movement sequence and having
it predict the continuation of the sequence. The second exercise
was a pair improvisation of free movement. In this exercise the
participant embodying the AI should take inspiration from their
partner’s movement and respond in a way they feel would comple-
ment their partner’s performance. This exercise mimics how one
might use a generative AI for improvisation in real time. Each pair
spent between 5 and 10 minutes in improvisation before switching
roles. Each exercise thereby lasted around 25 minutes.
3.2.3
Discussion session. After each movement exercise, partici-
pants were invited to reflect on the bodystorming experience. These
sessions were loosely structured to precipitate reflection on the
movement exercises. We aimed to promote discussion around how
an AI-dancer might play a role in their practice as well as explore
what aspects of the interactions participants felt worked well and
which worked less well. The following non-exhaustive list of ques-
tions was used to guide our discussions:
• What do you find most important in how your partner (the
AI-dancer) responds to you?
• If you have previous experience with group/pair improvisa-
tion, what aspects make such co-creation sessions successful
or unsuccessful?
C&C ’23, June 19–21, 2023, Virtual Event, USA
Benedikte Wallace, Clarice Hilton, Kristian Nymoen, Jim Torresen, Charles Patrick Martin, and Rebecca Fiebrink
ID
Age
Experience
Preferred styles
P1W1
29
10
Contemporary dance, dance theatre,
communitarian dance.
P2W1
39
>30
Modern and contemporary.
P3W1
39
>30
Turkish folk dance, tap dance, lindy hop,
contemporary, Laban/Bartenieff Movement practices
P1W2
34
>20
Contemporary, modern and jazz
P2W2
20
6
Contemporary dance
P3W2
18
8
Contemporary jazz
Table 1: All participants responded to a questionnaire at the beginning of the workshops. This table details their self-reported
experience, age and the dance styles they were most interested in.
(a) Training a movement model with supervised learning
(b) Examples of generated movement sequences
Figure 1: During the introduction presentation, participants were shown how a generative AI model was trained and what
movements it learned to produce.
• As the AI-dancer, where was your attention focused on the
other person?
• Do you have any thoughts on how a generative AI could be
useful to you in your practice?
3.3
Data collection
The movement and discussion sessions were recorded using two
video cameras mounted on tripods. The videos from the London
workshop were automatically transcribed using YouTube’s auto-
matic captioning function applied to privately uploaded videos and
manually reviewed by the authors before removal from YouTube.
The discussion sessions in Oslo were held in Norwegian. Videos
from these sessions were transcribed and translated by the first au-
thor. We also took notes during both discussion and bodystorming
sessions. The data was stored on encrypted servers according to
local data security practices.
3.4
Data analysis
For analysis, we chose to use a reflexive thematic analysis (R-TA)
approach [7]. R-TA allowed the authors to develop and interpret
patterns in the qualitative dataset while drawing on our own experi-
ences and knowledge. The discussion session transcripts were coded
by the first author using an open-coding scheme. Then, the analysis
was performed through the following steps: first, the transcripts
were read through several times to get a sense of their content.
This was followed up by iteratively assigning codes to each tran-
script. As new codes arose, the previously coded data was reviewed
again until no new codes were added. This resulted in 34 initial
codes. A systematic grouping of these codes into categories and
sub-categories was performed, resulting in 6 code groups: Improvi-
sation strategies, introspection, challenges for an AI-dancer, what
is wanted from an AI-dancer, affecting an AI-dancer and curiosity
about AI. A final revision of all transcripts ensured that the latest
codes were sufficient to cover the data in all transcripts. The code
groups, their descriptions and example quotes were then reviewed
and discussed by the authors. Analysis was primarily conducted by
the lead author, a PhD researcher working in AI generated dance
movements but not a dance practitioner.
Through engaging with the data from coding, three themes were
conceptualized. These are presented in the following section. The
video recordings of the bodystorming sessions were also revisited
during the analysis of the transcripts to examine specific moments
referred to by participants but were not further analysed. Partic-
ipants’ identities were encoded by number and workshop ID to
Embodying an Interactive AI for Dance Through Movement Ideation
C&C ’23, June 19–21, 2023, Virtual Event, USA
Figure 2: A group discussion was held between each bodystorming session.
reflect their participation in the London (1) or Oslo (2) workshop.
P1W1 thereby refers to participant 1 in the London workshop.
4
REFLEXIVE THEMES
We define three themes through the process of reflexive thematic
analysis. The first theme, beyond replica, relates to the concepts
of embodied experience and the potential associated with an AI-
dancer which produces artifacts that are beyond human ability. Our
second theme is intuiting flow, encapsulating the dancers’ descrip-
tions of elusive moments of non-verbal communication that allow
them to predict their partners’ intentions and the challenges this
may introduce for an AI-dancer. Lastly we introduce the theme
building and breaking shared images to describe how participants’
experiences and culture allow them to build rapport with one an-
other as well as surprise each other by deviating from their partner’s
expectations. This theme in many ways lies at the cross-section of
the two prior themes and reflects their interaction. Figure 4 shows
how we link the most prominent discussion topics distilled from
the coding process to the themes presented below.
4.1
Beyond replica
“Does the AI get tired? Tiredness changed the way of
movement, the rhythm and the expression. And that
gives the next step for the choreography.” -P1W1
This theme aims to encapsulate the concepts relating to the body
and the limitations and affordances participants envision when
interacting with a non-human form. Many of the concerns the
dancers expressed are closely related to an AI-dancer’s lack of
embodied experience. One participant points out how their bodily
experience becomes part of the improvisation in meaningful ways.
Participants question how exhaustion, restrictions in physical space
and the occurrence of mistakes could be fully exploited by the AI
the way it is by humans: “[...] to make mistakes [is] important in
the creative process. And I feel like AI usually tries to find the correct
answer [...]”-P2W1. An AI-dancer, trained to generate stylistically
perfect and increasingly realistic movement sequences, might be a
poor reflection of the way human dancers learn, create and explore
movement. Perfection and precision are not necessarily the source
of creative inspiration, nor the aim. As one participant explains
it: “I always believe that we need to make mistakes. And [...] when I
saw this [referring to the training of the AI] you improve it and then
the AI is going to be better. I feel like, yeah that’s the idea of human
progress, but on the other hand it’s like why do we need to think like
this?”-P2W1
Exactly how the participants should imagine the AI-dancers’
presence in the physical world was discussed in the workshop
introduction sessions. Participants were asked to imagine the AI-
dancer as a visualisation on a large screen. This impacted their
improvisation in various ways. Participants did not touch each other,
and spent the majority of time facing each other as though separated
by a screen. The lack of a physical body, with its innate limitations,
is also seen by the participants as a possible affordance of an AI-
dancer. As one participant explains: “Our body is our freedom but
at the same time it’s limited, it’s our limitation.”-P3W1 While the
embodied experience of the dancers shapes their interactions and
experience there are some interesting possibilities afforded by the
AI-dancer not conforming to the bio-mechanical limits or shape
of a human body. Some of the mistakes the AI-dancer makes may
break with what is possible for a human, such as limbs growing
or shrinking and distorting into strange angles and shapes. As
the participants point out, this does not need to be a hindrance,
instead, it can invite the dancer to attempt to translate these abstract
shapes or impossible movements into their bodies: “Even if it is
bio-mechanically not possible to produce, we as an artist get a lot of
inspiration just by seeing that”-P1W1. These observations prompted
further reflections in the authors’ discussions regarding the utility
of AI as going beyond replica.
An AI-dancer does not need to always look human, and several of
the participants mention wanting the AI to “go beyond that”-P3W1.
While co-creating with someone who has a similar movement reper-
toire as oneself may be “frictionless”-P1W2 and feel “safe”-P3W2,
the challenge introduced by collaborating with something alien
C&C ’23, June 19–21, 2023, Virtual Event, USA
Benedikte Wallace, Clarice Hilton, Kristian Nymoen, Jim Torresen, Charles Patrick Martin, and Rebecca Fiebrink
Figure 3: The dancers pair up and move together in an improvisation task where one participant (wearing a colored headband)
takes on the role of an AI-dancer, reacting to the input of their partner.
can be a source of innovation. As participant P3W2 notes when
discussing the AI generating movements that would not be possible
to mimic: “I would take that as a challenge.” The body forms a lens
through which the many possible movements appear. Any poten-
tial movement is filtered through this lens of the physical body,
its limitations and affordances. In the participants’ discussions, we
discover a concern about how they would be able to relate to an
AI-dancer given the AI’s lack of a body. The dancers express a
feeling which seems akin to alienation when talking about how an
AI would not have access to many of the sensory experiences that
form the dancers’ improvisation. Simultaneously, we encounter in
the participants a curiosity for what kinds of movements might
arise from an agent which does not necessarily share our physical
limitations.
4.2
Intuiting flow
“It happens all the time when you move together, you
read the intention before movement comes”. - P3W1
Our second theme emerges from the many moments of intuition,
kinaesthetic awareness and the lightning-fast prediction that are
displayed by the participants during their improvisation. While
the first movement prompt (in which the AI-dancer continues the
movement begun by the human input) resulted in participants
standing facing each other throughout most of the improvisation
(without having been given explicit directions to do so), the dancers
utilised the space in new ways during the second session (in which
AI-dancer and human improvise together), changing their orienta-
tion and at times not facing each other. Still, despite the increase in
activity and loss of eye contact, the participants were able to inter-
pret their partner’s movements and co-create. As one participant
explained: “I didn’t need to see the full movement that she performed
to understand what she would do next.”-P1W1 Other senses such as
sound and kinaesthesia play a role in their communication: “by the
first less-than-second that I saw her, or I felt her energy, I know what
she will do. So I can follow”-P1W1. Participants describe tuning in to
their partner’s movement intentions, a skill which is fundamental
in improvisation and emerges through training and experience.
It was difficult for the dancers to explain exactly their predic-
tions of their partner’s intentions as well as their own internal
decision-making process. As one participant expressed it: “we can-
not consciously understand it, but still at the moment that we intend to
do something, in our energy level, it can be flow or weight, at any time
something is changing. That’s why we can understand”-P3W1. The
lack of conscious decision-making is crucial to upholding the flow
of the improvisation due to the speed at which the interpretations
are happening. Participant P3W1 clarifies: “You have to somehow
learn not to think about [your actions], but to act intuitively. To train
your senses, train your intention, attention and everything. Because
it happens in maybe less than a second”. The goal of this approach
is further explained by another participant as a means to uphold
the flow, or rhythm, of the improvisation: “[...] if we have to name
something as an aim, as an objective, I would say it is to not drop the
rhythm that we are building”-P1W1. The terms “uphold the flow”
and “don’t drop the rhythm” were used by participants to explain
what they were experiencing when trying to keep the improvi-
sation moving. One of the ways the participants did this was by
intuiting their partner’s movements. This intuition often relies on
eye contact, but when their partner is not in view the participants
explain that the sound of their partner played an important part
of determine their energy. The participants also describe “filling
the empty space”-P1W2 left by their partner’s movements. This
may refer to concrete assumptions such as observing their partner
moving a limb upwards indicating that their next steps will involve
moving that limb downwards, but it is also referred to as a more
intangible sense of what is “missing” in the improvisation.
The dancers report that, in the moment, they are not fully con-
scious of how they are making creative decisions. The skills in-
volved in the improvisation process are acquired through years of
training, practising awareness of the space, their partner’s energy,
intent and their own body. Several comments were made clarifying
that while their choice of words seemed to imply something meta-
physical, they did not necessarily mean that this was the case, but
that the experience in some ways defies explanation. Participants
are aware of the challenge of formalising these tacit experiences in
a way that could be useful for developing an AI-dancer. Participant
P1W1 concludes with the following: “[...] so imagine how to teach
that to AI. Because I think I am not even able to explain what we are
[doing]”.
Embodying an Interactive AI for Dance Through Movement Ideation
C&C ’23, June 19–21, 2023, Virtual Event, USA
Figure 4: We distil three themes, intuiting flow (in yellow), beyond replica (in blue) and building and breaking shared images (in
green). This image gives an overview of the most prominent concepts related to each theme. The concept of shared imagery lies
in the cross section, affected by both the notion of prediction, inherent to intuiting flow, and the novelty that emerges from
going beyond replica.
4.3
Building and breaking shared images
“We take each other’s movement languages and sort
of make them our own.” -P2W2
Our final theme describes the complex act of communication through
movement that participants displayed in the bodystorming sessions.
This is perhaps most clear in the participant’s descriptions of shared
images. These images exist in the mind of the dancer, having been
formed by their experiences through, and separate from, dance.
Their interactions consist of an ongoing communication of these
images. In some interactions, these shared images stem from pop-
culture references, shared history or even internal jokes. However,
an important aspect of the participants’ improvisations was how
they chose to interpret their partner’s movement, either symbol-
ically, accepting their partner’s image, or purely kinaesthetically,
without taking into account the historical or social connotations
of the movement. These choices are personal, immediate and sub-
jective. These shared images and individual interpretations are
inseparable from the dancers themselves, as participant P2W1 puts
it: “what we created right now, it’s not separated from who we are or
our background or how we grow up. We have all our own memories
and everything, it’s subjective. It’s so personal and individual”. While
observing both bodystorming exercises, we noted that the dancers
were frequently using strategies of mirroring and shadowing simi-
lar to how these concepts are explained by Blackwell et al. in the
context of musical co-improvisation [6]. The dancers would move
in similar ways, but would rarely mimic their partner’s movements
directly. Instead, their movements were often inspired by their part-
ner, taking on certain characteristics such as the trajectories of the
arms or mirroring their use of space, but altering it in a way to
bring some new aspect into play: “I try to really capture something,
but out of it I try to do something new”-P3W1. For example, this
could be seen when one participant created long lines with their
arm movements which were then transferred to their partner’s
movements but shifted from arm movement to movement of the
feet.
One challenge here is to achieve a balance between the novelty
of a dancer’s movements and likeness to what their partner was
already doing, as pointed out by P1W1: “My aim as an AI was to give
something new, probably not surprising, but it was something new,
an expansion of that input”. This strategy of mirroring and expan-
sion lead to a pull between converging and diverging movement
similarity. In both roles, the dancers found themselves consciously
or unconsciously incorporating elements of their partner’s move-
ments.
C&C ’23, June 19–21, 2023, Virtual Event, USA
Benedikte Wallace, Clarice Hilton, Kristian Nymoen, Jim Torresen, Charles Patrick Martin, and Rebecca Fiebrink
Many interesting moments of humour and play arose through
the process of building and breaking these shared images. For exam-
ple, participants elaborated on their partner’s perhaps unintended
movements in a way that would distil certain aspects and exag-
gerate them. In one such moment, participant P3W1 assumed a
specific position and noted “[...] immediately for me it was like a
two-dimensional image, like Egyptian culture and then my next move-
ment immediately was something related to Egyptian dance. [...] I was
aware that an AI probably will not take that input, but as a human,
I did [...].” Two of the participants further pointed out how they
became aware of all the shared references, or “common images”,
they have with each other. Participant P2W1 explained: “We have
common images that we share with each other from our history and
who we are and this keeps evolving.” She went on to summarize this
as “the AI is reacting, but not maybe looking for a communication,
that’s a human thing.”. While the AI may be able to measure a mul-
titude of movement qualities and features, “[...] still it’s not enough
to create the image that we can perceive”-P3W1, participants raise
doubts regarding whether an AI would ever be able to learn this:
“as humans we have emotions and stories of things that are happening
to us when we want to dance and that isn’t happening to the AI, or
maybe it is, but I think it’s not.”-P3W1
While the above points indicate that the familiarity which devel-
ops through communication between two individuals is an essential
aspect of the improvisation and creation process, many interesting
moments occurred when the expectations of a shared image were
proven wrong. This moment of surprise seems to reset the impro-
visation or shift its direction. The ability of their partner to be able
to take their improvisation in a new direction was appreciated and
used both as a strategy in their improvisations and presented as one
of the elements that made an improvisation enjoyable. The partici-
pants themselves were the ones to decide when to end a particular
improvisation session and when to change roles. This allowed us
to explore what made them decide that they were done. In one ses-
sion, after a few minutes of improvisation, one of the participants
in the role of input proclaimed that they were “empty”-P2W2. In
the following discussion session, we prompted the participant to
reflect on what their AI-embodying partner could have done to
keep the improvisation going. “[...] If [P3W2] brought something
completely new, almost like do the opposite of what I would do, then
maybe more inspiration would come.” The concept of novelty was
mentioned explicitly by both groups as a crucial part of what their
partners can contribute to open “a way out when you are out of
creativity”-P1W2 and avoid getting stuck “in a loop”-P3W1.
5
IMPLICATIONS FOR GENERATIVE AI IN
DANCE
We turn to examine our findings in the context of current methods
in generative AI for movement and identify some concrete areas
of focus for the development and use of generative AI in dance
practice. We then discuss the limitations of this study and suggest
future research directions.
5.1
Predictability and surprise
Novelty is a central aspect of our understanding of creativity, both in
humans and in computer systems [51, 53]. A necessary counterpoint
to novelty and surprise is the notion of predictability. Participants
build a rapport which we describe as a shared image, an understand-
ing of what they are creating together. This shared image helps the
participants predict what their partner might do next and where
the improvisation is going. The complex interaction between the
dancers, their interpretations and their shared experience pinpoints
some interesting potential obstacles for generative AI in dance—in
particular, the challenge of teaching an AI to emulate the dancers’
accumulation of experiences and the individual idiosyncrasies they
foster. Training an AI-dancer on a single dancer’s archive, making
it true to the individual’s movement repertoire—similar to what has
been done within AI-generated drum patterns [58], musical control
interfaces [38] and embodied instruments [39]—could increase the
likelihood that the AI-generated movements are familiar in the
images they might communicate to the dancer [44].
Participants further describe that their interpretations of their
partner’s movements can be either literal or coloured by culture
and association. Allowing users to shift the AI-dancer’s interpre-
tation of a movement sequence from cultural interpretations to
quantitative interpretations during improvisation would be an in-
teresting feature for an interactive AI-dancer. We can imagine an
AI-dancer trained on a rich archive of dance styles and traditions
to be able to recognize the similarity between a given pose and its
semantic, ethnographic roots in addition to recognising physical
similarities between poses. This would however require the con-
struction of a larger and more varied dance dataset than what is
currently available, for example by combining data sets such as
the AIST dance database [57], which contains examples of hip hop,
ballet jazz, waacking and more, with smaller datasets such as folk
dance datasets [1]. Additionally, this would require annotation of
these datasets using methods for measuring both content similarity
[42] and similarity in movement quality [17].
The ability to surprise their partners by changing the shared im-
age and thereby the “direction” of the improvisation is a reoccurring
topic in the dancers’ descriptions of what keeps them interested in
the interaction. This change in direction, or “flow”, may manifest
in various ways, through amount of movement, shifts in dynamics
and energy or use of space. When participants are able to surprise
each other and change the “shared image” of the improvisation,
it injects new energy into the interaction. If an AI-dancer could
extract salient aspects of a dancer’s movement and expand on it in
surprising ways, perhaps these serendipitous moments could also
be brought about in human-AI interaction. Creating an AI-dancer
that avoids too much imitation and repetition seems more likely to
produce an engaging creative input for the dancer.
5.2
Leveraging AI glitches
While the potential for AI to be non-human in its movements and
shape is seen partly as a challenge, it is also seen as a positive aspect.
When imagining a collaboration with an AI-dancer, the participants
express a desire for an AI-dancer which exploits its non-humanness
in favour of an AI which is a mere replica. As one participant put it,
it would be a “waste” to have a non-embodied agent be constrained
by similar physical limitations as humans. This further ties into the
notion of mistakes having value. When the participants experience
fatigue or simply misstep, these unintentional movements shape
Embodying an Interactive AI for Dance Through Movement Ideation
C&C ’23, June 19–21, 2023, Virtual Event, USA
the improvisation, sometimes in ways the participants appreciate.
The mistakes can cause a break in the flow of the improvisation,
allowing it to shift and making room for new directions. The idea
that an AI-dancer would be trained with the aim of achieving per-
fect mimicry is seen by participants as less interesting for their
practice and potentially as a threat to their craft. The training of
any AI necessarily requires some measure of quality or a goal state
to be defined. Our theme beyond replica challenges the view that
a high-fidelity replica of a human dancer should be the goal for a
generative model of dance. Instead, the potential for an AI-dancer to
produce non-human movements sparks the imagination of the par-
ticipants. Approaches such as hierarchical reinforcement learning
[31] attempt to model the intrinsic motivation inherent in curios-
ity. These approaches could lend themselves better to producing
models that prioritise novelty and challenge our expectations.
To transform the impossible movements produced by an AI-
dancer into something the body can do, an expansion of interpreta-
tion is required as the dancer must attempt to translate the abstract
movements through their bodies. In this way, the process would be
similar to how dancers and performance artists might use inanimate
objects in performances or their creative process [20]. Exploiting
an apparent limitation or boundary to promote creativity as with
Oblique Strategies by Brian Eno and Peter Schmidt [15] is a familiar
strategy in many creative fields. When interacting with strange
virtual agents [3], or bodies with unfamiliar morphologies [22]
we can experience a broadening of our own movement qualities.
Leveraging the absurd nature of AI-generated art has also been
explored in other contexts, such as text generation [54]. The task
of co-creation with an unpredictable and strange agent becomes
transmodal, as the dancer must attempt to imbue the artefacts with
meaning and movement. This transmodality can be likened to tasks
such as converting text to images [48], or creating drum patterns
using a Chopin étude [55].
Rather than merely teaching AI to emulate human dancers, more
valuable interactions with generative AI could involve the gen-
eration of unexpected and even unrealistic movement sequences.
Leveraging the model’s lack of physical restraints may give rise
to novel moments of inspiration and prove to be one way that
generative AI can contribute to a dancer’s creative practice.
5.3
Intangible moments
In our discussion, there were many occasions where the participants
struggled to put into words exactly what they were thinking and
why they responded the way they did to their partner, simply
explaining that they were acting on intuition. These intangible
moments of interpretation and response are made possible due to
the participant’s years of improvisation practice. Bresnahan [8]
describes improvisation in dance as embodied and extended agency,
referring to Clark’s embodied and extended mind theory [12]. The
complexities involved in formalising these interactions are a clear
challenge for building an AI-dancer that can foster kinaesthetic
creativity and contribute to dance creation.
The most common approach to evaluating AI-generated dance is
through audience ratings of pre-generated video clips, not through
real-time interactions. While ratings and comparison are arguably
good approaches for comparing model performance [34] or un-
derstanding audience perception of AI-generated movement [60],
our theme of intuiting flow suggest that bringing the dancers and
AI-dancer together in movement is essential in getting access to
these nameless, embodied perspectives that are inherent to the
experience of creation and interaction in dance and can be difficult
to articulate or describe algorithmically. The intuitive responses
between participants point to obstacles for an AI-dancer which go
beyond the technical challenges of real-time feedback and large, var-
ied datasets. It points to the importance of the enactive, embodied
experience inherent to the dancers’ interactions. The implications
of an enactivist philosophy of mind [21] applied to generative AI
may take the form of additional sensor data and the development
of mappings between this data and meaningful dimensions of the
generated movements [29].
Bringing the dancers into the development process through the
choice of a generative model, its input parameters, training data and
visualisation would further allow insight into the expressive nature
of dance. The results of such research can also prove beneficial in
human-robot interaction and the development of virtual agents [10,
33], as the ability to efficiently convey emotions through expressive
movement may aid in establishing trust.
6
LIMITATIONS AND FUTURE WORK
An important aspect to consider in using methods from specula-
tive design is that it is impossible to completely separate our ideas
about the future of technology from our ideas about the present
state of that technology. Discussions with participants are undoubt-
edly coloured by their existing biases towards AI, both positive
and negative. The participants were aware that the workshops
would centre around AI-generated dance and while none of the
participants reported previous experience using dance-generation
systems. However it can be presumed by their choice to participate
in the workshop that they have some interest in the topic of AI
and technology in dance. From this we might assume that the par-
ticipants are largely open to the idea of AI, or at least technology,
having a place in dance practice. As AI-generated dance becomes
more flexible, realistic and available the way text and image gener-
ation has, we may see a shift in sentiment towards integrating AI
into dance practice. The potential dichotomy between technology
and the embodied, ephemeral nature of dance has been at the centre
of several debates [46], and it is easy to imagine that an influx of
generative AI for dance would be met with critique from many
dance communities and scholars. There is in general a lack of longi-
tudinal studies on tools for ideation and enhancing creativity [50].
As such, we hope to extend this work to examine the sentiment
towards AI-generated dance across diverse dance communities as
technology evolves. We further acknowledge that the relatively
small amount of participants in this work causes limited perspec-
tives. Future work involving more participants may reveal different
results.
7
CONCLUSION
This work aims to explore the participant’s expectations, hopes and
fears regarding generative AI in dance. Through examining this,
we hope to clarify how a generative movement model could play
C&C ’23, June 19–21, 2023, Virtual Event, USA
Benedikte Wallace, Clarice Hilton, Kristian Nymoen, Jim Torresen, Charles Patrick Martin, and Rebecca Fiebrink
a part in the creative practice of dancers. Participants graciously
shared their insights allowing us to gain a better understanding of
how dancers would want to interact with generative AI. Through
embodying AI in speculative ideation and discussion, we identify
various challenges that can arise when human dancers translate
practices with other people into interactions with an AI-dancer.
By implementing an embodied variant of speculative design we
gained access to the participants reflections on an emergent branch
of generative AI dance. This approach proved beneficial to study
the experience of engaging with nascent technologies. Our findings
result in the development of three themes, intuiting flow, beyond
replica and building and breaking shared images.
The subsequent implications for generative AI in dance practice
can be summarised as the following three points which we consider
to be of particular importance to the future development of interac-
tive and generative AI models of dance: Leaning into the potential
non-human artefacts created by AI-generated dance, developing
systems that allow for serendipitous moments of discovery and
bringing dance practitioners into the process of developing and
evaluating generative movement models. Leveraging these concepts
would not be possible without the facilitation of interdisciplinary
forums. Bringing together creative practitioners and AI develop-
ers through interaction and reflection sessions can be beneficial
for both parties. In addition to improving current approaches in
generative and interactive AI, it also brings practitioners in “under
the hood” of AI, increasing their understanding of how the models
work. This could empower practitioners to influence the develop-
ment and use of AI in creative domains, bridging the gap between
art and technology and perhaps furthering both.
ACKNOWLEDGMENTS
The authors would like to thank the participants for contributing
their time and insight. We would also like to thank the reviewers
who through their feedback have improved this work. This work
was partially supported by the Research Council of Norway through
its Centres of Excellence scheme, project number 262762.
REFERENCES
[1] Andreas Aristidou, Efstathios Stavrakis, and Yiorgos Chrysanthou. 2012. Dance
Motion Capture Database of the University of Cyprus. University of Cyprus.
http://dancedb.cs.ucy.ac.cy
[2] James Auger. 2013. Speculative design: crafting the speculation. Digital Creativity
24, 1 (2013), 11–35.
[3] Alexander Berman and Valencia James. 2015. Kinetic dialogues: enhancing
creativity in dance. In Proceedings of the 2nd International Workshop on Movement
and Computing. ACM, 80–83.
[4] Christopher M. Bishop. 1994. Mixture density networks. Technical Report. Aston
University, Birmingham, UK. http://publications.aston.ac.uk/373/
[5] Daniel Bisig. 2022. Generative Dance-a Taxonomy and Survey. In Proceedings of
the 8th International Conference on Movement and Computing. ACM, 1–10.
[6] Tim Blackwell, Oliver Bown, and Michael Young. 2012. Live Algorithms: towards
autonomous computer improvisers. In Computers and creativity. Springer, 147–
174.
[7] Virginia Braun and Victoria Clarke. 2019. Reflecting on reflexive thematic analysis.
Qualitative research in sport, exercise and health 11, 4 (2019), 589–597.
[8] Aili Bresnahan. 2014. Improvisational artistry in live dance performance as
embodied and extended agency. Dance Research Journal 46, 1 (2014), 85–94.
[9] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,
Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020. Language models are few-shot learners. Advances in neural
information processing systems 33 (2020), 1877–1901.
[10] Sarah Jane Burton, Ali-Akbar Samadani, Rob Gorbet, and Dana Kulić. 2016. Laban
movement analysis and affective movement generation for robots and other near-
living creatures. In Dance Notations and Robot Motion. Springer, 25–48.
[11] Yves Candau, Jules Françoise, Sarah Fdili Alaoui, and Thecla Schiphorst. 2017.
Cultivating kinaesthetic awareness through interaction: Perspectives from so-
matic practices and embodied cognition. In Proceedings of the 4th International
Conference on Movement Computing. ACM, 1–8.
[12] Andy Clark. 2008. Supersizing the mind: Embodiment, action, and cognitive exten-
sion. New York: Oxford University Press.
[13] Luka Crnkovic-Friis and Louise Crnkovic-Friis. 2016. Generative Choreography
using Deep Learning. In Proceedings of the Seventh International Conference on
Computational Creativity.
[14] Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford,
and Ilya Sutskever. 2020. Jukebox: A generative model for music. arXiv preprint
arXiv:2005.00341 (2020).
[15] Brian Eno. 2001. Oblique Strategies. https://www.enoshop.co.uk/product/oblique-
strategies.html
[16] Sarah Fdili Alaoui. 2019. Making an interactive dance piece: Tensions in integrat-
ing technology in art. In Proceedings of the 2019 on designing interactive systems
conference. 1195–1208.
[17] Sarah Fdili Alaoui, Jules Françoise, Thecla Schiphorst, Karen Studd, and Frédéric
Bevilacqua. 2017. Seeing, Sensing and Recognizing Laban Movement Qualities.
In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems.
ACM, 4009–4020.
[18] Sarah Fdili Alaoui, Thecla Schiphorst, Shannon Cuykendall, Kristin Carlson,
Karen Studd, and Karen Bradley. 2015. Strategies for embodied design: The value
and challenges of observing movement. In Proceedings of the 2015 ACM SIGCHI
Conference on Creativity and Cognition. 121–130.
[19] Rebecca Fiebrink, Daniel Trueman, N Cameron Britt, Michelle Nagai, Konrad
Kaczmarek, Michael Early, MR Daniel, Anne Hege, and Perry R Cook. 2010. To-
ward understanding human-computer interaction in composing the instrument.
In ICMC.
[20] Andrés Galeano and Joanna Matuszak. 2014. Performing with Objects. PAJ: A
Journal of Performance and Art 36, 3 (2014), 102–111.
https://www.jstor.org/
stable/26386709
[21] Shaun Gallagher. 2017. Enactivist interventions: Rethinking the mind. Oxford
University Press.
[22] Petra Gemeinboeck and Rob Saunders. 2017. Movement matters: How a robot
becomes body. In Proceedings of the 4th international conference on movement
Computing. 1–8.
[23] Alex Graves. 2013. Generating sequences with recurrent neural networks. arXiv
preprint arXiv:1308.0850 (2013).
[24] David Ha and Douglas Eck. 2017. A neural representation of sketch drawings. In
International Conference on Learning Representations.
[25] Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long Short-term Memory. Neural
computation 9 (12 1997), 1735–80. https://doi.org/10.1162/neco.1997.9.8.1735
[26] David Holz. 2022. Midjourney. https://www.midjourney.com/
[27] Stacy Hsueh, Sarah Fdili Alaoui, and Wendy E Mackay. 2019. Understanding
kinaesthetic creativity in dance. In Proceedings of the 2019 CHI Conference on
Human Factors in Computing Systems. 1–12.
[28] Mikhail Jacob and Brian Magerko. 2015. Viewpoints ai. In Proceedings of the 2015
ACM SIGCHI Conference on Creativity and Cognition. 361–362.
[29] Mauri Kaipainen, Niklas Ravaja, Pia Tikka, Rasmus Vuori, Roberto Pugliese,
Marco Rapino, and Tapio Takala. 2011. Enactive systems and enactive media:
embodied human-machine coupling beyond interfaces. Leonardo 44, 5 (2011),
433–438.
[30] Hyung-Kwon Ko, Gwanmo Park, Hyeon Jeon, Jaemin Jo, Juho Kim, and Jinwook
Seo. 2023. Large-scale text-to-image generation models for visual artists’ creative
works. In Proceedings of the 28th International Conference on Intelligent User
Interfaces. 919–933.
[31] Tejas D Kulkarni, Karthik Narasimhan, Ardavan Saeedi, and Josh Tenenbaum.
2016. Hierarchical deep reinforcement learning: Integrating temporal abstraction
and intrinsic motivation. Advances in neural information processing systems 29
(2016).
[32] Meha Kumar, Duri Long, and Brian Magerko. 2020. Creativity Metrics for a
Lead-and-Follow Dynamic in an Improvisational Dance Agent. In Proceedings of
the Eleventh International Conference on Computational Creativity.
[33] Jina Lee and Stacy Marsella. 2006. Nonverbal Behavior Generator for Embodied
Conversational Agents. In Proceedings of the 6th International Conference on
Intelligent Virtual Agents (Marina Del Rey, CA) (IVA’06). Springer-Verlag, Berlin,
Heidelberg, 243–255. https://doi.org/10.1007/11821830_20
[34] Jiaman Li, Yihang Yin, Hang Chu, Yi Zhou, Tingwu Wang, Sanja Fidler, and Hao
Li. 2020. Learning to generate diverse dance motions with transformer. arXiv
preprint arXiv:2008.08171 (2020).
[35] Ruilong Li, Shan Yang, David A Ross, and Angjoo Kanazawa. 2021. Ai choreog-
rapher: Music conditioned 3d dance generation with aist++. In Proceedings of the
IEEE/CVF International Conference on Computer Vision. 13401–13412.
Embodying an Interactive AI for Dance Through Movement Ideation
C&C ’23, June 19–21, 2023, Virtual Event, USA
[36] Lucas Liu, Duri Long, Swar Gujrania, and Brian Magerko. 2019. Learning move-
ment through human-computer co-creative improvisation. In Proceedings of the
6th International Conference on Movement and Computing. 1–8.
[37] Clara Mancini, Yvonne Rogers, Arosha K Bandara, Tony Coe, Lukasz Jedrzejczyk,
Adam N Joinson, Blaine A Price, Keerthi Thomas, and Bashar Nuseibeh. 2010.
Contravision: exploring users’ reactions to futuristic technology. In Proceedings
of the SIGCHI conference on human factors in computing systems. 153–162.
[38] Charles Patrick Martin. 2022. Performing with a Generative Electronic Music
Controller. Joint proceedings of the ACM IUI Workshops (2022).
[39] Charles Patrick Martin, Kyrre Glette, Tønnes Frostad Nygaard, and Jim Torresen.
2020. Understanding musical predictions with an embodied interface for musical
machine learning. Frontiers in artificial intelligence 3 (2020), 6.
[40] Charles Patrick Martin and Jim Torresen. 2018. RoboJam: A musical mixture
density network for collaborative touchscreen interaction. In Int’l. Conference on
Computational Intelligence in Music, Sound, Art and Design. Springer, 161–176.
[41] Crypton Future Media. 2007. Who is Hatsune Miku? https://ec.crypton.co.jp/
pages/prod/virtualsinger/cv01_us
[42] Meinard Müller and Tido Röder. 2006. Motion templates for automatic classi-
fication and retrieval of motion capture data. In Proceedings of the 2006 ACM
SIGGRAPH/Eurographics symposium on Computer animation. Eurographics Asso-
ciation, 137–146.
[43] A Michael Noll. 2016. Early Digital Computer Art at Bell Telephone Laboratories,
Incorporated. Leonardo 49, 1 (2016), 55–65.
[44] Mariel Pettee, Chase Shimmin, Douglas Duhaime, and Ilya Vidrin. 2019. Beyond
Imitation: Generative and Variational Choreography via Machine Learning. 10th
International Conference on Computational Creativity (2019).
[45] Nicola Plant, Clarice Hilton, Marco Gillies, Rebecca Fiebrink, Phoenix Perry,
Carlos González Díaz, Ruth Gibson, Bruno Martelli, and Michael Zbyszynski.
2021. Interactive Machine Learning for Embodied Interaction Design: A tool and
methodology. In Proceedings of the Fifteenth International Conference on Tangible,
Embedded, and Embodied Interaction. 1–5.
[46] Richard Povall. 1998. Technology is with us. Dance Research Journal 30, 1 (1998),
1–4.
[47] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen.
2022. Hierarchical text-conditional image generation with clip latents. arXiv
preprint arXiv:2204.06125 (2022).
[48] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec
Radford, Mark Chen, and Ilya Sutskever. 2021. Zero-shot text-to-image generation.
In International Conference on Machine Learning. PMLR, 8821–8831.
[49] Susanne Ravn. 2020. Investigating dance improvisation: from spontaneity to
agency. Dance Research Journal 52, 2 (2020), 75–87.
[50] Christian Remy, Lindsay MacDonald Vermeulen, Jonas Frich, Michael Mose
Biskjaer, and Peter Dalsgaard. 2020. Evaluating creativity support tools in HCI
research. In Proceedings of the 2020 ACM designing interactive systems conference.
457–476.
[51] Graeme Ritchie. 2007. Some Empirical Criteria for Attributing Creativity to a
Computer Program. Minds and Machines 17 (2007), 76–99.
[52] Thecla Schiphorst. 2013. Merce Cunningham: Making dances with the computer.
Merce Cunningham: Creative elements (2013), 79–98.
[53] Jürgen Schmidhuber. 2006. Developmental robotics, optimal artificial curiosity,
creativity, music, and the fine arts. Connection Science 18, 2 (2006), 173–187.
[54] Nikhil Singh, Guillermo Bernal, Daria Savchenko, and Elena L Glassman. 2022.
Where to hide a stolen elephant: Leaps in creative writing with multimodal
machine intelligence. ACM Transactions on Computer-Human Interaction (2022).
[55] Nahre Sol. 2022. I used CHOPIN to program DRUMS. The results were kind of
amazing. https://youtu.be/wY1dIY--UDY?t=366
[56] Bob L Sturm and Oded Ben-Tal. 2017. Taking the models back to music practice:
evaluating generative transcription models built using deep learning. Journal of
Creative Music Systems 2, 1 (2017).
[57] Shuhei Tsuchida, Satoru Fukayama, Masahiro Hamasaki, and Masataka Goto.
2019. AIST Dance Video Database: Multi-genre, Multi-dancer, and Multi-camera
Database for Dance Information Processing. In Proceedings of the 20th Inter-
national Society for Music Information Retrieval Conference, ISMIR 2019. Delft,
Netherlands.
[58] Gabriel Vigliensoni, Louis McCallum, and Rebecca Fiebrink. 2020. Creating latent
spaces for modern music genre rhythms using minimal training data. (2020).
[59] Benedikte Wallace, Charles P. Martin, Jim Torresen, and Kristian Nymoen. 2020.
Towards Movement Generation with Audio Features. In Proceedings of the 11th
International Conference on Computational Creativity.
[60] Benedikte Wallace, Charles P Martin, Jim Tørresen, and Kristian Nymoen. 2021.
Learning Embodied Sound-Motion Mappings: Evaluating AI-Generated Dance
Improvisation. In Creativity and Cognition. 1–9.
[61] Benedikte Wallace, Kristian Nymoen, Charles P. Martin, and Jim. Tør-
resen. 2019.
DeepDance: Motion capture data of improvised dance.
https://doi.org/10.5281/zenodo.5838179.
[62] Qiushi Zhou, Cheng Cheng Chua, Jarrod Knibbe, Jorge Goncalves, and Eduardo
Velloso. 2021. Dance and Choreography in HCI: A Two-Decade Retrospective. In
Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems.
1–14.
Paper VI
Breaking from Realism: Exploring
the Potential of Glitch in
AI-Generated Dance
B. Wallace, K. Nymoen, J. Torresen, C.P. Martin
Under review
VI
125
